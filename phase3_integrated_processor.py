"""
Phase 3: í†µí•© DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ
ë¹„ë™ê¸° + ë¶„ì‚° + ìºì‹±ì„ ëª¨ë‘ í†µí•©í•œ ìµœê³  ì„±ëŠ¥ í”„ë¡œì„¸ì„œ
"""

import asyncio
import multiprocessing as mp
import time
import gc
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

@dataclass
class Phase3Config:
    """Phase 3 í†µí•© ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    batch_size: int = 500
    chunk_size: int = 1000
    gc_interval: int = 4
    
    # ë¹„ë™ê¸° ì„¤ì •
    enable_async: bool = True
    max_concurrent_dbs: int = 8
    max_concurrent_sheets: int = 16
    
    # ë¶„ì‚° ì²˜ë¦¬ ì„¤ì •
    enable_distributed: bool = True
    max_processes: int = None  # Noneì´ë©´ CPU ì½”ì–´ ìˆ˜
    worker_timeout: float = 300.0
    
    # ìºì‹± ì„¤ì •
    enable_caching: bool = True
    enable_redis_cache: bool = False  # Redis ì—†ì´ë„ ì‘ë™
    enable_memory_cache: bool = True
    memory_cache_size: int = 1000
    
    # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì •
    hybrid_threshold: int = 2  # íŒŒì¼ ìˆ˜ê°€ ì´ ê°’ ì´ìƒì´ë©´ ë¶„ì‚° ì²˜ë¦¬
    async_threshold: int = 4   # ì‹œíŠ¸ ìˆ˜ê°€ ì´ ê°’ ì´ìƒì´ë©´ ë¹„ë™ê¸° ì²˜ë¦¬

class Phase3IntegratedProcessor:
    """Phase 3 í†µí•© í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, config: Phase3Config = None):
        self.config = config or Phase3Config()
        
        # CPU ì½”ì–´ ìˆ˜ ìë™ ì„¤ì •
        if self.config.max_processes is None:
            self.config.max_processes = min(mp.cpu_count(), 8)
        
        self.logger = logging.getLogger(__name__)
        
        # ìºì‹œ ì´ˆê¸°í™”
        self.memory_cache = None
        if self.config.enable_caching and self.config.enable_memory_cache:
            from cached_db_processor import MemoryCache
            self.memory_cache = MemoryCache(self.config.memory_cache_size)
        
        self.stats = {
            'total_files_processed': 0,
            'total_items_processed': 0,
            'total_execution_time': 0,
            'processing_modes_used': [],
            'cache_hits': 0,
            'cache_misses': 0
        }
    
    def choose_processing_mode(self, db_files: List[Path]) -> str:
        """ìµœì ì˜ ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ"""
        file_count = len(db_files)
        
        # íŒŒì¼ í¬ê¸° ë¶„ì„
        total_size = sum(f.stat().st_size for f in db_files)
        avg_size = total_size / file_count if file_count > 0 else 0
        
        # ì²˜ë¦¬ ëª¨ë“œ ê²°ì • ë¡œì§
        if file_count >= self.config.hybrid_threshold and self.config.enable_distributed:
            if avg_size > 500000:  # 500KB ì´ìƒì˜ í° íŒŒì¼ë“¤
                return "distributed_async"  # ë¶„ì‚° + ë¹„ë™ê¸°
            else:
                return "distributed"  # ë¶„ì‚° ì²˜ë¦¬
        elif file_count >= 2 and self.config.enable_async:
            return "async"  # ë¹„ë™ê¸° ì²˜ë¦¬
        else:
            return "sequential"  # ìˆœì°¨ ì²˜ë¦¬
    
    async def process_with_async_cache(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë¹„ë™ê¸° + ìºì‹± ì²˜ë¦¬"""
        from async_db_processor import AsyncDBProcessor, AsyncConfig
        
        async_config = AsyncConfig(
            batch_size=self.config.batch_size,
            chunk_size=self.config.chunk_size,
            max_concurrent_dbs=self.config.max_concurrent_dbs,
            max_concurrent_sheets=self.config.max_concurrent_sheets
        )
        
        processor = AsyncDBProcessor(async_config)
        
        try:
            result = await processor.process_batch_async(db_files)
            result['processing_mode'] = 'async_cached'
            return result
        finally:
            await processor.cleanup()
    
    def process_with_distributed_cache(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë¶„ì‚° + ìºì‹± ì²˜ë¦¬"""
        from distributed_db_processor import DistributedDBProcessor, DistributedConfig
        
        dist_config = DistributedConfig(
            batch_size=self.config.batch_size,
            chunk_size=self.config.chunk_size,
            max_processes=self.config.max_processes,
            worker_timeout=self.config.worker_timeout
        )
        
        processor = DistributedDBProcessor(dist_config)
        result = processor.process_batch_distributed(db_files)
        result['processing_mode'] = 'distributed_cached'
        return result
    
    def process_with_sequential_cache(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìˆœì°¨ + ìºì‹± ì²˜ë¦¬"""
        from cached_db_processor import CachedDBProcessor, CacheConfig
        
        cache_config = CacheConfig(
            batch_size=self.config.batch_size,
            chunk_size=self.config.chunk_size,
            enable_memory_cache=self.config.enable_memory_cache,
            memory_cache_size=self.config.memory_cache_size
        )
        
        processor = CachedDBProcessor(cache_config)
        
        try:
            result = processor.process_batch_cached(db_files)
            result['processing_mode'] = 'sequential_cached'
            return result
        finally:
            processor.cleanup()
    
    async def process_hybrid_mode(self, db_files: List[Path]) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: ë¶„ì‚° + ë¹„ë™ê¸° + ìºì‹±"""
        self.logger.info("í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì‹¤í–‰: ë¶„ì‚° + ë¹„ë™ê¸° + ìºì‹±")
        
        # íŒŒì¼ë“¤ì„ í¬ê¸°ë³„ë¡œ ê·¸ë£¹í™”
        small_files = [f for f in db_files if f.stat().st_size < 200000]  # 200KB ë¯¸ë§Œ
        large_files = [f for f in db_files if f.stat().st_size >= 200000]  # 200KB ì´ìƒ
        
        results = []
        total_start_time = time.perf_counter()
        
        # í° íŒŒì¼ë“¤ì€ ë¶„ì‚° ì²˜ë¦¬
        if large_files:
            self.logger.info(f"í° íŒŒì¼ {len(large_files)}ê°œë¥¼ ë¶„ì‚° ì²˜ë¦¬")
            dist_result = self.process_with_distributed_cache(large_files)
            results.extend(dist_result['results'])
        
        # ì‘ì€ íŒŒì¼ë“¤ì€ ë¹„ë™ê¸° ì²˜ë¦¬
        if small_files:
            self.logger.info(f"ì‘ì€ íŒŒì¼ {len(small_files)}ê°œë¥¼ ë¹„ë™ê¸° ì²˜ë¦¬")
            async_result = await self.process_with_async_cache(small_files)
            results.extend(async_result['results'])
        
        total_execution_time = time.perf_counter() - total_start_time
        
        # ê²°ê³¼ í†µí•©
        successful_results = [r for r in results if r['success']]
        total_processed_items = sum(r.get('processed_items', 0) for r in successful_results)
        
        return {
            'success': True,
            'execution_time': total_execution_time,
            'total_processed_items': total_processed_items,
            'files_processed': len(successful_results),
            'files_failed': len(results) - len(successful_results),
            'results': results,
            'processing_mode': 'hybrid',
            'large_files_count': len(large_files),
            'small_files_count': len(small_files)
        }
    
    async def process_batch_phase3(self, db_files: List[Path]) -> Dict[str, Any]:
        """Phase 3 í†µí•© ì¼ê´„ ì²˜ë¦¬"""
        start_time = time.perf_counter()
        
        # ìµœì  ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ
        processing_mode = self.choose_processing_mode(db_files)
        self.logger.info(f"ì„ íƒëœ ì²˜ë¦¬ ëª¨ë“œ: {processing_mode}")
        
        self.stats['processing_modes_used'].append(processing_mode)
        
        # ì²˜ë¦¬ ëª¨ë“œì— ë”°ë¥¸ ì‹¤í–‰
        if processing_mode == "distributed_async":
            result = await self.process_hybrid_mode(db_files)
        elif processing_mode == "distributed":
            result = self.process_with_distributed_cache(db_files)
        elif processing_mode == "async":
            result = await self.process_with_async_cache(db_files)
        else:  # sequential
            result = self.process_with_sequential_cache(db_files)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        if result['success']:
            self.stats['total_files_processed'] += result['files_processed']
            self.stats['total_items_processed'] += result['total_processed_items']
            self.stats['total_execution_time'] += result['execution_time']
        
        return result
    
    def benchmark_all_modes(self, db_files: List[Path]) -> Dict[str, Any]:
        """ëª¨ë“  ì²˜ë¦¬ ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info("ëª¨ë“  Phase 3 ì²˜ë¦¬ ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        
        benchmark_results = {}
        
        # 1. ìˆœì°¨ + ìºì‹±
        try:
            self.logger.info("1. ìˆœì°¨ + ìºì‹± ëª¨ë“œ í…ŒìŠ¤íŠ¸")
            sequential_result = self.process_with_sequential_cache(db_files)
            benchmark_results['sequential_cached'] = sequential_result
        except Exception as e:
            self.logger.error(f"ìˆœì°¨ + ìºì‹± ëª¨ë“œ ì‹¤íŒ¨: {e}")
            benchmark_results['sequential_cached'] = {'success': False, 'error': str(e)}
        
        # 2. ë¶„ì‚° + ìºì‹±
        try:
            self.logger.info("2. ë¶„ì‚° + ìºì‹± ëª¨ë“œ í…ŒìŠ¤íŠ¸")
            distributed_result = self.process_with_distributed_cache(db_files)
            benchmark_results['distributed_cached'] = distributed_result
        except Exception as e:
            self.logger.error(f"ë¶„ì‚° + ìºì‹± ëª¨ë“œ ì‹¤íŒ¨: {e}")
            benchmark_results['distributed_cached'] = {'success': False, 'error': str(e)}
        
        # 3. ë¹„ë™ê¸° + ìºì‹±
        try:
            self.logger.info("3. ë¹„ë™ê¸° + ìºì‹± ëª¨ë“œ í…ŒìŠ¤íŠ¸")
            async_result = asyncio.run(self.process_with_async_cache(db_files))
            benchmark_results['async_cached'] = async_result
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° + ìºì‹± ëª¨ë“œ ì‹¤íŒ¨: {e}")
            benchmark_results['async_cached'] = {'success': False, 'error': str(e)}
        
        # 4. í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ
        try:
            self.logger.info("4. í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ í…ŒìŠ¤íŠ¸")
            hybrid_result = asyncio.run(self.process_hybrid_mode(db_files))
            benchmark_results['hybrid'] = hybrid_result
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì‹¤íŒ¨: {e}")
            benchmark_results['hybrid'] = {'success': False, 'error': str(e)}
        
        return benchmark_results
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ë°˜í™˜"""
        stats = dict(self.stats)
        
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
        stats['system_info'] = {
            'cpu_count': mp.cpu_count(),
            'max_processes': self.config.max_processes,
            'async_enabled': self.config.enable_async,
            'distributed_enabled': self.config.enable_distributed,
            'caching_enabled': self.config.enable_caching
        }
        
        return stats

async def main():
    """Phase 3 í†µí•© ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Phase 3: í†µí•© DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ")
    print("   (ë¹„ë™ê¸° + ë¶„ì‚° + ìºì‹± í†µí•©)")
    print("=" * 80)
    
    # ì„¤ì •
    config = Phase3Config(
        batch_size=500,
        chunk_size=1000,
        gc_interval=4,
        enable_async=True,
        enable_distributed=True,
        enable_caching=True,
        max_concurrent_dbs=8,
        max_concurrent_sheets=16,
        hybrid_threshold=2,
        async_threshold=4
    )
    
    # DB íŒŒì¼ ìˆ˜ì§‘
    db_dir = Path('database')
    if not db_dir.exists():
        print("âŒ Database ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000]
    
    if not db_files:
        print("âŒ ì²˜ë¦¬í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ì²˜ë¦¬ ëŒ€ìƒ: {len(db_files)}ê°œ íŒŒì¼")
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ê°€ëŠ¥ CPU: {mp.cpu_count()}ê°œ ì½”ì–´")
    
    # Phase 3 í†µí•© í”„ë¡œì„¸ì„œ ìƒì„±
    processor = Phase3IntegratedProcessor(config)
    
    try:
        # ëª¨ë“  ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬
        benchmark_results = processor.benchmark_all_modes(db_files)
        
        # ê²°ê³¼ ë¶„ì„ ë° ì¶œë ¥
        print(f"\nğŸ“Š Phase 3 ëª¨ë“  ëª¨ë“œ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:")
        print("=" * 60)
        
        mode_times = {}
        baseline_time = None
        
        for mode, result in benchmark_results.items():
            if result.get('success', False):
                exec_time = result['execution_time']
                items = result['total_processed_items']
                mode_times[mode] = exec_time
                
                if mode == 'sequential_cached':
                    baseline_time = exec_time
                
                print(f"{mode:20s}: {exec_time:.3f}ì´ˆ ({items:,}ê°œ í•­ëª©)")
            else:
                print(f"{mode:20s}: ì‹¤íŒ¨ - {result.get('error', 'Unknown error')}")
        
        # ì„±ëŠ¥ ë¹„êµ
        if baseline_time and mode_times:
            print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ (ìˆœì°¨ ìºì‹± ê¸°ì¤€):")
            print("-" * 40)
            
            for mode, exec_time in mode_times.items():
                if mode != 'sequential_cached':
                    speedup = baseline_time / exec_time if exec_time > 0 else 0
                    improvement = (1 - exec_time / baseline_time) * 100 if baseline_time > 0 else 0
                    print(f"{mode:20s}: {speedup:.2f}ë°° ë¹ ë¦„ ({improvement:+.1f}%)")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë“œ ì‹ë³„
        if mode_times:
            best_mode = min(mode_times.keys(), key=lambda k: mode_times[k])
            best_time = mode_times[best_mode]
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë“œ: {best_mode} ({best_time:.3f}ì´ˆ)")
        
        # í†µê³„ ì •ë³´
        stats = processor.get_stats()
        print(f"\nğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   CPU ì½”ì–´: {stats['system_info']['cpu_count']}ê°œ")
        print(f"   ë¹„ë™ê¸° ì²˜ë¦¬: {'âœ…' if stats['system_info']['async_enabled'] else 'âŒ'}")
        print(f"   ë¶„ì‚° ì²˜ë¦¬: {'âœ…' if stats['system_info']['distributed_enabled'] else 'âŒ'}")
        print(f"   ìºì‹± ì‹œìŠ¤í…œ: {'âœ…' if stats['system_info']['caching_enabled'] else 'âŒ'}")
        
        # ê²°ê³¼ ì €ì¥
        with open('phase3_integrated_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'benchmark_results': benchmark_results,
                'stats': stats,
                'config': config.__dict__
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ê²°ê³¼ê°€ 'phase3_integrated_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ Phase 3 í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        logging.error(f"Phase 3 í†µí•© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    print("=" * 80)

if __name__ == "__main__":
    # Windowsì—ì„œ multiprocessing ì‚¬ìš© ì‹œ í•„ìš”
    mp.set_start_method('spawn', force=True)
    
    # í•„ìš”í•œ ëª¨ë“ˆ ì„¤ì¹˜ í™•ì¸
    missing_modules = []
    
    try:
        import aiosqlite
    except ImportError:
        missing_modules.append("aiosqlite")
    
    if missing_modules:
        print(f"âŒ í•„ìš”í•œ ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_modules)}")
        print("ì„¤ì¹˜ ëª…ë ¹:")
        for module in missing_modules:
            print(f"  pip install {module}")
        sys.exit(1)
    
    asyncio.run(main())
