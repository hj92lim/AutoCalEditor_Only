"""
í”„ë¡œë•ì…˜ ì¤€ë¹„ëœ DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ
Phase 1 + Phase 2 ìµœì í™”ë¥¼ ì‹¤ì œ ì‹œìŠ¤í…œì— ì ìš©
"""

import time
import gc
import logging
import os
import sys
import threading
from pathlib import Path
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

@dataclass
class ProductionConfig:
    """í”„ë¡œë•ì…˜ ì„¤ì •"""
    # Phase 1 ìµœì í™”
    batch_size: int = 500
    chunk_size: int = 1000
    gc_interval: int = 4
    
    # Phase 2 ìµœì í™”
    enable_connection_pooling: bool = True
    enable_parallel_processing: bool = True
    max_workers: int = 4
    connection_pool_size: int = 10
    
    # ë©”ëª¨ë¦¬ ê´€ë¦¬
    max_memory_mb: int = 1024  # 1GB ì œí•œ
    memory_check_interval: int = 10

class ProductionConnectionPool:
    """í”„ë¡œë•ì…˜ìš© DB ì—°ê²° í’€"""
    
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self.connections = {}
        self.connection_usage = {}
        self.lock = threading.Lock()
        self.logger = logging.getLogger(__name__)
    
    def get_connection(self, db_path: str):
        """ì—°ê²° ê°€ì ¸ì˜¤ê¸° (ìŠ¤ë ˆë“œ ì•ˆì „)"""
        with self.lock:
            if db_path not in self.connections:
                try:
                    from data_manager.db_handler_v2 import DBHandlerV2
                    db_handler = DBHandlerV2(db_path)
                    db_handler.connect()
                    self.connections[db_path] = db_handler
                    self.connection_usage[db_path] = 0
                    self.logger.info(f"ìƒˆ DB ì—°ê²° ìƒì„±: {Path(db_path).name}")
                except Exception as e:
                    self.logger.error(f"DB ì—°ê²° ì‹¤íŒ¨ {db_path}: {e}")
                    raise
            
            self.connection_usage[db_path] += 1
            return self.connections[db_path]
    
    def release_connection(self, db_path: str):
        """ì—°ê²° ì‚¬ìš© ì™„ë£Œ í‘œì‹œ"""
        with self.lock:
            if db_path in self.connection_usage:
                self.connection_usage[db_path] -= 1
    
    def close_all(self):
        """ëª¨ë“  ì—°ê²° í•´ì œ"""
        with self.lock:
            for db_path, db_handler in self.connections.items():
                try:
                    db_handler.disconnect()
                    self.logger.info(f"DB ì—°ê²° í•´ì œ: {Path(db_path).name}")
                except Exception as e:
                    self.logger.error(f"DB ì—°ê²° í•´ì œ ì‹¤íŒ¨ {db_path}: {e}")
            
            self.connections.clear()
            self.connection_usage.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """ì—°ê²° í’€ í†µê³„"""
        with self.lock:
            return {
                'total_connections': len(self.connections),
                'active_connections': sum(1 for usage in self.connection_usage.values() if usage > 0),
                'connection_usage': dict(self.connection_usage)
            }

class ProductionDBProcessor:
    """í”„ë¡œë•ì…˜ìš© DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, config: ProductionConfig = None):
        self.config = config or ProductionConfig()
        self.connection_pool = ProductionConnectionPool(self.config.connection_pool_size) if self.config.enable_connection_pooling else None
        self.logger = logging.getLogger(__name__)
        self.stats = {
            'total_files_processed': 0,
            'total_items_processed': 0,
            'total_execution_time': 0,
            'memory_usage_peak': 0,
            'errors': []
        }
    
    def check_memory_usage(self) -> float:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬"""
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        if memory_mb > self.config.max_memory_mb:
            self.logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {memory_mb:.1f}MB > {self.config.max_memory_mb}MB")
            gc.collect()  # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        
        return memory_mb
    
    def process_single_db_production(self, db_file: Path) -> Dict[str, Any]:
        """í”„ë¡œë•ì…˜ìš© ë‹¨ì¼ DB ì²˜ë¦¬"""
        start_time = time.perf_counter()
        file_name = db_file.name
        
        try:
            self.logger.info(f"DB ì²˜ë¦¬ ì‹œì‘: {file_name}")
            
            # ì—°ê²° í’€ ì‚¬ìš© ë˜ëŠ” ì§ì ‘ ì—°ê²°
            if self.connection_pool:
                db_handler = self.connection_pool.get_connection(str(db_file))
            else:
                from data_manager.db_handler_v2 import DBHandlerV2
                db_handler = DBHandlerV2(str(db_file))
                db_handler.connect()
            
            # $ ì‹œíŠ¸ ì°¾ê¸°
            sheets = db_handler.get_sheets()
            dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
            
            if not dollar_sheets:
                self.logger.warning(f"$ ì‹œíŠ¸ê°€ ì—†ìŒ: {file_name}")
                return {
                    'success': True,
                    'file_name': file_name,
                    'execution_time': time.perf_counter() - start_time,
                    'processed_items': 0,
                    'warning': 'No dollar sheets found'
                }
            
            total_processed_items = 0
            
            # Ultra ìµœì í™”ëœ Cython ëª¨ë“ˆ ì‚¬ìš©
            from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
            for sheet_idx, sheet in enumerate(dollar_sheets):
                sheet_data = db_handler.get_sheet_data(sheet['id'])
                if not sheet_data:
                    continue
                
                self.logger.debug(f"ì‹œíŠ¸ ì²˜ë¦¬: {sheet['name']} ({len(sheet_data)}ê°œ ì…€)")
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                for chunk_start in range(0, len(sheet_data), self.config.chunk_size):
                    chunk_end = min(chunk_start + self.config.chunk_size, len(sheet_data))
                    chunk_data = sheet_data[chunk_start:chunk_end]
                    
                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½”ë“œ ìƒì„±
                    batch_count = 0
                    for batch_start in range(0, len(chunk_data), self.config.batch_size):
                        batch_end = min(batch_start + self.config.batch_size, len(chunk_data))
                        batch_data = chunk_data[batch_start:batch_end]
                        
                        # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                        code_items = []
                        for row_data in batch_data:
                            if len(row_data) >= 3:
                                code_items.append([
                                    "DEFINE", "CONST", "FLOAT32",
                                    f"VAL_{row_data[0]}_{row_data[1]}", 
                                    str(row_data[2]) if row_data[2] else "",
                                    f"Generated from {sheet['name']}"
                                ])
                        
                        # Ultra ìµœì í™”ëœ Cython ì½”ë“œ ìƒì„±
                        if code_items:
                            processed_code = ultra_fast_write_cal_list_processing(code_items)
                            total_processed_items += len(processed_code)
                        
                        # ë°°ì¹˜ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                        del code_items
                        if 'processed_code' in locals():
                            del processed_code
                        
                        batch_count += 1
                        
                        # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                        if batch_count % self.config.gc_interval == 0:
                            gc.collect()
                        
                        # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì²´í¬
                        if batch_count % self.config.memory_check_interval == 0:
                            self.check_memory_usage()
            
            # ì—°ê²° í’€ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì—°ê²° í•´ì œ
            if not self.connection_pool:
                db_handler.disconnect()
            else:
                self.connection_pool.release_connection(str(db_file))
            
            execution_time = time.perf_counter() - start_time
            
            self.logger.info(f"DB ì²˜ë¦¬ ì™„ë£Œ: {file_name} ({execution_time:.3f}ì´ˆ, {total_processed_items:,}ê°œ í•­ëª©)")
            
            return {
                'success': True,
                'file_name': file_name,
                'execution_time': execution_time,
                'processed_items': total_processed_items,
                'sheets_processed': len(dollar_sheets)
            }
            
        except Exception as e:
            execution_time = time.perf_counter() - start_time
            error_msg = f"DB ì²˜ë¦¬ ì‹¤íŒ¨: {file_name} - {str(e)}"
            self.logger.error(error_msg)
            self.stats['errors'].append(error_msg)
            
            return {
                'success': False,
                'file_name': file_name,
                'execution_time': execution_time,
                'error': str(e)
            }
    
    def process_batch_production(self, db_files: List[Path]) -> Dict[str, Any]:
        """í”„ë¡œë•ì…˜ìš© ì¼ê´„ ì²˜ë¦¬"""
        start_time = time.perf_counter()
        
        self.logger.info(f"ì¼ê´„ ì²˜ë¦¬ ì‹œì‘: {len(db_files)}ê°œ íŒŒì¼")
        
        if self.config.enable_parallel_processing and len(db_files) > 1:
            # ë³‘ë ¬ ì²˜ë¦¬
            self.logger.info(f"ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ (ì›Œì»¤: {self.config.max_workers}ê°œ)")
            results = self._process_parallel(db_files)
        else:
            # ìˆœì°¨ ì²˜ë¦¬
            self.logger.info("ìˆœì°¨ ì²˜ë¦¬ ëª¨ë“œ")
            results = self._process_sequential(db_files)
        
        execution_time = time.perf_counter() - start_time
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        successful_results = [r for r in results if r['success']]
        total_processed_items = sum(r.get('processed_items', 0) for r in successful_results)
        
        self.stats['total_files_processed'] += len(successful_results)
        self.stats['total_items_processed'] += total_processed_items
        self.stats['total_execution_time'] += execution_time
        
        self.logger.info(f"ì¼ê´„ ì²˜ë¦¬ ì™„ë£Œ: {execution_time:.3f}ì´ˆ, {total_processed_items:,}ê°œ í•­ëª©")
        
        return {
            'success': True,
            'execution_time': execution_time,
            'total_processed_items': total_processed_items,
            'files_processed': len(successful_results),
            'files_failed': len(results) - len(successful_results),
            'results': results,
            'processing_mode': 'parallel' if self.config.enable_parallel_processing and len(db_files) > 1 else 'sequential'
        }
    
    def _process_sequential(self, db_files: List[Path]) -> List[Dict[str, Any]]:
        """ìˆœì°¨ ì²˜ë¦¬"""
        results = []
        for db_file in db_files:
            result = self.process_single_db_production(db_file)
            results.append(result)
        return results
    
    def _process_parallel(self, db_files: List[Path]) -> List[Dict[str, Any]]:
        """ë³‘ë ¬ ì²˜ë¦¬"""
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            results = list(executor.map(self.process_single_db_production, db_files))
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        stats = dict(self.stats)
        
        if self.connection_pool:
            stats['connection_pool'] = self.connection_pool.get_stats()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        stats['current_memory_mb'] = self.check_memory_usage()
        
        return stats
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘")
        
        if self.connection_pool:
            self.connection_pool.close_all()
        
        gc.collect()
        self.logger.info("ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í”„ë¡œë•ì…˜ ì¤€ë¹„ëœ DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ")
    print("=" * 80)
    
    # ì„¤ì •
    config = ProductionConfig(
        batch_size=500,
        chunk_size=1000,
        gc_interval=4,
        enable_connection_pooling=True,
        enable_parallel_processing=True,
        max_workers=4,
        connection_pool_size=10,
        max_memory_mb=1024
    )
    
    # DB íŒŒì¼ ìˆ˜ì§‘
    db_dir = Path('database')
    if not db_dir.exists():
        print("âŒ Database ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000]
    
    if not db_files:
        print("âŒ ì²˜ë¦¬í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ì²˜ë¦¬ ëŒ€ìƒ: {len(db_files)}ê°œ íŒŒì¼")
    
    # í”„ë¡œì„¸ì„œ ìƒì„± ë° ì‹¤í–‰
    processor = ProductionDBProcessor(config)
    
    try:
        # ì¼ê´„ ì²˜ë¦¬ ì‹¤í–‰
        result = processor.process_batch_production(db_files)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.3f}ì´ˆ")
        print(f"   ì²˜ë¦¬ í•­ëª©: {result['total_processed_items']:,}ê°œ")
        print(f"   ì„±ê³µ íŒŒì¼: {result['files_processed']}ê°œ")
        print(f"   ì‹¤íŒ¨ íŒŒì¼: {result['files_failed']}ê°œ")
        print(f"   ì²˜ë¦¬ ëª¨ë“œ: {result['processing_mode']}")
        
        # í†µê³„ ì¶œë ¥
        stats = processor.get_stats()
        print(f"\nğŸ“ˆ ì‹œìŠ¤í…œ í†µê³„:")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['current_memory_mb']:.1f}MB")
        if 'connection_pool' in stats:
            print(f"   ì—°ê²° í’€: {stats['connection_pool']['total_connections']}ê°œ ì—°ê²°")
        
        # ê²°ê³¼ ì €ì¥
        with open('production_processing_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'result': result,
                'stats': stats,
                'config': config.__dict__
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ê²°ê³¼ê°€ 'production_processing_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        logging.error(f"í”„ë¡œë•ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    finally:
        processor.cleanup()
    
    print("=" * 80)

if __name__ == "__main__":
    main()
