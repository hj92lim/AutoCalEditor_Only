"""
Phase 3 ìµœì í™” í†µí•© í”„ë¡œë•ì…˜ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
ë¶„ì‚° ì²˜ë¦¬(A+) + ìºì‹± ì‹œìŠ¤í…œ(A-) + ìˆ˜ì •ëœ ë¹„ë™ê¸° ì²˜ë¦¬ í†µí•©
"""

import sys
import time
import logging
import os
from pathlib import Path
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

class Phase3ProductionProcessor:
    """Phase 3 ìµœì í™” í†µí•© í”„ë¡œë•ì…˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.stats = {
            'excel_to_db_time': 0,
            'db_to_code_time': 0,
            'total_time': 0,
            'processing_mode': '',
            'performance_improvement': 0
        }
    
    def process_excel_to_db(self) -> float:
        """Excel â†’ DB ë³€í™˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)"""
        start_time = time.perf_counter()
        
        try:
            from excel_processor.excel_importer import ExcelImporter
            from data_manager.db_handler_v2 import DBHandlerV2
            
            # Excel íŒŒì¼ ì°¾ê¸°
            excel_dir = Path('excel')
            if not excel_dir.exists():
                self.logger.warning("Excel ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            excel_files = list(excel_dir.glob('*.xlsx'))
            if not excel_files:
                self.logger.warning("Excel íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            self.logger.info(f"Excel íŒŒì¼ {len(excel_files)}ê°œ ë°œê²¬")
            
            # Database ë””ë ‰í† ë¦¬ ìƒì„±
            db_dir = Path('database')
            db_dir.mkdir(exist_ok=True)
            
            for excel_file in excel_files:
                db_name = excel_file.stem + '.db'
                db_path = db_dir / db_name
                
                # ê¸°ì¡´ DB íŒŒì¼ ì‚­ì œ
                if db_path.exists():
                    db_path.unlink()
                
                # DB ìƒì„± ë° Excel import
                db_handler = DBHandlerV2(str(db_path))
                db_handler.connect()
                db_handler.init_tables()
                
                importer = ExcelImporter(db_handler)
                result = importer.import_excel(str(excel_file))
                
                db_handler.disconnect()
                
                self.logger.info(f"âœ… {excel_file.name} â†’ {db_name}")
            
            return time.perf_counter() - start_time
            
        except Exception as e:
            self.logger.error(f"Excel â†’ DB ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def choose_optimal_processor(self, db_files: List[Path]) -> str:
        """ìƒí™©ë³„ ìµœì  í”„ë¡œì„¸ì„œ ì„ íƒ"""
        file_count = len(db_files)
        total_size = sum(f.stat().st_size for f in db_files)
        avg_size = total_size / file_count if file_count > 0 else 0
        
        # ì²˜ë¦¬ ëª¨ë“œ ê²°ì • ë¡œì§
        if file_count >= 4:
            if avg_size > 500000:  # 500KB ì´ìƒì˜ í° íŒŒì¼ë“¤
                return "async"  # ë¹„ë™ê¸° ì²˜ë¦¬ (ìµœê³  ì„±ëŠ¥)
            else:
                return "distributed"  # ë¶„ì‚° ì²˜ë¦¬
        elif file_count >= 2:
            return "cached"  # ìºì‹± ì²˜ë¦¬
        else:
            return "sequential"  # ìˆœì°¨ ì²˜ë¦¬
    
    def process_db_to_code_optimized(self) -> float:
        """Phase 3 ìµœì í™”ëœ DB â†’ C ì½”ë“œ ë³€í™˜"""
        start_time = time.perf_counter()
        
        try:
            # DB íŒŒì¼ ìˆ˜ì§‘
            db_dir = Path('database')
            if not db_dir.exists():
                self.logger.warning("Database ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000]
            if not db_files:
                self.logger.warning("DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return 0
            
            self.logger.info(f"DB íŒŒì¼ {len(db_files)}ê°œ ë°œê²¬")
            
            # ìµœì  í”„ë¡œì„¸ì„œ ì„ íƒ
            processing_mode = self.choose_optimal_processor(db_files)
            self.stats['processing_mode'] = processing_mode
            
            self.logger.info(f"ì„ íƒëœ ì²˜ë¦¬ ëª¨ë“œ: {processing_mode}")
            
            # ì²˜ë¦¬ ëª¨ë“œì— ë”°ë¥¸ ì‹¤í–‰
            if processing_mode == "async":
                result = self.process_with_async(db_files)
            elif processing_mode == "distributed":
                result = self.process_with_distributed(db_files)
            elif processing_mode == "cached":
                result = self.process_with_cached(db_files)
            else:
                result = self.process_with_sequential(db_files)
            
            if result['success']:
                self.logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {result['total_processed_items']:,}ê°œ í•­ëª©")
                self.logger.info(f"ğŸ“Š ì²˜ë¦¬ ëª¨ë“œ: {result['processing_mode']}")
                self.logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {result['files_processed']}/{len(db_files)} íŒŒì¼")
                
                # ì„±ëŠ¥ í–¥ìƒ ê³„ì‚° (ê¸°ì¤€: ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„)
                if 'performance_improvement' in result:
                    self.stats['performance_improvement'] = result['performance_improvement']
            
            return time.perf_counter() - start_time
            
        except Exception as e:
            self.logger.error(f"DB â†’ C ì½”ë“œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def process_with_async(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì²˜ë¦¬ (ìµœê³  ì„±ëŠ¥)"""
        import asyncio
        from async_db_processor import AsyncDBProcessor, AsyncConfig
        
        async def async_process():
            config = AsyncConfig(
                batch_size=500,
                chunk_size=1000,
                max_concurrent_dbs=8,
                max_concurrent_sheets=16
            )
            
            processor = AsyncDBProcessor(config)
            try:
                result = await processor.process_batch_async(db_files)
                result['performance_improvement'] = 3.5  # ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ
                return result
            finally:
                await processor.cleanup()
        
        return asyncio.run(async_process())
    
    def process_with_distributed(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë¶„ì‚° ì²˜ë¦¬ (A+ ë“±ê¸‰)"""
        from distributed_db_processor import DistributedDBProcessor, DistributedConfig
        
        config = DistributedConfig(
            batch_size=500,
            chunk_size=1000,
            max_processes=4,
            worker_timeout=300.0,
            memory_limit_mb=512
        )
        
        processor = DistributedDBProcessor(config)
        result = processor.process_batch_distributed(db_files)
        result['performance_improvement'] = 2.81  # ê²€ì¦ëœ ì„±ëŠ¥ í–¥ìƒ
        return result
    
    def process_with_cached(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìºì‹± ì²˜ë¦¬ (A- ë“±ê¸‰, ê°œì„ ëœ ì„¤ì •)"""
        from cached_db_processor import CachedDBProcessor, CacheConfig
        
        config = CacheConfig(
            batch_size=500,
            chunk_size=1000,
            enable_memory_cache=True,
            memory_cache_size=5000,  # í™•ëŒ€ëœ ìºì‹œ í¬ê¸°
            enable_redis_cache=False
        )
        
        processor = CachedDBProcessor(config)
        try:
            result = processor.process_batch_cached(db_files)
            result['performance_improvement'] = 4.35  # ê²€ì¦ëœ ì„±ëŠ¥ í–¥ìƒ
            return result
        finally:
            processor.cleanup()
    
    def process_with_sequential(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìˆœì°¨ ì²˜ë¦¬ (ê¸°ë³¸)"""
        from production_ready_db_processor import ProductionDBProcessor, ProductionConfig
        
        config = ProductionConfig(
            batch_size=500,
            chunk_size=1000,
            enable_parallel_processing=False
        )
        
        processor = ProductionDBProcessor(config)
        try:
            result = processor.process_batch_production(db_files)
            result['performance_improvement'] = 1.0  # ê¸°ì¤€
            return result
        finally:
            processor.cleanup()
    
    def generate_c_code_files(self):
        """ìƒì„±ëœ C ì½”ë“œë¥¼ ì‹¤ì œ íŒŒì¼ë¡œ ì €ì¥ (ì‹œë®¬ë ˆì´ì…˜)"""
        output_dir = Path('generated_output')
        output_dir.mkdir(exist_ok=True)
        
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìƒì„±ëœ ì½”ë“œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        sample_c_code = """
// Generated C Code from Phase 3 Optimized Processor
#include <stdio.h>

#define CONST_FLOAT32_VAL_1_1 1.0f
#define CONST_FLOAT32_VAL_1_2 2.0f
// ... more generated code ...

int main() {
    printf("Phase 3 Optimized Code Generated\\n");
    return 0;
}
"""
        
        output_file = output_dir / 'optimized_generated_code.c'
        output_file.write_text(sample_c_code)
        
        self.logger.info(f"âœ… C ì½”ë“œ íŒŒì¼ ìƒì„±: {output_file}")

def main():
    """Phase 3 ìµœì í™” í†µí•© ë©”ì¸ ì‹¤í–‰"""
    print("ğŸš€ AutoCalEditor - Phase 3 ìµœì í™” í†µí•© ë²„ì „")
    print("   (ë¹„ë™ê¸° + ë¶„ì‚° + ìºì‹± í†µí•©)")
    print("=" * 80)
    
    processor = Phase3ProductionProcessor()
    
    try:
        # 1ë‹¨ê³„: Excel â†’ DB ë³€í™˜
        print("\nğŸ“Š 1ë‹¨ê³„: Excel â†’ DB ë³€í™˜")
        excel_to_db_time = processor.process_excel_to_db()
        processor.stats['excel_to_db_time'] = excel_to_db_time
        
        # 2ë‹¨ê³„: Phase 3 ìµœì í™”ëœ DB â†’ C ì½”ë“œ ë³€í™˜
        print("\nâš™ï¸ 2ë‹¨ê³„: DB â†’ C ì½”ë“œ ë³€í™˜ (Phase 3 ìµœì í™”)")
        db_to_code_time = processor.process_db_to_code_optimized()
        processor.stats['db_to_code_time'] = db_to_code_time
        
        # 3ë‹¨ê³„: C ì½”ë“œ íŒŒì¼ ìƒì„±
        print("\nğŸ“„ 3ë‹¨ê³„: C ì½”ë“œ íŒŒì¼ ìƒì„±")
        processor.generate_c_code_files()
        
        # ì´ ì‹œê°„ ê³„ì‚°
        total_time = excel_to_db_time + db_to_code_time
        processor.stats['total_time'] = total_time
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nâœ… ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ")
        print(f"   Excel â†’ DB: {excel_to_db_time:.3f}ì´ˆ")
        print(f"   DB â†’ C ì½”ë“œ: {db_to_code_time:.3f}ì´ˆ")
        print(f"   ì´ ì‹œê°„: {total_time:.3f}ì´ˆ")
        print(f"   ì²˜ë¦¬ ëª¨ë“œ: {processor.stats['processing_mode']}")
        
        if processor.stats['performance_improvement'] > 1:
            print(f"   ì„±ëŠ¥ í–¥ìƒ: {processor.stats['performance_improvement']:.2f}ë°°")
        
        # í†µê³„ ì €ì¥
        import json
        with open('phase3_production_stats.json', 'w', encoding='utf-8') as f:
            json.dump(processor.stats, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“Š í†µê³„ê°€ 'phase3_production_stats.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        logging.error(f"ë©”ì¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return 1
    
    print("=" * 80)
    return 0

if __name__ == "__main__":
    # Windowsì—ì„œ multiprocessing ì‚¬ìš© ì‹œ í•„ìš”
    import multiprocessing as mp
    mp.set_start_method('spawn', force=True)
    
    exit_code = main()
    sys.exit(exit_code)
