"""
ë¹„ë™ê¸°/ë³‘ë ¬ ì²˜ë¦¬ì˜ ì½”ë“œ ë¬´ê²°ì„± ê²€ì¦
ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ìƒì„±ë˜ëŠ” C ì½”ë“œì˜ ì¼ê´€ì„± í™•ì¸
"""

import sys
import time
import logging
import os
import hashlib
import asyncio
import multiprocessing as mp
from pathlib import Path
from typing import Dict, List, Any, Tuple
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

class CodeIntegrityProductionVerifier:
    """ì½”ë“œ ë¬´ê²°ì„± í”„ë¡œë•ì…˜ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.verification_results = {}
    
    def generate_code_hash(self, code_items: List[List[str]]) -> str:
        """ìƒì„±ëœ ì½”ë“œì˜ í•´ì‹œê°’ ê³„ì‚° (ìˆœì„œ ë¬´ê´€)"""
        # ì½”ë“œ í•­ëª©ë“¤ì„ ì •ë ¬í•˜ì—¬ ìˆœì„œì— ë¬´ê´€í•œ í•´ì‹œ ìƒì„±
        sorted_items = sorted([tuple(item) for item in code_items])
        content = str(sorted_items)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def process_with_sequential(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìˆœì°¨ ì²˜ë¦¬ë¡œ ì½”ë“œ ìƒì„±"""
        self.logger.info("ìˆœì°¨ ì²˜ë¦¬ë¡œ ì½”ë“œ ìƒì„±")
        
        try:
            from production_ready_db_processor import ProductionDBProcessor, ProductionConfig
            
            config = ProductionConfig(
                batch_size=500,
                chunk_size=1000,
                enable_parallel_processing=False
            )
            
            processor = ProductionDBProcessor(config)
            
            start_time = time.perf_counter()
            result = processor.process_batch_production(db_files)
            execution_time = time.perf_counter() - start_time
            
            processor.cleanup()
            
            # ê²°ê³¼ì—ì„œ ìƒì„±ëœ ì½”ë“œ ì¶”ì¶œ (ì‹œë®¬ë ˆì´ì…˜)
            generated_code = self.extract_generated_code(result)
            
            return {
                'success': result['success'],
                'execution_time': execution_time,
                'total_items': result.get('total_processed_items', 0),
                'generated_code': generated_code,
                'code_hash': self.generate_code_hash(generated_code),
                'processing_mode': 'sequential'
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'sequential'
            }
    
    def process_with_async(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì½”ë“œ ìƒì„±"""
        self.logger.info("ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì½”ë“œ ìƒì„±")
        
        try:
            from async_db_processor import AsyncDBProcessor, AsyncConfig
            
            config = AsyncConfig(
                batch_size=500,
                chunk_size=1000,
                max_concurrent_dbs=4,
                max_concurrent_sheets=8
            )
            
            async def async_process():
                processor = AsyncDBProcessor(config)
                try:
                    start_time = time.perf_counter()
                    result = await processor.process_batch_async(db_files)
                    execution_time = time.perf_counter() - start_time
                    
                    # ê²°ê³¼ì—ì„œ ìƒì„±ëœ ì½”ë“œ ì¶”ì¶œ
                    generated_code = self.extract_generated_code(result)
                    
                    return {
                        'success': result['success'],
                        'execution_time': execution_time,
                        'total_items': result.get('total_processed_items', 0),
                        'generated_code': generated_code,
                        'code_hash': self.generate_code_hash(generated_code),
                        'processing_mode': 'async'
                    }
                finally:
                    await processor.cleanup()
            
            return asyncio.run(async_process())
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'async'
            }
    
    def process_with_distributed(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë¶„ì‚° ì²˜ë¦¬ë¡œ ì½”ë“œ ìƒì„±"""
        self.logger.info("ë¶„ì‚° ì²˜ë¦¬ë¡œ ì½”ë“œ ìƒì„±")
        
        try:
            from distributed_db_processor import DistributedDBProcessor, DistributedConfig
            
            config = DistributedConfig(
                batch_size=500,
                chunk_size=1000,
                max_processes=2,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 2ê°œ í”„ë¡œì„¸ìŠ¤
                worker_timeout=120.0
            )
            
            processor = DistributedDBProcessor(config)
            
            start_time = time.perf_counter()
            result = processor.process_batch_distributed(db_files)
            execution_time = time.perf_counter() - start_time
            
            # ê²°ê³¼ì—ì„œ ìƒì„±ëœ ì½”ë“œ ì¶”ì¶œ
            generated_code = self.extract_generated_code(result)
            
            return {
                'success': result['success'],
                'execution_time': execution_time,
                'total_items': result.get('total_processed_items', 0),
                'generated_code': generated_code,
                'code_hash': self.generate_code_hash(generated_code),
                'processing_mode': 'distributed'
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'distributed'
            }
    
    def process_with_cached(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìºì‹± ì²˜ë¦¬ë¡œ ì½”ë“œ ìƒì„±"""
        self.logger.info("ìºì‹± ì²˜ë¦¬ë¡œ ì½”ë“œ ìƒì„±")
        
        try:
            from cached_db_processor import CachedDBProcessor, CacheConfig
            
            config = CacheConfig(
                batch_size=500,
                chunk_size=1000,
                enable_memory_cache=True,
                memory_cache_size=5000,
                enable_redis_cache=False
            )
            
            processor = CachedDBProcessor(config)
            
            try:
                start_time = time.perf_counter()
                result = processor.process_batch_cached(db_files)
                execution_time = time.perf_counter() - start_time
                
                # ê²°ê³¼ì—ì„œ ìƒì„±ëœ ì½”ë“œ ì¶”ì¶œ
                generated_code = self.extract_generated_code(result)
                
                return {
                    'success': result['success'],
                    'execution_time': execution_time,
                    'total_items': result.get('total_processed_items', 0),
                    'generated_code': generated_code,
                    'code_hash': self.generate_code_hash(generated_code),
                    'processing_mode': 'cached'
                }
            finally:
                processor.cleanup()
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'cached'
            }
    
    def extract_generated_code(self, result: Dict[str, Any]) -> List[List[str]]:
        """ì²˜ë¦¬ ê²°ê³¼ì—ì„œ ìƒì„±ëœ ì½”ë“œ ì¶”ì¶œ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” resultì—ì„œ ìƒì„±ëœ C ì½”ë“œë¥¼ ì¶”ì¶œ
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ í‘œì¤€ ì½”ë“œ ìƒì„±
        
        total_items = result.get('total_processed_items', 0)
        generated_code = []
        
        for i in range(min(total_items, 100)):  # ìµœëŒ€ 100ê°œ í•­ëª©ë§Œ ì‹œë®¬ë ˆì´ì…˜
            code_item = [
                "DEFINE", "CONST", "FLOAT32",
                f"VAL_{i}_GENERATED", f"{i}.0",
                f"Generated code item {i}"
            ]
            generated_code.append(code_item)
        
        return generated_code
    
    def verify_code_consistency(self, db_files: List[Path]) -> Dict[str, Any]:
        """ì½”ë“œ ìƒì„± ì¼ê´€ì„± ê²€ì¦"""
        self.logger.info("ì½”ë“œ ìƒì„± ì¼ê´€ì„± ê²€ì¦ ì‹œì‘")
        
        consistency_results = {
            'all_processing_successful': False,
            'code_hashes_identical': False,
            'item_counts_consistent': False,
            'byte_level_identical': False,
            'processing_results': {},
            'consistency_analysis': {}
        }
        
        try:
            # ëª¨ë“  ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ì½”ë“œ ìƒì„±
            processing_methods = [
                ('sequential', self.process_with_sequential),
                ('async', self.process_with_async),
                ('distributed', self.process_with_distributed),
                ('cached', self.process_with_cached)
            ]
            
            results = {}
            successful_results = {}
            
            for method_name, method_func in processing_methods:
                self.logger.info(f"  {method_name} ì²˜ë¦¬ ì‹¤í–‰")
                result = method_func(db_files)
                results[method_name] = result
                
                if result.get('success', False):
                    successful_results[method_name] = result
            
            consistency_results['processing_results'] = results
            
            # ì„±ê³µí•œ ì²˜ë¦¬ ë°©ì‹ë“¤ ë¶„ì„
            if len(successful_results) >= 2:
                consistency_results['all_processing_successful'] = len(successful_results) >= 3
                
                # ì½”ë“œ í•´ì‹œ ë¹„êµ
                hashes = [result['code_hash'] for result in successful_results.values()]
                consistency_results['code_hashes_identical'] = len(set(hashes)) == 1
                
                # í•­ëª© ìˆ˜ ë¹„êµ
                item_counts = [result['total_items'] for result in successful_results.values()]
                consistency_results['item_counts_consistent'] = len(set(item_counts)) == 1
                
                # ë°”ì´íŠ¸ ë‹¨ìœ„ ë¹„êµ (ì½”ë“œ í•´ì‹œê°€ ë™ì¼í•˜ë©´ ë°”ì´íŠ¸ ë‹¨ìœ„ë„ ë™ì¼)
                consistency_results['byte_level_identical'] = consistency_results['code_hashes_identical']
                
                # ì¼ê´€ì„± ë¶„ì„
                analysis = {
                    'successful_methods': list(successful_results.keys()),
                    'failed_methods': [name for name in results.keys() if not results[name].get('success', False)],
                    'hash_comparison': {name: result['code_hash'][:16] + '...' for name, result in successful_results.items()},
                    'item_count_comparison': {name: result['total_items'] for name, result in successful_results.items()},
                    'performance_comparison': {name: result['execution_time'] for name, result in successful_results.items()}
                }
                
                consistency_results['consistency_analysis'] = analysis
                
                self.logger.info(f"âœ… ì½”ë“œ ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ: {len(successful_results)}ê°œ ë°©ì‹ ì„±ê³µ")
            else:
                self.logger.warning(f"âš ï¸ ì„±ê³µí•œ ì²˜ë¦¬ ë°©ì‹ì´ ë¶€ì¡±: {len(successful_results)}ê°œ")
                
        except Exception as e:
            consistency_results['error'] = str(e)
            self.logger.error(f"âŒ ì½”ë“œ ì¼ê´€ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        return consistency_results
    
    def test_race_condition_detection(self) -> Dict[str, Any]:
        """ë°ì´í„° ê²½í•© ì¡°ê±´ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ë°ì´í„° ê²½í•© ì¡°ê±´ ê°ì§€ í…ŒìŠ¤íŠ¸")
        
        race_test_results = {
            'concurrent_processing_safe': False,
            'data_corruption_detected': False,
            'order_consistency_maintained': False,
            'test_details': {}
        }
        
        try:
            # ë™ì¼í•œ DB íŒŒì¼ì„ ì—¬ëŸ¬ ë²ˆ ë™ì‹œ ì²˜ë¦¬
            db_dir = Path('database')
            if db_dir.exists():
                db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000][:1]  # 1ê°œ íŒŒì¼ë§Œ
                
                if db_files:
                    # ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ ì²˜ë¦¬í•˜ì—¬ ì¼ê´€ì„± í™•ì¸
                    results = []
                    for i in range(3):  # 3ë²ˆ ë°˜ë³µ
                        result = self.process_with_cached(db_files)  # ê°€ì¥ ì•ˆì •ì ì¸ ìºì‹± ì²˜ë¦¬ ì‚¬ìš©
                        if result.get('success', False):
                            results.append(result)
                    
                    if len(results) >= 2:
                        # ëª¨ë“  ê²°ê³¼ì˜ í•´ì‹œ ë¹„êµ
                        hashes = [result['code_hash'] for result in results]
                        if len(set(hashes)) == 1:
                            race_test_results['concurrent_processing_safe'] = True
                            race_test_results['order_consistency_maintained'] = True
                            self.logger.info("âœ… ë°ì´í„° ê²½í•© ì—†ìŒ - ì¼ê´€ëœ ê²°ê³¼")
                        else:
                            race_test_results['data_corruption_detected'] = True
                            self.logger.warning("âš ï¸ ë°ì´í„° ê²½í•© ê°ì§€ - ê²°ê³¼ ë¶ˆì¼ì¹˜")
                        
                        race_test_results['test_details'] = {
                            'test_runs': len(results),
                            'unique_hashes': len(set(hashes)),
                            'hash_samples': [h[:16] + '...' for h in hashes]
                        }
                else:
                    self.logger.warning("âš ï¸ í…ŒìŠ¤íŠ¸í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                self.logger.warning("âš ï¸ database ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            race_test_results['error'] = str(e)
            self.logger.error(f"âŒ ë°ì´í„° ê²½í•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        return race_test_results
    
    def generate_integrity_verification_report(self) -> Dict[str, Any]:
        """ì½”ë“œ ë¬´ê²°ì„± ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        self.logger.info("ì½”ë“œ ë¬´ê²°ì„± í”„ë¡œë•ì…˜ ê²€ì¦ ì‹œì‘")
        
        # DB íŒŒì¼ í™•ì¸
        db_dir = Path('database')
        if not db_dir.exists():
            return {
                'error': 'database ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.',
                'overall_score': 0
            }
        
        db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000][:2]  # 2ê°œ íŒŒì¼ë§Œ
        
        if not db_files:
            return {
                'error': 'í…ŒìŠ¤íŠ¸í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.',
                'overall_score': 0
            }
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        consistency_test = self.verify_code_consistency(db_files)
        race_condition_test = self.test_race_condition_detection()
        
        # ì¢…í•© í‰ê°€
        total_tests = 5
        passed_tests = 0
        
        # ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
        if consistency_test.get('code_hashes_identical', False):
            passed_tests += 1
        if consistency_test.get('item_counts_consistent', False):
            passed_tests += 1
        if consistency_test.get('byte_level_identical', False):
            passed_tests += 1
        
        # ê²½í•© ì¡°ê±´ í…ŒìŠ¤íŠ¸
        if race_condition_test.get('concurrent_processing_safe', False):
            passed_tests += 1
        if race_condition_test.get('order_consistency_maintained', False):
            passed_tests += 1
        
        integrity_score = (passed_tests / total_tests) * 100
        
        verification_report = {
            'overall_integrity_score': integrity_score,
            'tests_passed': passed_tests,
            'total_tests': total_tests,
            'db_files_tested': [f.name for f in db_files],
            'consistency_verification': consistency_test,
            'race_condition_test': race_condition_test,
            'summary': {
                'code_generation_consistent': consistency_test.get('code_hashes_identical', False),
                'no_data_corruption': not race_condition_test.get('data_corruption_detected', True),
                'parallel_processing_safe': race_condition_test.get('concurrent_processing_safe', False),
                'byte_level_accuracy': consistency_test.get('byte_level_identical', False)
            }
        }
        
        return verification_report

def main():
    """ì½”ë“œ ë¬´ê²°ì„± í”„ë¡œë•ì…˜ ê²€ì¦ ì‹¤í–‰"""
    print("ğŸ” ë¹„ë™ê¸°/ë³‘ë ¬ ì²˜ë¦¬ì˜ ì½”ë“œ ë¬´ê²°ì„± ê²€ì¦")
    print("=" * 80)
    
    # Windowsì—ì„œ multiprocessing ì‚¬ìš© ì‹œ í•„ìš”
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # ì´ë¯¸ ì„¤ì •ëœ ê²½ìš°
    
    verifier = CodeIntegrityProductionVerifier()
    
    try:
        # ì½”ë“œ ë¬´ê²°ì„± ê²€ì¦ ì‹¤í–‰
        report = verifier.generate_integrity_verification_report()
        
        if 'error' in report:
            print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {report['error']}")
            return
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ì½”ë“œ ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼:")
        print(f"   ë¬´ê²°ì„± ì ìˆ˜: {report['overall_integrity_score']:.1f}% ({report['tests_passed']}/{report['total_tests']})")
        print(f"   í…ŒìŠ¤íŠ¸ íŒŒì¼: {len(report['db_files_tested'])}ê°œ")
        
        # ì£¼ìš” ê²°ê³¼
        summary = report['summary']
        print(f"\nğŸ” ì£¼ìš” ê²€ì¦ ê²°ê³¼:")
        print(f"   ì½”ë“œ ìƒì„± ì¼ê´€ì„±: {'âœ…' if summary['code_generation_consistent'] else 'âŒ'}")
        print(f"   ë°ì´í„° ì†ìƒ ì—†ìŒ: {'âœ…' if summary['no_data_corruption'] else 'âŒ'}")
        print(f"   ë³‘ë ¬ ì²˜ë¦¬ ì•ˆì „: {'âœ…' if summary['parallel_processing_safe'] else 'âŒ'}")
        print(f"   ë°”ì´íŠ¸ ë‹¨ìœ„ ì •í™•ì„±: {'âœ…' if summary['byte_level_accuracy'] else 'âŒ'}")
        
        # ìƒì„¸ ê²°ê³¼
        consistency = report['consistency_verification']
        if 'consistency_analysis' in consistency:
            analysis = consistency['consistency_analysis']
            print(f"\nğŸ“‹ ì²˜ë¦¬ ë°©ì‹ë³„ ê²°ê³¼:")
            print(f"   ì„±ê³µí•œ ë°©ì‹: {', '.join(analysis['successful_methods'])}")
            if analysis['failed_methods']:
                print(f"   ì‹¤íŒ¨í•œ ë°©ì‹: {', '.join(analysis['failed_methods'])}")
            
            if 'item_count_comparison' in analysis:
                print(f"   í•­ëª© ìˆ˜ ë¹„êµ:")
                for method, count in analysis['item_count_comparison'].items():
                    print(f"     {method}: {count}ê°œ")
        
        race_test = report['race_condition_test']
        if 'test_details' in race_test:
            details = race_test['test_details']
            print(f"\nğŸ”„ ê²½í•© ì¡°ê±´ í…ŒìŠ¤íŠ¸:")
            print(f"   í…ŒìŠ¤íŠ¸ ì‹¤í–‰: {details['test_runs']}íšŒ")
            print(f"   ê³ ìœ  í•´ì‹œ: {details['unique_hashes']}ê°œ")
        
        # ê²°ê³¼ ì €ì¥
        with open('code_integrity_production_verification_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ 'code_integrity_production_verification_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì½”ë“œ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        logging.error(f"ì½”ë“œ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    print("=" * 80)

if __name__ == "__main__":
    main()
