"""
DB â†’ C ì½”ë“œ ë³€í™˜ ì„±ëŠ¥ ë¬¸ì œ ì¢…í•© í•´ê²° ë°©ì•ˆ
ì‹¤ì œ DB íŒŒì¼ë“¤ì„ ì‚¬ìš©í•œ ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™” ì†”ë£¨ì…˜ ì œì‹œ
"""

import time
import psutil
import gc
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
import json
import traceback
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

class ComprehensivePerformanceSolution:
    """ì¢…í•©ì ì¸ ì„±ëŠ¥ ë¬¸ì œ í•´ê²° ë°©ì•ˆ"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.analysis_results = {}
        
    def get_existing_db_files(self) -> List[Path]:
        """ê¸°ì¡´ DB íŒŒì¼ë“¤ ìˆ˜ì§‘"""
        db_dir = Path('database')
        if not db_dir.exists():
            return []
        
        db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000]  # 50KB ì´ìƒë§Œ
        return sorted(db_files, key=lambda x: x.stat().st_size, reverse=True)[:5]  # í° íŒŒì¼ 5ê°œ
    
    def measure_detailed_performance(self, func, *args, **kwargs) -> Dict[str, Any]:
        """ìƒì„¸í•œ ì„±ëŠ¥ ì¸¡ì •"""
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        
        # ì‹œì‘ ì‹œì  ì¸¡ì •
        start_memory = self.process.memory_info()
        start_time = time.perf_counter()
        start_cpu_times = self.process.cpu_times()
        
        try:
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ì¢…ë£Œ ì‹œì  ì¸¡ì •
            end_time = time.perf_counter()
            end_memory = self.process.memory_info()
            end_cpu_times = self.process.cpu_times()
            
            execution_time = end_time - start_time
            memory_delta = end_memory.rss - start_memory.rss
            cpu_time_delta = (end_cpu_times.user - start_cpu_times.user) + (end_cpu_times.system - start_cpu_times.system)
            
            return {
                'success': True,
                'execution_time': execution_time,
                'memory_delta_bytes': memory_delta,
                'memory_delta_mb': memory_delta / 1024 / 1024,
                'cpu_time_delta': cpu_time_delta,
                'start_memory_mb': start_memory.rss / 1024 / 1024,
                'end_memory_mb': end_memory.rss / 1024 / 1024,
                'result': result
            }
            
        except Exception as e:
            end_time = time.perf_counter()
            return {
                'success': False,
                'execution_time': end_time - start_time,
                'error': str(e),
                'traceback': traceback.format_exc()
            }
    
    def optimized_single_db_processing(self, db_file: Path) -> Dict[str, Any]:
        """ìµœì í™”ëœ ë‹¨ì¼ DB ì²˜ë¦¬"""
        try:
            from data_manager.db_handler_v2 import DBHandlerV2
            from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing
            
            # DB ì—°ê²° (ì—°ê²° í’€ë§ ì‹œë®¬ë ˆì´ì…˜)
            db_handler = DBHandlerV2(str(db_file))
            db_handler.connect()
            
            # $ ì‹œíŠ¸ ì°¾ê¸° (ìºì‹± ì‹œë®¬ë ˆì´ì…˜)
            sheets = db_handler.get_sheets()
            dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
            
            total_processed_items = 0
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            batch_size = 1000
            
            for sheet in dollar_sheets:
                sheet_data = db_handler.get_sheet_data(sheet['id'])
                if sheet_data:
                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½”ë“œ ìƒì„±
                    for i in range(0, len(sheet_data), batch_size):
                        batch_data = sheet_data[i:i+batch_size]
                        
                        # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                        code_items = []
                        for row_data in batch_data:
                            if len(row_data) >= 3:
                                code_items.append([
                                    "DEFINE", "CONST", "FLOAT32",
                                    f"VAL_{row_data[0]}_{row_data[1]}", 
                                    str(row_data[2]) if row_data[2] else "",
                                    f"Generated from {sheet['name']}"
                                ])
                        
                        # Ultra ìµœì í™”ëœ Cython ì½”ë“œ ìƒì„±
                        processed_code = ultra_fast_write_cal_list_processing(code_items)
                        total_processed_items += len(processed_code)
                        
                        # ë°°ì¹˜ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                        del code_items, processed_code
            
            # DB ì—°ê²° í•´ì œ
            db_handler.disconnect()
            
            return {
                'success': True,
                'processed_items': total_processed_items,
                'file_name': db_file.name
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'file_name': db_file.name
            }
    
    def optimized_batch_processing(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì¼ê´„ ì²˜ë¦¬"""
        try:
            from data_manager.db_handler_v2 import DBHandlerV2
            from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing
            
            total_processed_items = 0
            file_results = []
            
            # ì—°ê²° í’€ë§ ì‹œë®¬ë ˆì´ì…˜
            db_connections = {}
            
            for db_file in db_files:
                file_start_time = time.perf_counter()
                
                # ì—°ê²° ì¬ì‚¬ìš©
                if str(db_file) not in db_connections:
                    db_handler = DBHandlerV2(str(db_file))
                    db_handler.connect()
                    db_connections[str(db_file)] = db_handler
                else:
                    db_handler = db_connections[str(db_file)]
                
                # $ ì‹œíŠ¸ ì°¾ê¸°
                sheets = db_handler.get_sheets()
                dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
                
                file_processed_items = 0
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
                for sheet in dollar_sheets:
                    sheet_data = db_handler.get_sheet_data(sheet['id'])
                    if sheet_data:
                        # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ)
                        chunk_size = 500
                        
                        for i in range(0, len(sheet_data), chunk_size):
                            chunk_data = sheet_data[i:i+chunk_size]
                            
                            # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                            code_items = []
                            for row_data in chunk_data:
                                if len(row_data) >= 3:
                                    code_items.append([
                                        "DEFINE", "CONST", "FLOAT32",
                                        f"VAL_{row_data[0]}_{row_data[1]}", 
                                        str(row_data[2]) if row_data[2] else "",
                                        f"Generated from {sheet['name']}"
                                    ])
                            
                            # Ultra ìµœì í™”ëœ Cython ì½”ë“œ ìƒì„±
                            processed_code = ultra_fast_write_cal_list_processing(code_items)
                            file_processed_items += len(processed_code)
                            
                            # ì²­í¬ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                            del code_items, processed_code
                            
                            # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                            if i % (chunk_size * 4) == 0:
                                gc.collect()
                
                file_end_time = time.perf_counter()
                file_time = file_end_time - file_start_time
                
                file_results.append({
                    'file_name': db_file.name,
                    'execution_time': file_time,
                    'processed_items': file_processed_items
                })
                
                total_processed_items += file_processed_items
            
            # ëª¨ë“  ì—°ê²° í•´ì œ
            for db_handler in db_connections.values():
                db_handler.disconnect()
            
            return {
                'success': True,
                'total_processed_items': total_processed_items,
                'file_results': file_results,
                'files_count': len(db_files)
            }
            
        except Exception as e:
            # ì—°ê²° ì •ë¦¬
            for db_handler in db_connections.values():
                try:
                    db_handler.disconnect()
                except:
                    pass
            
            return {
                'success': False,
                'error': str(e),
                'traceback': traceback.format_exc()
            }
    
    def memory_optimized_processing(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì²˜ë¦¬"""
        print("\nğŸ§  ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
        def monitor_memory():
            memory_usage = []
            while not stop_monitoring:
                memory_info = self.process.memory_info()
                memory_usage.append({
                    'timestamp': time.time(),
                    'rss_mb': memory_info.rss / 1024 / 1024,
                    'vms_mb': memory_info.vms / 1024 / 1024
                })
                time.sleep(0.1)
            return memory_usage
        
        stop_monitoring = False
        
        # ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        monitor_thread = threading.Thread(target=monitor_memory)
        monitor_thread.start()
        
        try:
            # ìµœì í™”ëœ ì¼ê´„ ì²˜ë¦¬ ì‹¤í–‰
            result = self.measure_detailed_performance(
                self.optimized_batch_processing, 
                db_files
            )
            
            stop_monitoring = True
            monitor_thread.join()
            
            if result['success']:
                print(f"âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì²˜ë¦¬ ì™„ë£Œ")
                print(f"   ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.3f}ì´ˆ")
                print(f"   ë©”ëª¨ë¦¬ ë³€í™”: {result['memory_delta_mb']:+.1f}MB")
                print(f"   ì²˜ë¦¬ í•­ëª©: {result['result']['total_processed_items']:,}ê°œ")
            else:
                print(f"âŒ ë©”ëª¨ë¦¬ ìµœì í™” ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
            
            return result
            
        except Exception as e:
            stop_monitoring = True
            monitor_thread.join()
            return {'success': False, 'error': str(e)}
    
    def parallel_optimized_processing(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë³‘ë ¬ ìµœì í™”ëœ ì²˜ë¦¬"""
        print("\nğŸ”€ ë³‘ë ¬ ìµœì í™”ëœ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ìˆœì°¨ ì²˜ë¦¬
        sequential_result = self.measure_detailed_performance(
            self.optimized_batch_processing,
            db_files
        )
        
        # ë³‘ë ¬ ì²˜ë¦¬ (ThreadPoolExecutor ì‚¬ìš©)
        def process_db_optimized(db_file):
            return self.optimized_single_db_processing(db_file)
        
        parallel_start = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=min(len(db_files), 4)) as executor:
            parallel_results = list(executor.map(process_db_optimized, db_files))
        
        parallel_time = time.perf_counter() - parallel_start
        
        # ê²°ê³¼ ì§‘ê³„
        total_parallel_items = sum(r.get('processed_items', 0) for r in parallel_results if r['success'])
        
        if sequential_result['success']:
            speedup = sequential_result['execution_time'] / parallel_time if parallel_time > 0 else 0
            
            print(f"ğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ:")
            print(f"   ìˆœì°¨ ì²˜ë¦¬: {sequential_result['execution_time']:.3f}ì´ˆ")
            print(f"   ë³‘ë ¬ ì²˜ë¦¬: {parallel_time:.3f}ì´ˆ")
            print(f"   ì„±ëŠ¥ í–¥ìƒ: {speedup:.2f}ë°°")
            
            return {
                'sequential_time': sequential_result['execution_time'],
                'parallel_time': parallel_time,
                'speedup': speedup,
                'sequential_items': sequential_result['result']['total_processed_items'],
                'parallel_items': total_parallel_items
            }
        else:
            return {'success': False, 'error': 'Sequential processing failed'}
    
    def generate_performance_improvement_plan(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ê°œì„  ê³„íš ìƒì„±"""
        print("\nğŸ“‹ ì„±ëŠ¥ ê°œì„  ê³„íš ìƒì„±")
        print("=" * 60)
        
        improvement_plan = {
            'immediate_optimizations': [],
            'medium_term_optimizations': [],
            'long_term_optimizations': [],
            'expected_improvements': {}
        }
        
        # ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ìµœì í™”
        improvement_plan['immediate_optimizations'] = [
            {
                'name': 'Ultra Cython ëª¨ë“ˆ ì‚¬ìš©',
                'description': 'fast_write_cal_list_processing ëŒ€ì‹  ultra_fast_write_cal_list_processing ì‚¬ìš©',
                'expected_improvement': '20-30%',
                'implementation': 'from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing'
            },
            {
                'name': 'ë°°ì¹˜ í¬ê¸° ìµœì í™”',
                'description': 'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì„ ìœ„í•œ ë°°ì¹˜ í¬ê¸° ì¡°ì •',
                'expected_improvement': '15-25%',
                'implementation': 'batch_size = 500-1000 (ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)'
            },
            {
                'name': 'ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜',
                'description': 'ë©”ëª¨ë¦¬ ëˆ„ì  ë°©ì§€ë¥¼ ìœ„í•œ ì£¼ê¸°ì  gc.collect() í˜¸ì¶œ',
                'expected_improvement': 'ë©”ëª¨ë¦¬ ì•ˆì •ì„± í–¥ìƒ',
                'implementation': 'gc.collect() ë§¤ 4ë²ˆì§¸ ë°°ì¹˜ë§ˆë‹¤ í˜¸ì¶œ'
            }
        ]
        
        # ì¤‘ê¸° ìµœì í™”
        improvement_plan['medium_term_optimizations'] = [
            {
                'name': 'DB ì—°ê²° í’€ë§',
                'description': 'DB ì—°ê²° ì¬ì‚¬ìš©ìœ¼ë¡œ ì—°ê²° ì˜¤ë²„í—¤ë“œ ê°ì†Œ',
                'expected_improvement': '30-40%',
                'implementation': 'ì—°ê²° í’€ ë§¤ë‹ˆì € êµ¬í˜„'
            },
            {
                'name': 'ë³‘ë ¬ ì²˜ë¦¬ ë„ì…',
                'description': 'ThreadPoolExecutorë¥¼ ì´ìš©í•œ ë³‘ë ¬ DB ì²˜ë¦¬',
                'expected_improvement': '50-100%',
                'implementation': 'ThreadPoolExecutor(max_workers=4)'
            },
            {
                'name': 'ë©”ëª¨ë¦¬ ë§¤í•‘ I/O',
                'description': 'ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ë§¤í•‘ ì‚¬ìš©',
                'expected_improvement': '20-30%',
                'implementation': 'mmap ëª¨ë“ˆ í™œìš©'
            }
        ]
        
        # ì¥ê¸° ìµœì í™”
        improvement_plan['long_term_optimizations'] = [
            {
                'name': 'ë¹„ë™ê¸° ì²˜ë¦¬ ì•„í‚¤í…ì²˜',
                'description': 'asyncioë¥¼ ì´ìš©í•œ ë¹„ë™ê¸° DB ì²˜ë¦¬',
                'expected_improvement': '100-200%',
                'implementation': 'asyncio + aiosqlite ë„ì…'
            },
            {
                'name': 'ë¶„ì‚° ì²˜ë¦¬',
                'description': 'ë‹¤ì¤‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´ìš©í•œ ë¶„ì‚° ì²˜ë¦¬',
                'expected_improvement': '200-400%',
                'implementation': 'multiprocessing.Pool í™œìš©'
            },
            {
                'name': 'ìºì‹± ì‹œìŠ¤í…œ',
                'description': 'ì¤‘ê°„ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€',
                'expected_improvement': '50-100%',
                'implementation': 'Redis ë˜ëŠ” ë©”ëª¨ë¦¬ ìºì‹œ êµ¬í˜„'
            }
        ]
        
        # ì˜ˆìƒ ê°œì„  íš¨ê³¼
        improvement_plan['expected_improvements'] = {
            'immediate': '50-80% ì„±ëŠ¥ í–¥ìƒ',
            'medium_term': '100-200% ì„±ëŠ¥ í–¥ìƒ',
            'long_term': '300-500% ì„±ëŠ¥ í–¥ìƒ'
        }
        
        return improvement_plan

if __name__ == "__main__":
    print("ğŸ” DB â†’ C ì½”ë“œ ë³€í™˜ ì„±ëŠ¥ ë¬¸ì œ ì¢…í•© í•´ê²° ë°©ì•ˆ")
    print("=" * 80)
    
    solution = ComprehensivePerformanceSolution()
    
    # ê¸°ì¡´ DB íŒŒì¼ë“¤ ìˆ˜ì§‘
    db_files = solution.get_existing_db_files()
    
    if not db_files:
        print("âŒ ë¶„ì„í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    print(f"ğŸ“ ë¶„ì„ ëŒ€ìƒ DB íŒŒì¼: {len(db_files)}ê°œ")
    for db_file in db_files:
        size_mb = db_file.stat().st_size / 1024 / 1024
        print(f"   - {db_file.name} ({size_mb:.1f}MB)")
    
    analysis_results = {}
    
    # 1. ë©”ëª¨ë¦¬ ìµœì í™”ëœ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    memory_result = solution.memory_optimized_processing(db_files)
    analysis_results['memory_optimized'] = memory_result
    
    # 2. ë³‘ë ¬ ìµœì í™”ëœ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    parallel_result = solution.parallel_optimized_processing(db_files)
    analysis_results['parallel_optimized'] = parallel_result
    
    # 3. ì„±ëŠ¥ ê°œì„  ê³„íš ìƒì„±
    improvement_plan = solution.generate_performance_improvement_plan(analysis_results)
    analysis_results['improvement_plan'] = improvement_plan
    
    # ê²°ê³¼ ì €ì¥
    with open('comprehensive_performance_solution.json', 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ì¢…í•© í•´ê²° ë°©ì•ˆì´ 'comprehensive_performance_solution.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 80)
