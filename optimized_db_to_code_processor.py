"""
ìµœì í™”ëœ DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ
ëª¨ë“  ì„±ëŠ¥ ê°œì„  ê¸°ë²•ì„ ì ìš©í•œ ìµœì¢… ì†”ë£¨ì…˜
"""

import time
import gc
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import threading
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

@dataclass
class ProcessingConfig:
    """ì²˜ë¦¬ ì„¤ì •"""
    batch_size: int = 500
    chunk_size: int = 1000
    max_workers: int = 4
    gc_interval: int = 4
    enable_connection_pooling: bool = True
    enable_parallel_processing: bool = True
    enable_memory_optimization: bool = True

class ConnectionPool:
    """DB ì—°ê²° í’€"""
    
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self.connections = {}
        self.lock = threading.Lock()
    
    def get_connection(self, db_path: str):
        """ì—°ê²° ê°€ì ¸ì˜¤ê¸°"""
        with self.lock:
            if db_path not in self.connections:
                from data_manager.db_handler_v2 import DBHandlerV2
                db_handler = DBHandlerV2(db_path)
                db_handler.connect()
                self.connections[db_path] = db_handler
            return self.connections[db_path]
    
    def close_all(self):
        """ëª¨ë“  ì—°ê²° í•´ì œ"""
        with self.lock:
            for db_handler in self.connections.values():
                try:
                    db_handler.disconnect()
                except:
                    pass
            self.connections.clear()

class OptimizedDBToCodeProcessor:
    """ìµœì í™”ëœ DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, config: ProcessingConfig = None):
        self.config = config or ProcessingConfig()
        self.connection_pool = ConnectionPool() if self.config.enable_connection_pooling else None
        self.stats = {
            'total_files_processed': 0,
            'total_items_processed': 0,
            'total_execution_time': 0,
            'memory_usage_peak': 0
        }
    
    def process_single_db_optimized(self, db_file: Path) -> Dict[str, Any]:
        """ìµœì í™”ëœ ë‹¨ì¼ DB ì²˜ë¦¬"""
        start_time = time.perf_counter()
        
        try:
            # ì—°ê²° í’€ ì‚¬ìš© ë˜ëŠ” ì§ì ‘ ì—°ê²°
            if self.connection_pool:
                db_handler = self.connection_pool.get_connection(str(db_file))
            else:
                from data_manager.db_handler_v2 import DBHandlerV2
                db_handler = DBHandlerV2(str(db_file))
                db_handler.connect()
            
            # $ ì‹œíŠ¸ ì°¾ê¸°
            sheets = db_handler.get_sheets()
            dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
            
            total_processed_items = 0
            
            # Ultra ìµœì í™”ëœ Cython ëª¨ë“ˆ ì‚¬ìš©
            from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
            for sheet_idx, sheet in enumerate(dollar_sheets):
                sheet_data = db_handler.get_sheet_data(sheet['id'])
                if not sheet_data:
                    continue
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
                for chunk_start in range(0, len(sheet_data), self.config.chunk_size):
                    chunk_end = min(chunk_start + self.config.chunk_size, len(sheet_data))
                    chunk_data = sheet_data[chunk_start:chunk_end]
                    
                    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½”ë“œ ìƒì„±
                    for batch_start in range(0, len(chunk_data), self.config.batch_size):
                        batch_end = min(batch_start + self.config.batch_size, len(chunk_data))
                        batch_data = chunk_data[batch_start:batch_end]
                        
                        # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                        code_items = []
                        for row_data in batch_data:
                            if len(row_data) >= 3:
                                code_items.append([
                                    "DEFINE", "CONST", "FLOAT32",
                                    f"VAL_{row_data[0]}_{row_data[1]}", 
                                    str(row_data[2]) if row_data[2] else "",
                                    f"Generated from {sheet['name']}"
                                ])
                        
                        # Ultra ìµœì í™”ëœ Cython ì½”ë“œ ìƒì„±
                        if code_items:
                            processed_code = ultra_fast_write_cal_list_processing(code_items)
                            total_processed_items += len(processed_code)
                        
                        # ë°°ì¹˜ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                        del code_items
                        if 'processed_code' in locals():
                            del processed_code
                        
                        # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                        if (batch_start // self.config.batch_size) % self.config.gc_interval == 0:
                            gc.collect()
            
            # ì—°ê²° í’€ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì—°ê²° í•´ì œ
            if not self.connection_pool:
                db_handler.disconnect()
            
            execution_time = time.perf_counter() - start_time
            
            return {
                'success': True,
                'file_name': db_file.name,
                'execution_time': execution_time,
                'processed_items': total_processed_items,
                'sheets_processed': len(dollar_sheets)
            }
            
        except Exception as e:
            execution_time = time.perf_counter() - start_time
            return {
                'success': False,
                'file_name': db_file.name,
                'execution_time': execution_time,
                'error': str(e)
            }
    
    def process_batch_sequential(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìˆœì°¨ ì¼ê´„ ì²˜ë¦¬"""
        start_time = time.perf_counter()
        
        results = []
        total_processed_items = 0
        
        for db_file in db_files:
            result = self.process_single_db_optimized(db_file)
            results.append(result)
            
            if result['success']:
                total_processed_items += result['processed_items']
        
        execution_time = time.perf_counter() - start_time
        
        return {
            'method': 'sequential',
            'execution_time': execution_time,
            'total_processed_items': total_processed_items,
            'files_processed': len([r for r in results if r['success']]),
            'results': results
        }
    
    def process_batch_parallel(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë³‘ë ¬ ì¼ê´„ ì²˜ë¦¬"""
        start_time = time.perf_counter()
        
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            results = list(executor.map(self.process_single_db_optimized, db_files))
        
        execution_time = time.perf_counter() - start_time
        
        total_processed_items = sum(r.get('processed_items', 0) for r in results if r['success'])
        
        return {
            'method': 'parallel',
            'execution_time': execution_time,
            'total_processed_items': total_processed_items,
            'files_processed': len([r for r in results if r['success']]),
            'max_workers': self.config.max_workers,
            'results': results
        }
    
    def process_batch_optimized(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì¼ê´„ ì²˜ë¦¬ (ìë™ ì„ íƒ)"""
        if self.config.enable_parallel_processing and len(db_files) > 1:
            return self.process_batch_parallel(db_files)
        else:
            return self.process_batch_sequential(db_files)
    
    def benchmark_processing_methods(self, db_files: List[Path]) -> Dict[str, Any]:
        """ì²˜ë¦¬ ë°©ë²•ë³„ ë²¤ì¹˜ë§ˆí¬"""
        print("\nğŸ ì²˜ë¦¬ ë°©ë²•ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬")
        print("=" * 60)
        
        benchmark_results = {}
        
        # 1. ê¸°ì¡´ ë°©ì‹ (ì°¸ì¡°ìš©)
        print("ğŸ”„ ê¸°ì¡´ ë°©ì‹ ì¸¡ì •")
        original_result = self.measure_original_processing(db_files)
        benchmark_results['original'] = original_result
        
        # 2. ìˆœì°¨ ìµœì í™” ì²˜ë¦¬
        print("ğŸ”„ ìˆœì°¨ ìµœì í™” ì²˜ë¦¬ ì¸¡ì •")
        self.config.enable_parallel_processing = False
        sequential_result = self.process_batch_optimized(db_files)
        benchmark_results['sequential_optimized'] = sequential_result
        
        # 3. ë³‘ë ¬ ìµœì í™” ì²˜ë¦¬
        print("ğŸ”„ ë³‘ë ¬ ìµœì í™” ì²˜ë¦¬ ì¸¡ì •")
        self.config.enable_parallel_processing = True
        parallel_result = self.process_batch_optimized(db_files)
        benchmark_results['parallel_optimized'] = parallel_result
        
        # ì„±ëŠ¥ ë¹„êµ
        print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
        
        if original_result['success']:
            original_time = original_result['execution_time']
            sequential_time = sequential_result['execution_time']
            parallel_time = parallel_result['execution_time']
            
            sequential_speedup = original_time / sequential_time if sequential_time > 0 else 0
            parallel_speedup = original_time / parallel_time if parallel_time > 0 else 0
            
            print(f"   ê¸°ì¡´ ë°©ì‹:      {original_time:.3f}ì´ˆ")
            print(f"   ìˆœì°¨ ìµœì í™”:    {sequential_time:.3f}ì´ˆ ({sequential_speedup:.2f}ë°° ë¹ ë¦„)")
            print(f"   ë³‘ë ¬ ìµœì í™”:    {parallel_time:.3f}ì´ˆ ({parallel_speedup:.2f}ë°° ë¹ ë¦„)")
            
            # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
            target_improvement = 0.5  # 50% ë‹¨ì¶• ëª©í‘œ
            sequential_improvement = (original_time - sequential_time) / original_time
            parallel_improvement = (original_time - parallel_time) / original_time
            
            print(f"\nğŸ¯ ì„±ëŠ¥ ê°œì„  ëª©í‘œ ë‹¬ì„± ì—¬ë¶€:")
            print(f"   ëª©í‘œ: {target_improvement*100:.0f}% ë‹¨ì¶•")
            print(f"   ìˆœì°¨ ìµœì í™”: {sequential_improvement*100:.1f}% ë‹¨ì¶• {'âœ…' if sequential_improvement >= target_improvement else 'âŒ'}")
            print(f"   ë³‘ë ¬ ìµœì í™”: {parallel_improvement*100:.1f}% ë‹¨ì¶• {'âœ…' if parallel_improvement >= target_improvement else 'âŒ'}")
        
        return benchmark_results
    
    def measure_original_processing(self, db_files: List[Path]) -> Dict[str, Any]:
        """ê¸°ì¡´ ë°©ì‹ ì„±ëŠ¥ ì¸¡ì • (ì°¸ì¡°ìš©)"""
        start_time = time.perf_counter()
        
        try:
            from data_manager.db_handler_v2 import DBHandlerV2
            from cython_extensions.code_generator_v2 import fast_write_cal_list_processing
            
            total_processed_items = 0
            
            for db_file in db_files:
                # ë§¤ë²ˆ ìƒˆë¡œìš´ ì—°ê²°
                db_handler = DBHandlerV2(str(db_file))
                db_handler.connect()
                
                sheets = db_handler.get_sheets()
                dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
                
                for sheet in dollar_sheets:
                    sheet_data = db_handler.get_sheet_data(sheet['id'])
                    if sheet_data:
                        # ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
                        code_items = []
                        for row_data in sheet_data:
                            if len(row_data) >= 3:
                                code_items.append([
                                    "DEFINE", "CONST", "FLOAT32",
                                    f"VAL_{row_data[0]}_{row_data[1]}", 
                                    str(row_data[2]) if row_data[2] else "",
                                    f"Generated from {sheet['name']}"
                                ])
                        
                        # ê¸°ë³¸ Cython ì½”ë“œ ìƒì„±
                        processed_code = fast_write_cal_list_processing(code_items)
                        total_processed_items += len(processed_code)
                
                db_handler.disconnect()
            
            execution_time = time.perf_counter() - start_time
            
            return {
                'success': True,
                'execution_time': execution_time,
                'total_processed_items': total_processed_items,
                'method': 'original'
            }
            
        except Exception as e:
            execution_time = time.perf_counter() - start_time
            return {
                'success': False,
                'execution_time': execution_time,
                'error': str(e),
                'method': 'original'
            }
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.connection_pool:
            self.connection_pool.close_all()
        gc.collect()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ìµœì í™”ëœ DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ")
    print("=" * 80)
    
    # DB íŒŒì¼ ìˆ˜ì§‘
    db_dir = Path('database')
    if not db_dir.exists():
        print("âŒ Database ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000]  # 50KB ì´ìƒ
    db_files = sorted(db_files, key=lambda x: x.stat().st_size, reverse=True)[:5]  # í° íŒŒì¼ 5ê°œ
    
    if not db_files:
        print("âŒ ë¶„ì„í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ì²˜ë¦¬ ëŒ€ìƒ DB íŒŒì¼: {len(db_files)}ê°œ")
    for db_file in db_files:
        size_mb = db_file.stat().st_size / 1024 / 1024
        print(f"   - {db_file.name} ({size_mb:.1f}MB)")
    
    # ìµœì í™”ëœ í”„ë¡œì„¸ì„œ ìƒì„±
    config = ProcessingConfig(
        batch_size=500,
        chunk_size=1000,
        max_workers=4,
        gc_interval=4,
        enable_connection_pooling=True,
        enable_parallel_processing=True,
        enable_memory_optimization=True
    )
    
    processor = OptimizedDBToCodeProcessor(config)
    
    try:
        # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        benchmark_results = processor.benchmark_processing_methods(db_files)
        
        # ê²°ê³¼ ì €ì¥
        import json
        with open('optimized_db_processing_results.json', 'w', encoding='utf-8') as f:
            json.dump(benchmark_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ìµœì í™” ê²°ê³¼ê°€ 'optimized_db_processing_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    finally:
        processor.cleanup()
    
    print("=" * 80)

if __name__ == "__main__":
    main()
