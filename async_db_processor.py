"""
Phase 3: ë¹„ë™ê¸° DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ
asyncio + aiosqliteë¥¼ ì´ìš©í•œ ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬í˜„
"""

import asyncio
import aiosqlite
import time
import gc
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, AsyncGenerator
from dataclasses import dataclass
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

@dataclass
class AsyncConfig:
    """ë¹„ë™ê¸° ì²˜ë¦¬ ì„¤ì •"""
    batch_size: int = 500
    chunk_size: int = 1000
    gc_interval: int = 4
    max_concurrent_dbs: int = 8
    max_concurrent_sheets: int = 16
    connection_timeout: float = 30.0
    query_timeout: float = 10.0
    enable_connection_pooling: bool = True
    pool_size: int = 20

class AsyncConnectionPool:
    """ë¹„ë™ê¸° DB ì—°ê²° í’€"""
    
    def __init__(self, max_connections: int = 20):
        self.max_connections = max_connections
        self.connections = {}
        self.semaphore = asyncio.Semaphore(max_connections)
        self.logger = logging.getLogger(__name__)
    
    async def get_connection(self, db_path: str) -> aiosqlite.Connection:
        """ë¹„ë™ê¸° ì—°ê²° ê°€ì ¸ì˜¤ê¸°"""
        async with self.semaphore:
            if db_path not in self.connections:
                try:
                    conn = await aiosqlite.connect(db_path)
                    # SQLite ìµœì í™” ì„¤ì •
                    await conn.execute("PRAGMA journal_mode=WAL")
                    await conn.execute("PRAGMA synchronous=NORMAL")
                    await conn.execute("PRAGMA cache_size=10000")
                    await conn.execute("PRAGMA temp_store=MEMORY")
                    
                    self.connections[db_path] = conn
                    self.logger.info(f"ë¹„ë™ê¸° DB ì—°ê²° ìƒì„±: {Path(db_path).name}")
                except Exception as e:
                    self.logger.error(f"ë¹„ë™ê¸° DB ì—°ê²° ì‹¤íŒ¨ {db_path}: {e}")
                    raise
            
            return self.connections[db_path]
    
    async def close_all(self):
        """ëª¨ë“  ì—°ê²° í•´ì œ"""
        for db_path, conn in self.connections.items():
            try:
                await conn.close()
                self.logger.info(f"ë¹„ë™ê¸° DB ì—°ê²° í•´ì œ: {Path(db_path).name}")
            except Exception as e:
                self.logger.error(f"ë¹„ë™ê¸° DB ì—°ê²° í•´ì œ ì‹¤íŒ¨ {db_path}: {e}")
        
        self.connections.clear()

class AsyncDBProcessor:
    """ë¹„ë™ê¸° DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, config: AsyncConfig = None):
        self.config = config or AsyncConfig()
        self.connection_pool = AsyncConnectionPool(self.config.pool_size) if self.config.enable_connection_pooling else None
        self.logger = logging.getLogger(__name__)
        self.stats = {
            'total_files_processed': 0,
            'total_items_processed': 0,
            'total_execution_time': 0,
            'concurrent_operations': 0
        }
    
    async def get_sheets_async(self, conn: aiosqlite.Connection) -> List[Dict[str, Any]]:
        """ë¹„ë™ê¸° ì‹œíŠ¸ ëª©ë¡ ì¡°íšŒ"""
        query = """
        SELECT id, name, source_file, is_dollar_sheet 
        FROM sheets 
        WHERE is_dollar_sheet = 1
        ORDER BY id
        """
        
        async with conn.execute(query) as cursor:
            rows = await cursor.fetchall()
            
        return [
            {
                'id': row[0],
                'name': row[1],
                'source_file': row[2],
                'is_dollar_sheet': bool(row[3])
            }
            for row in rows
        ]
    
    async def get_sheet_data_async(self, conn: aiosqlite.Connection, sheet_id: int) -> List[tuple]:
        """ë¹„ë™ê¸° ì‹œíŠ¸ ë°ì´í„° ì¡°íšŒ"""
        query = """
        SELECT row, col, value
        FROM cells
        WHERE sheet_id = ?
        ORDER BY row, col
        """

        async with conn.execute(query, (sheet_id,)) as cursor:
            rows = await cursor.fetchall()

        return rows
    
    async def process_sheet_async(self, conn: aiosqlite.Connection, sheet: Dict[str, Any]) -> int:
        """ë¹„ë™ê¸° ì‹œíŠ¸ ì²˜ë¦¬"""
        try:
            sheet_data = await self.get_sheet_data_async(conn, sheet['id'])
            if not sheet_data:
                return 0
            
            self.logger.debug(f"ë¹„ë™ê¸° ì‹œíŠ¸ ì²˜ë¦¬: {sheet['name']} ({len(sheet_data)}ê°œ ì…€)")
            
            # Ultra ìµœì í™”ëœ Cython ëª¨ë“ˆ ì‚¬ìš©
            from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing
            
            total_processed_items = 0
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
            for chunk_start in range(0, len(sheet_data), self.config.chunk_size):
                chunk_end = min(chunk_start + self.config.chunk_size, len(sheet_data))
                chunk_data = sheet_data[chunk_start:chunk_end]
                
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½”ë“œ ìƒì„±
                batch_count = 0
                for batch_start in range(0, len(chunk_data), self.config.batch_size):
                    batch_end = min(batch_start + self.config.batch_size, len(chunk_data))
                    batch_data = chunk_data[batch_start:batch_end]
                    
                    # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                    code_items = []
                    for row_data in batch_data:
                        if len(row_data) >= 3:
                            code_items.append([
                                "DEFINE", "CONST", "FLOAT32",
                                f"VAL_{row_data[0]}_{row_data[1]}", 
                                str(row_data[2]) if row_data[2] else "",
                                f"Generated from {sheet['name']}"
                            ])
                    
                    # Ultra ìµœì í™”ëœ Cython ì½”ë“œ ìƒì„±
                    if code_items:
                        processed_code = ultra_fast_write_cal_list_processing(code_items)
                        total_processed_items += len(processed_code)
                    
                    # ë°°ì¹˜ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    del code_items
                    if 'processed_code' in locals():
                        del processed_code
                    
                    batch_count += 1
                    
                    # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    if batch_count % self.config.gc_interval == 0:
                        gc.collect()
                    
                    # ë¹„ë™ê¸° ì–‘ë³´ (ë‹¤ë¥¸ ì½”ë£¨í‹´ì—ê²Œ ì‹¤í–‰ ê¸°íšŒ ì œê³µ)
                    if batch_count % 10 == 0:
                        await asyncio.sleep(0)
            
            return total_processed_items
            
        except Exception as e:
            self.logger.error(f"ë¹„ë™ê¸° ì‹œíŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ {sheet['name']}: {e}")
            return 0
    
    async def process_single_db_async(self, db_file: Path) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ë‹¨ì¼ DB ì²˜ë¦¬"""
        start_time = time.perf_counter()
        file_name = db_file.name
        
        try:
            self.logger.info(f"ë¹„ë™ê¸° DB ì²˜ë¦¬ ì‹œì‘: {file_name}")
            
            # ë¹„ë™ê¸° ì—°ê²° í’€ ì‚¬ìš©
            if self.connection_pool:
                conn = await self.connection_pool.get_connection(str(db_file))
            else:
                conn = await aiosqlite.connect(str(db_file))
            
            # $ ì‹œíŠ¸ ì°¾ê¸°
            sheets = await self.get_sheets_async(conn)
            
            if not sheets:
                self.logger.warning(f"$ ì‹œíŠ¸ê°€ ì—†ìŒ: {file_name}")
                return {
                    'success': True,
                    'file_name': file_name,
                    'execution_time': time.perf_counter() - start_time,
                    'processed_items': 0,
                    'warning': 'No dollar sheets found'
                }
            
            # ì‹œíŠ¸ë“¤ì„ ë™ì‹œì— ë¹„ë™ê¸° ì²˜ë¦¬
            semaphore = asyncio.Semaphore(self.config.max_concurrent_sheets)
            
            async def process_sheet_with_semaphore(sheet):
                async with semaphore:
                    return await self.process_sheet_async(conn, sheet)
            
            # ëª¨ë“  ì‹œíŠ¸ë¥¼ ë™ì‹œì— ì²˜ë¦¬
            tasks = [process_sheet_with_semaphore(sheet) for sheet in sheets]
            sheet_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì§‘ê³„
            total_processed_items = 0
            for result in sheet_results:
                if isinstance(result, int):
                    total_processed_items += result
                else:
                    self.logger.error(f"ì‹œíŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {result}")
            
            # ì—°ê²° í’€ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì—°ê²° í•´ì œ
            if not self.connection_pool:
                await conn.close()
            
            execution_time = time.perf_counter() - start_time
            
            self.logger.info(f"ë¹„ë™ê¸° DB ì²˜ë¦¬ ì™„ë£Œ: {file_name} ({execution_time:.3f}ì´ˆ, {total_processed_items:,}ê°œ í•­ëª©)")
            
            return {
                'success': True,
                'file_name': file_name,
                'execution_time': execution_time,
                'processed_items': total_processed_items,
                'sheets_processed': len(sheets)
            }
            
        except Exception as e:
            execution_time = time.perf_counter() - start_time
            error_msg = f"ë¹„ë™ê¸° DB ì²˜ë¦¬ ì‹¤íŒ¨: {file_name} - {str(e)}"
            self.logger.error(error_msg)
            
            return {
                'success': False,
                'file_name': file_name,
                'execution_time': execution_time,
                'error': str(e)
            }
    
    async def process_batch_async(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì¼ê´„ ì²˜ë¦¬"""
        start_time = time.perf_counter()
        
        self.logger.info(f"ë¹„ë™ê¸° ì¼ê´„ ì²˜ë¦¬ ì‹œì‘: {len(db_files)}ê°œ íŒŒì¼")
        
        # DB íŒŒì¼ë“¤ì„ ë™ì‹œì— ë¹„ë™ê¸° ì²˜ë¦¬
        semaphore = asyncio.Semaphore(self.config.max_concurrent_dbs)
        
        async def process_db_with_semaphore(db_file):
            async with semaphore:
                return await self.process_single_db_async(db_file)
        
        # ëª¨ë“  DB íŒŒì¼ì„ ë™ì‹œì— ì²˜ë¦¬
        tasks = [process_db_with_semaphore(db_file) for db_file in db_files]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for result in results:
            if isinstance(result, dict):
                processed_results.append(result)
            else:
                self.logger.error(f"DB ì²˜ë¦¬ ì˜¤ë¥˜: {result}")
                processed_results.append({
                    'success': False,
                    'error': str(result)
                })
        
        execution_time = time.perf_counter() - start_time
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        successful_results = [r for r in processed_results if r['success']]
        total_processed_items = sum(r.get('processed_items', 0) for r in successful_results)
        
        self.stats['total_files_processed'] += len(successful_results)
        self.stats['total_items_processed'] += total_processed_items
        self.stats['total_execution_time'] += execution_time
        
        self.logger.info(f"ë¹„ë™ê¸° ì¼ê´„ ì²˜ë¦¬ ì™„ë£Œ: {execution_time:.3f}ì´ˆ, {total_processed_items:,}ê°œ í•­ëª©")
        
        return {
            'success': True,
            'execution_time': execution_time,
            'total_processed_items': total_processed_items,
            'files_processed': len(successful_results),
            'files_failed': len(processed_results) - len(successful_results),
            'results': processed_results,
            'processing_mode': 'async'
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ë¹„ë™ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘")
        
        if self.connection_pool:
            await self.connection_pool.close_all()
        
        gc.collect()
        self.logger.info("ë¹„ë™ê¸° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

async def main():
    """ë¹„ë™ê¸° ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Phase 3: ë¹„ë™ê¸° DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ")
    print("=" * 80)
    
    # ì„¤ì •
    config = AsyncConfig(
        batch_size=500,
        chunk_size=1000,
        gc_interval=4,
        max_concurrent_dbs=8,
        max_concurrent_sheets=16,
        enable_connection_pooling=True,
        pool_size=20
    )
    
    # DB íŒŒì¼ ìˆ˜ì§‘
    db_dir = Path('database')
    if not db_dir.exists():
        print("âŒ Database ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000]
    
    if not db_files:
        print("âŒ ì²˜ë¦¬í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ì²˜ë¦¬ ëŒ€ìƒ: {len(db_files)}ê°œ íŒŒì¼")
    
    # ë¹„ë™ê¸° í”„ë¡œì„¸ì„œ ìƒì„± ë° ì‹¤í–‰
    processor = AsyncDBProcessor(config)
    
    try:
        # ë¹„ë™ê¸° ì¼ê´„ ì²˜ë¦¬ ì‹¤í–‰
        result = await processor.process_batch_async(db_files)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ë¹„ë™ê¸° ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.3f}ì´ˆ")
        print(f"   ì²˜ë¦¬ í•­ëª©: {result['total_processed_items']:,}ê°œ")
        print(f"   ì„±ê³µ íŒŒì¼: {result['files_processed']}ê°œ")
        print(f"   ì‹¤íŒ¨ íŒŒì¼: {result['files_failed']}ê°œ")
        print(f"   ì²˜ë¦¬ ëª¨ë“œ: {result['processing_mode']}")
        
        # ì²˜ë¦¬ ì†ë„ ê³„ì‚°
        if result['execution_time'] > 0:
            items_per_second = result['total_processed_items'] / result['execution_time']
            print(f"   ì²˜ë¦¬ ì†ë„: {items_per_second:,.0f} í•­ëª©/ì´ˆ")
        
        # ê²°ê³¼ ì €ì¥
        with open('async_processing_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'result': result,
                'config': config.__dict__
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ê²°ê³¼ê°€ 'async_processing_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë¹„ë™ê¸° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        logging.error(f"ë¹„ë™ê¸° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    finally:
        await processor.cleanup()
    
    print("=" * 80)

if __name__ == "__main__":
    # aiosqlite ì„¤ì¹˜ í™•ì¸
    try:
        import aiosqlite
    except ImportError:
        print("âŒ aiosqliteê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ëª…ë ¹: pip install aiosqlite")
        sys.exit(1)
    
    asyncio.run(main())
