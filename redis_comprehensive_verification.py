"""
Redis ìºì‹± ì‹œìŠ¤í…œ ì¢…í•© ê²€ì¦
main.py ì‹¤í–‰ í™˜ê²½ì—ì„œì˜ ì™„ì „í•œ ì‘ë™ ìƒíƒœ í™•ì¸
"""

import time
import threading
import hashlib
import pickle
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
import json
from concurrent.futures import ThreadPoolExecutor

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

class RedisComprehensiveVerifier:
    """Redis ìºì‹± ì‹œìŠ¤í…œ ì¢…í•© ê²€ì¦ê¸°"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.verification_results = {}

    def test_redis_server_connection(self) -> Dict[str, Any]:
        """Redis ì„œë²„ ì—°ê²° ìƒíƒœ ì‹¤ì œ í…ŒìŠ¤íŠ¸"""
        self.logger.info("1. Redis ì„œë²„ ì—°ê²° ìƒíƒœ ê²€ì¦")

        test_results = {
            'redis_module_available': False,
            'redis_server_running': False,
            'basic_operations_working': False,
            'connection_details': {},
            'error_details': None
        }

        try:
            # Redis ëª¨ë“ˆ ì„¤ì¹˜ í™•ì¸
            import redis
            test_results['redis_module_available'] = True
            self.logger.info("âœ… Redis ëª¨ë“ˆ ì„¤ì¹˜ë¨")

            # Redis ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
            redis_client = redis.Redis(
                host='localhost',
                port=6379,
                db=0,
                decode_responses=False,
                socket_connect_timeout=5,
                socket_timeout=5
            )

            # ì—°ê²° í…ŒìŠ¤íŠ¸
            response = redis_client.ping()
            if response:
                test_results['redis_server_running'] = True
                self.logger.info("âœ… Redis ì„œë²„ ì—°ê²° ì„±ê³µ")

                # ì„œë²„ ì •ë³´ ìˆ˜ì§‘
                info = redis_client.info()
                test_results['connection_details'] = {
                    'redis_version': info.get('redis_version', 'unknown'),
                    'used_memory': info.get('used_memory_human', 'unknown'),
                    'connected_clients': info.get('connected_clients', 0),
                    'total_commands_processed': info.get('total_commands_processed', 0)
                }

                # ê¸°ë³¸ ì‘ì—… í…ŒìŠ¤íŠ¸
                test_key = 'verification_test_key'
                test_value = {
                    'test_data': 'Redis ê²€ì¦ í…ŒìŠ¤íŠ¸',
                    'timestamp': time.time(),
                    'complex_data': [1, 2, {'nested': 'value'}]
                }

                # SET ì‘ì—…
                redis_client.set(test_key, pickle.dumps(test_value), ex=60)

                # GET ì‘ì—…
                retrieved_data = redis_client.get(test_key)
                if retrieved_data:
                    unpickled_data = pickle.loads(retrieved_data)
                    if unpickled_data == test_value:
                        test_results['basic_operations_working'] = True
                        self.logger.info("âœ… Redis ê¸°ë³¸ ì‘ì—… ì„±ê³µ")

                # ì •ë¦¬
                redis_client.delete(test_key)

            else:
                test_results['error_details'] = "Redis ping ì‹¤íŒ¨"

        except ImportError:
            test_results['error_details'] = "Redis ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ"
            self.logger.warning("âš ï¸ Redis ëª¨ë“ˆ ë¯¸ì„¤ì¹˜")
        except redis.ConnectionError as e:
            test_results['error_details'] = f"Redis ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}"
            self.logger.warning(f"âš ï¸ Redis ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        except Exception as e:
            test_results['error_details'] = f"Redis í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}"
            self.logger.error(f"âŒ Redis í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")

        return test_results

    def test_hierarchical_caching(self) -> Dict[str, Any]:
        """ê³„ì¸µì  ìºì‹± (Redis + ë©”ëª¨ë¦¬) ì‹¤ì œ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        self.logger.info("2. ê³„ì¸µì  ìºì‹± ì‹œìŠ¤í…œ ê²€ì¦")

        from cached_db_processor import CachedDBProcessor, CacheConfig

        # Redis + ë©”ëª¨ë¦¬ ìºì‹œ ì„¤ì •
        config_with_redis = CacheConfig(
            enable_redis_cache=True,
            enable_memory_cache=True,
            memory_cache_size=100,
            redis_host='localhost',
            redis_port=6379
        )

        test_results = {
            'redis_cache_working': False,
            'memory_cache_working': False,
            'hierarchical_flow': False,
            'cache_statistics': {}
        }

        try:
            processor = CachedDBProcessor(config_with_redis)

            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            test_key = "hierarchical_test_key"
            test_value = [["DEFINE", "CONST", "FLOAT32", "TEST_VAL", "1.0", "Hierarchical Test"]]

            # 1. ìºì‹œì— ì €ì¥ (Redis + ë©”ëª¨ë¦¬)
            processor.set_to_cache(test_key, test_value)

            # 2. ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ì¡°íšŒ (ë¹ ë¥¸ ê²½ë¡œ)
            memory_result = processor.get_from_cache(test_key)
            if memory_result == test_value:
                test_results['memory_cache_working'] = True
                self.logger.info("âœ… ë©”ëª¨ë¦¬ ìºì‹œ ì‘ë™")

            # 3. ë©”ëª¨ë¦¬ ìºì‹œ í´ë¦¬ì–´ í›„ Redisì—ì„œ ì¡°íšŒ
            if processor.memory_cache:
                processor.memory_cache.clear()

            redis_result = processor.get_from_cache(test_key)
            if redis_result == test_value:
                test_results['redis_cache_working'] = True
                test_results['hierarchical_flow'] = True
                self.logger.info("âœ… Redis ìºì‹œ ì‘ë™ ë° ê³„ì¸µì  íë¦„ í™•ì¸")

            # ìºì‹œ í†µê³„ ìˆ˜ì§‘
            cache_stats = processor.get_cache_stats()
            test_results['cache_statistics'] = cache_stats

            processor.cleanup()

        except Exception as e:
            test_results['error'] = str(e)
            self.logger.error(f"âŒ ê³„ì¸µì  ìºì‹± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        return test_results

    def test_fallback_mechanism(self) -> Dict[str, Any]:
        """Fallback ë©”ì»¤ë‹ˆì¦˜ ì‹¤ì œ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        self.logger.info("3. Fallback ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦")

        from cached_db_processor import CachedDBProcessor, CacheConfig

        # ì˜ëª»ëœ Redis ì„¤ì • (ì˜ë„ì  ì‹¤íŒ¨)
        config_fallback = CacheConfig(
            enable_redis_cache=True,
            enable_memory_cache=True,
            memory_cache_size=100,
            redis_host='nonexistent_host',  # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í˜¸ìŠ¤íŠ¸
            redis_port=9999  # ì˜ëª»ëœ í¬íŠ¸
        )

        test_results = {
            'redis_connection_failed': False,
            'fallback_to_memory': False,
            'data_consistency': False,
            'graceful_degradation': False
        }

        try:
            processor = CachedDBProcessor(config_fallback)

            # Redis ì—°ê²° ì‹¤íŒ¨ í™•ì¸
            if processor.redis_cache and not processor.redis_cache.redis_client:
                test_results['redis_connection_failed'] = True
                self.logger.info("âœ… Redis ì—°ê²° ì‹¤íŒ¨ ê°ì§€ë¨")

            # ë©”ëª¨ë¦¬ ìºì‹œë¡œ fallback í…ŒìŠ¤íŠ¸
            test_key = "fallback_test_key"
            test_value = [["DEFINE", "CONST", "INT32", "FALLBACK_VAL", "42", "Fallback Test"]]

            # ì €ì¥ ë° ì¡°íšŒ í…ŒìŠ¤íŠ¸
            processor.set_to_cache(test_key, test_value)
            retrieved_value = processor.get_from_cache(test_key)

            if retrieved_value == test_value:
                test_results['fallback_to_memory'] = True
                test_results['data_consistency'] = True
                test_results['graceful_degradation'] = True
                self.logger.info("âœ… Fallback ë©”ì»¤ë‹ˆì¦˜ ì •ìƒ ì‘ë™")

            processor.cleanup()

        except Exception as e:
            test_results['error'] = str(e)
            self.logger.error(f"âŒ Fallback í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        return test_results

    def test_cache_key_generation(self) -> Dict[str, Any]:
        """ìºì‹œ í‚¤ ìƒì„± ë¡œì§ ì •í™•ì„± ê²€ì¦"""
        self.logger.info("4. ìºì‹œ í‚¤ ìƒì„± ë¡œì§ ê²€ì¦")

        from cached_db_processor import CachedDBProcessor

        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_file = Path('cache_key_test.db')
        test_file.write_text("initial content")

        test_results = {
            'key_consistency': False,
            'parameter_sensitivity': False,
            'modification_sensitivity': False,
            'mtime_included': False,
            'size_included': False
        }

        try:
            processor = CachedDBProcessor()

            # 1. ë™ì¼ íŒŒë¼ë¯¸í„°ë¡œ í‚¤ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
            key1 = processor.generate_cache_key(test_file, 1, 0, 100)
            key2 = processor.generate_cache_key(test_file, 1, 0, 100)

            if key1 == key2:
                test_results['key_consistency'] = True
                self.logger.info("âœ… í‚¤ ì¼ê´€ì„± í™•ì¸")

            # 2. íŒŒë¼ë¯¸í„° ë¯¼ê°ì„± í…ŒìŠ¤íŠ¸
            key_diff_sheet = processor.generate_cache_key(test_file, 2, 0, 100)
            key_diff_chunk = processor.generate_cache_key(test_file, 1, 0, 200)

            if len(set([key1, key_diff_sheet, key_diff_chunk])) == 3:
                test_results['parameter_sensitivity'] = True
                self.logger.info("âœ… íŒŒë¼ë¯¸í„° ë¯¼ê°ì„± í™•ì¸")

            # 3. íŒŒì¼ ìˆ˜ì • ì‹œê°„/í¬ê¸° í¬í•¨ í™•ì¸
            stat_before = test_file.stat()

            # íŒŒì¼ ìˆ˜ì •
            time.sleep(0.1)
            test_file.write_text("modified content - longer")

            key_after_modification = processor.generate_cache_key(test_file, 1, 0, 100)
            stat_after = test_file.stat()

            if key1 != key_after_modification:
                test_results['modification_sensitivity'] = True
                self.logger.info("âœ… íŒŒì¼ ìˆ˜ì • ë¯¼ê°ì„± í™•ì¸")

            # mtimeê³¼ sizeê°€ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if stat_before.st_mtime != stat_after.st_mtime:
                test_results['mtime_included'] = True
                self.logger.info("âœ… ìˆ˜ì • ì‹œê°„ ë³€í™” ê°ì§€")

            if stat_before.st_size != stat_after.st_size:
                test_results['size_included'] = True
                self.logger.info("âœ… íŒŒì¼ í¬ê¸° ë³€í™” ê°ì§€")

        except Exception as e:
            test_results['error'] = str(e)
            self.logger.error(f"âŒ ìºì‹œ í‚¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        finally:
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
            if test_file.exists():
                test_file.unlink()

        return test_results

    def test_lru_algorithm(self) -> Dict[str, Any]:
        """LRU ìºì‹œ ì•Œê³ ë¦¬ì¦˜ ì‹¤ì œ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        self.logger.info("5. LRU ìºì‹œ ì•Œê³ ë¦¬ì¦˜ ê²€ì¦")

        from cached_db_processor import MemoryCache

        # ì‘ì€ ìºì‹œ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
        cache = MemoryCache(max_size=5)

        test_results = {
            'lru_eviction_working': False,
            'access_order_maintained': False,
            'size_limit_enforced': False,
            'statistics_accurate': False
        }

        try:
            # 1. ìºì‹œ í¬ê¸° ì œí•œ í…ŒìŠ¤íŠ¸
            for i in range(10):  # ìºì‹œ í¬ê¸°(5)ë¥¼ ì´ˆê³¼í•˜ëŠ” ë°ì´í„° ì €ì¥
                cache.set(f'key_{i}', f'value_{i}')

            if len(cache.cache) == 5:  # ìµœëŒ€ í¬ê¸° ìœ ì§€
                test_results['size_limit_enforced'] = True
                self.logger.info("âœ… ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸")

            # 2. LRU ì œê±° í™•ì¸ (ì´ˆê¸° í‚¤ë“¤ì´ ì œê±°ë˜ì—ˆëŠ”ì§€)
            if cache.get('key_0') is None and cache.get('key_9') is not None:
                test_results['lru_eviction_working'] = True
                self.logger.info("âœ… LRU ì œê±° ë©”ì»¤ë‹ˆì¦˜ í™•ì¸")

            # 3. ì ‘ê·¼ ìˆœì„œ ìœ ì§€ í…ŒìŠ¤íŠ¸
            cache.get('key_5')  # key_5 ì ‘ê·¼ìœ¼ë¡œ ìµœì‹ ìœ¼ë¡œ ë§Œë“¦
            cache.set('key_new', 'new_value')  # ìƒˆ í•­ëª© ì¶”ê°€

            if cache.get('key_5') is not None:  # ìµœê·¼ ì ‘ê·¼í•œ key_5ëŠ” ìœ ì§€ë˜ì–´ì•¼ í•¨
                test_results['access_order_maintained'] = True
                self.logger.info("âœ… ì ‘ê·¼ ìˆœì„œ ìœ ì§€ í™•ì¸")

            # 4. í†µê³„ ì •í™•ì„± í™•ì¸
            stats = cache.get_stats()
            if stats['cache_size'] == len(cache.cache) and stats['max_size'] == 5:
                test_results['statistics_accurate'] = True
                self.logger.info("âœ… ìºì‹œ í†µê³„ ì •í™•ì„± í™•ì¸")

        except Exception as e:
            test_results['error'] = str(e)
            self.logger.error(f"âŒ LRU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        return test_results

    def test_thread_safety(self) -> Dict[str, Any]:
        """ë©€í‹°ìŠ¤ë ˆë”© í™˜ê²½ì—ì„œ ìŠ¤ë ˆë“œ ì•ˆì „ì„± í…ŒìŠ¤íŠ¸"""
        self.logger.info("6. ìŠ¤ë ˆë“œ ì•ˆì „ì„± ê²€ì¦")

        from cached_db_processor import MemoryCache

        cache = MemoryCache(max_size=1000)
        test_results = {
            'concurrent_access_safe': False,
            'data_integrity_maintained': False,
            'no_race_conditions': False,
            'operations_completed': 0,
            'operations_expected': 0
        }

        def concurrent_cache_operation(thread_id: int, operations_count: int = 50):
            """ë™ì‹œ ìºì‹œ ì‘ì—…"""
            success_count = 0
            for i in range(operations_count):
                try:
                    key = f'thread_{thread_id}_key_{i}'
                    value = f'thread_{thread_id}_value_{i}'

                    # ì €ì¥
                    cache.set(key, value)

                    # ì¡°íšŒ
                    retrieved = cache.get(key)
                    if retrieved == value:
                        success_count += 1

                    # ì¼ë¶€ í‚¤ ì‚­ì œ
                    if i % 10 == 0:
                        cache.delete(key)

                except Exception:
                    pass

            return success_count

        try:
            # 10ê°œ ìŠ¤ë ˆë“œë¡œ ë™ì‹œ ì‘ì—…
            thread_count = 10
            operations_per_thread = 50
            test_results['operations_expected'] = thread_count * operations_per_thread

            with ThreadPoolExecutor(max_workers=thread_count) as executor:
                futures = [
                    executor.submit(concurrent_cache_operation, i, operations_per_thread)
                    for i in range(thread_count)
                ]

                results = [future.result() for future in futures]
                test_results['operations_completed'] = sum(results)

            # ì„±ê³µë¥  ê³„ì‚°
            success_rate = test_results['operations_completed'] / test_results['operations_expected']

            if success_rate > 0.95:  # 95% ì´ìƒ ì„±ê³µ
                test_results['concurrent_access_safe'] = True
                test_results['data_integrity_maintained'] = True
                test_results['no_race_conditions'] = True
                self.logger.info(f"âœ… ìŠ¤ë ˆë“œ ì•ˆì „ì„± í™•ì¸ (ì„±ê³µë¥ : {success_rate:.2%})")
            else:
                self.logger.warning(f"âš ï¸ ìŠ¤ë ˆë“œ ì•ˆì „ì„± ë¬¸ì œ (ì„±ê³µë¥ : {success_rate:.2%})")

        except Exception as e:
            test_results['error'] = str(e)
            self.logger.error(f"âŒ ìŠ¤ë ˆë“œ ì•ˆì „ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

        return test_results

    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        self.logger.info("Redis ìºì‹± ì‹œìŠ¤í…œ ì¢…í•© ê²€ì¦ ì‹œì‘")

        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        redis_connection = self.test_redis_server_connection()
        hierarchical_caching = self.test_hierarchical_caching()
        fallback_mechanism = self.test_fallback_mechanism()
        cache_key_generation = self.test_cache_key_generation()
        lru_algorithm = self.test_lru_algorithm()
        thread_safety = self.test_thread_safety()

        # ì¢…í•© í‰ê°€
        total_tests = 0
        passed_tests = 0

        # Redis ì—°ê²° (ì„ íƒì‚¬í•­)
        if redis_connection.get('redis_server_running', False):
            passed_tests += 1
        total_tests += 1

        # ê³„ì¸µì  ìºì‹± (Redis ìˆì„ ë•Œë§Œ)
        if redis_connection.get('redis_server_running', False):
            if hierarchical_caching.get('hierarchical_flow', False):
                passed_tests += 1
            total_tests += 1

        # Fallback ë©”ì»¤ë‹ˆì¦˜ (í•„ìˆ˜)
        if fallback_mechanism.get('graceful_degradation', False):
            passed_tests += 1
        total_tests += 1

        # ìºì‹œ í‚¤ ìƒì„± (í•„ìˆ˜)
        key_tests = ['key_consistency', 'parameter_sensitivity', 'modification_sensitivity']
        if all(cache_key_generation.get(test, False) for test in key_tests):
            passed_tests += 1
        total_tests += 1

        # LRU ì•Œê³ ë¦¬ì¦˜ (í•„ìˆ˜)
        lru_tests = ['lru_eviction_working', 'size_limit_enforced']
        if all(lru_algorithm.get(test, False) for test in lru_tests):
            passed_tests += 1
        total_tests += 1

        # ìŠ¤ë ˆë“œ ì•ˆì „ì„± (í•„ìˆ˜)
        if thread_safety.get('concurrent_access_safe', False):
            passed_tests += 1
        total_tests += 1

        quality_score = (passed_tests / total_tests) * 100 if total_tests > 0 else 0

        comprehensive_report = {
            'overall_quality_score': quality_score,
            'tests_passed': passed_tests,
            'total_tests': total_tests,
            'redis_connection_test': redis_connection,
            'hierarchical_caching_test': hierarchical_caching,
            'fallback_mechanism_test': fallback_mechanism,
            'cache_key_generation_test': cache_key_generation,
            'lru_algorithm_test': lru_algorithm,
            'thread_safety_test': thread_safety,
            'recommendations': self.generate_recommendations(
                redis_connection, hierarchical_caching, fallback_mechanism,
                cache_key_generation, lru_algorithm, thread_safety
            )
        }

        return comprehensive_report

    def generate_recommendations(self, *test_results) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        redis_test = test_results[0]
        if not redis_test.get('redis_server_running', False):
            if not redis_test.get('redis_module_available', False):
                recommendations.append("Redis ëª¨ë“ˆ ì„¤ì¹˜: pip install redis")
            recommendations.append("Redis ì„œë²„ ì„¤ì¹˜ ë° ì‹¤í–‰ì„ ê¶Œì¥í•©ë‹ˆë‹¤ (ì„±ëŠ¥ í–¥ìƒ)")

        thread_test = test_results[5]
        if not thread_test.get('concurrent_access_safe', False):
            recommendations.append("ìŠ¤ë ˆë“œ ì•ˆì „ì„± ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")

        if not recommendations:
            recommendations.append("ëª¨ë“  ìºì‹œ ê¸°ëŠ¥ì´ ìš°ìˆ˜í•˜ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤")

        return recommendations

def main():
    """Redis ìºì‹± ì‹œìŠ¤í…œ ì¢…í•© ê²€ì¦ ì‹¤í–‰"""
    print("ğŸ” Redis ìºì‹± ì‹œìŠ¤í…œ ì¢…í•© ê²€ì¦")
    print("=" * 80)

    verifier = RedisComprehensiveVerifier()

    try:
        # ì¢…í•© ê²€ì¦ ì‹¤í–‰
        report = verifier.generate_comprehensive_report()

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š Redis ìºì‹± ì‹œìŠ¤í…œ ì¢…í•© ê²€ì¦ ê²°ê³¼:")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {report['overall_quality_score']:.1f}% ({report['tests_passed']}/{report['total_tests']})")

        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        print(f"\nğŸ” ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")

        redis_test = report['redis_connection_test']
        print(f"   Redis ì—°ê²°: {'âœ…' if redis_test.get('redis_server_running') else 'âš ï¸ ë¯¸ì‚¬ìš©'}")
        if redis_test.get('connection_details'):
            details = redis_test['connection_details']
            print(f"     ë²„ì „: {details.get('redis_version', 'unknown')}")
            print(f"     ë©”ëª¨ë¦¬: {details.get('used_memory', 'unknown')}")

        fallback_test = report['fallback_mechanism_test']
        print(f"   Fallback ë©”ì»¤ë‹ˆì¦˜: {'âœ…' if fallback_test.get('graceful_degradation') else 'âŒ'}")

        key_test = report['cache_key_generation_test']
        print(f"   í‚¤ ìƒì„± ë¡œì§: {'âœ…' if key_test.get('key_consistency') else 'âŒ'}")

        lru_test = report['lru_algorithm_test']
        print(f"   LRU ì•Œê³ ë¦¬ì¦˜: {'âœ…' if lru_test.get('lru_eviction_working') else 'âŒ'}")

        thread_test = report['thread_safety_test']
        print(f"   ìŠ¤ë ˆë“œ ì•ˆì „ì„±: {'âœ…' if thread_test.get('concurrent_access_safe') else 'âŒ'}")
        if 'operations_completed' in thread_test:
            success_rate = thread_test['operations_completed'] / thread_test['operations_expected'] * 100
            print(f"     ì„±ê³µë¥ : {success_rate:.1f}%")

        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in report['recommendations']:
            print(f"   - {rec}")

        # ê²°ê³¼ ì €ì¥
        with open('redis_comprehensive_verification_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ 'redis_comprehensive_verification_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ ì¢…í•© ê²€ì¦ ì‹¤íŒ¨: {e}")
        logging.error(f"ì¢…í•© ê²€ì¦ ì‹¤íŒ¨: {e}")

    print("=" * 80)

if __name__ == "__main__":
    main()