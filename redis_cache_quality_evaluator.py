"""
Redis ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„ í’ˆì§ˆ í‰ê°€ê¸°
Redis ì—°ê²°, ë°ì´í„° ì¼ê´€ì„±, fallback ë©”ì»¤ë‹ˆì¦˜ ë“± ì¢…í•© ê²€ì¦
"""

import time
import logging
import os
import sys
import hashlib
import pickle
from pathlib import Path
from typing import Dict, List, Any, Optional
import json
import threading
from concurrent.futures import ThreadPoolExecutor

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

class RedisCacheQualityEvaluator:
    """Redis ìºì‹± ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.evaluation_results = {}
    
    def test_redis_connection(self) -> Dict[str, Any]:
        """Redis ì—°ê²° í…ŒìŠ¤íŠ¸"""
        self.logger.info("Redis ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        test_results = {
            'redis_available': False,
            'connection_successful': False,
            'basic_operations': False,
            'error_details': None
        }
        
        try:
            # Redis ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
            import redis
            test_results['redis_module_available'] = True
            
            # Redis ì—°ê²° í…ŒìŠ¤íŠ¸
            redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=False)
            redis_client.ping()
            test_results['connection_successful'] = True
            test_results['redis_available'] = True
            
            # ê¸°ë³¸ ì‘ì—… í…ŒìŠ¤íŠ¸
            test_key = 'cache_quality_test'
            test_value = {'test': 'data', 'timestamp': time.time()}
            
            # SET í…ŒìŠ¤íŠ¸
            redis_client.set(test_key, pickle.dumps(test_value))
            
            # GET í…ŒìŠ¤íŠ¸
            retrieved_data = redis_client.get(test_key)
            if retrieved_data:
                unpickled_data = pickle.loads(retrieved_data)
                if unpickled_data == test_value:
                    test_results['basic_operations'] = True
            
            # ì •ë¦¬
            redis_client.delete(test_key)
            
            self.logger.info("âœ… Redis ì—°ê²° ë° ê¸°ë³¸ ì‘ì—… ì„±ê³µ")
            
        except ImportError:
            test_results['error_details'] = "Redis ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ"
            self.logger.warning("âš ï¸ Redis ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        except Exception as e:
            test_results['error_details'] = str(e)
            self.logger.warning(f"âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨: {e}")
        
        return test_results
    
    def test_cache_fallback_mechanism(self) -> Dict[str, Any]:
        """ìºì‹œ fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ìºì‹œ fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        from cached_db_processor import CachedDBProcessor, CacheConfig
        
        # Redis ì—†ì´ ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©í•˜ëŠ” ì„¤ì •
        config_memory_only = CacheConfig(
            enable_redis_cache=False,
            enable_memory_cache=True,
            memory_cache_size=100
        )
        
        # Redis + ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©í•˜ëŠ” ì„¤ì • (Redis ì—†ì–´ë„ ì‘ë™í•´ì•¼ í•¨)
        config_redis_fallback = CacheConfig(
            enable_redis_cache=True,
            enable_memory_cache=True,
            memory_cache_size=100,
            redis_host='nonexistent_host',  # ì˜ë„ì ìœ¼ë¡œ ì˜ëª»ëœ í˜¸ìŠ¤íŠ¸
            redis_port=9999
        )
        
        fallback_results = {}
        
        # 1. ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©
        try:
            processor_memory = CachedDBProcessor(config_memory_only)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            test_key = "test_fallback_key"
            test_value = [["DEFINE", "CONST", "FLOAT32", "TEST", "1.0", "Test"]]
            
            # ìºì‹œ ì €ì¥/ì¡°íšŒ í…ŒìŠ¤íŠ¸
            processor_memory.set_to_cache(test_key, test_value)
            retrieved_value = processor_memory.get_from_cache(test_key)
            
            fallback_results['memory_only'] = {
                'success': retrieved_value == test_value,
                'cache_working': retrieved_value is not None
            }
            
            processor_memory.cleanup()
            
        except Exception as e:
            fallback_results['memory_only'] = {
                'success': False,
                'error': str(e)
            }
        
        # 2. Redis fallback í…ŒìŠ¤íŠ¸
        try:
            processor_fallback = CachedDBProcessor(config_redis_fallback)
            
            # Redis ì—°ê²° ì‹¤íŒ¨í•´ë„ ë©”ëª¨ë¦¬ ìºì‹œë¡œ ì‘ë™í•´ì•¼ í•¨
            test_key = "test_redis_fallback_key"
            test_value = [["DEFINE", "CONST", "FLOAT32", "FALLBACK", "2.0", "Fallback"]]
            
            processor_fallback.set_to_cache(test_key, test_value)
            retrieved_value = processor_fallback.get_from_cache(test_key)
            
            fallback_results['redis_fallback'] = {
                'success': retrieved_value == test_value,
                'fallback_working': retrieved_value is not None,
                'redis_failed_gracefully': True
            }
            
            processor_fallback.cleanup()
            
        except Exception as e:
            fallback_results['redis_fallback'] = {
                'success': False,
                'error': str(e),
                'redis_failed_gracefully': False
            }
        
        self.logger.info("ìºì‹œ fallback ë©”ì»¤ë‹ˆì¦˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return fallback_results
    
    def test_cache_key_generation(self) -> Dict[str, Any]:
        """ìºì‹œ í‚¤ ìƒì„± ë¡œì§ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ìºì‹œ í‚¤ ìƒì„± ë¡œì§ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        from cached_db_processor import CachedDBProcessor
        
        processor = CachedDBProcessor()
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_file = Path('test_cache_key.db')
        test_file.write_text("test content")
        
        try:
            # ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¡œ í‚¤ ìƒì„±
            key1 = processor.generate_cache_key(test_file, 1, 0, 100)
            key2 = processor.generate_cache_key(test_file, 1, 0, 100)
            
            # ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë¡œ í‚¤ ìƒì„±
            key3 = processor.generate_cache_key(test_file, 1, 0, 200)  # ë‹¤ë¥¸ chunk_end
            key4 = processor.generate_cache_key(test_file, 2, 0, 100)  # ë‹¤ë¥¸ sheet_id
            
            # íŒŒì¼ ìˆ˜ì • í›„ í‚¤ ìƒì„±
            time.sleep(0.1)  # ìˆ˜ì • ì‹œê°„ ì°¨ì´ ë³´ì¥
            test_file.write_text("modified content")
            key5 = processor.generate_cache_key(test_file, 1, 0, 100)
            
            key_test_results = {
                'consistency': key1 == key2,
                'parameter_sensitivity': len(set([key1, key3, key4])) == 3,
                'modification_sensitivity': key1 != key5,
                'key_format_valid': all(len(key) == 32 for key in [key1, key2, key3, key4, key5])
            }
            
            self.logger.info("âœ… ìºì‹œ í‚¤ ìƒì„± ë¡œì§ ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            key_test_results = {
                'success': False,
                'error': str(e)
            }
        finally:
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
            if test_file.exists():
                test_file.unlink()
        
        return key_test_results
    
    def test_cache_data_consistency(self) -> Dict[str, Any]:
        """ìºì‹œ ë°ì´í„° ì¼ê´€ì„± í…ŒìŠ¤íŠ¸"""
        self.logger.info("ìºì‹œ ë°ì´í„° ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        from cached_db_processor import MemoryCache
        
        cache = MemoryCache(max_size=10)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_data = {
            'key1': [["DEFINE", "CONST", "FLOAT32", "VAL1", "1.0", "Test1"]],
            'key2': [["DEFINE", "CONST", "INT32", "VAL2", "2", "Test2"]],
            'key3': [["DEFINE", "CONST", "STRING", "VAL3", '"test"', "Test3"]]
        }
        
        consistency_results = {
            'storage_retrieval': True,
            'lru_behavior': True,
            'data_integrity': True,
            'concurrent_access': True
        }
        
        try:
            # 1. ì €ì¥/ì¡°íšŒ ì¼ê´€ì„±
            for key, value in test_data.items():
                cache.set(key, value)
                retrieved = cache.get(key)
                if retrieved != value:
                    consistency_results['storage_retrieval'] = False
                    break
            
            # 2. LRU ë™ì‘ í™•ì¸
            # ìºì‹œ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ” ë°ì´í„° ì €ì¥
            for i in range(15):  # ìºì‹œ í¬ê¸°(10)ë¥¼ ì´ˆê³¼
                cache.set(f'lru_test_{i}', f'value_{i}')
            
            # ì´ˆê¸° í‚¤ë“¤ì´ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if cache.get('lru_test_0') is not None:  # ì²« ë²ˆì§¸ í‚¤ëŠ” ì œê±°ë˜ì–´ì•¼ í•¨
                consistency_results['lru_behavior'] = False
            
            if cache.get('lru_test_14') is None:  # ë§ˆì§€ë§‰ í‚¤ëŠ” ì¡´ì¬í•´ì•¼ í•¨
                consistency_results['lru_behavior'] = False
            
            # 3. ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
            complex_data = {
                'nested': {'list': [1, 2, 3], 'dict': {'a': 'b'}},
                'unicode': 'í•œê¸€ í…ŒìŠ¤íŠ¸',
                'special_chars': '!@#$%^&*()'
            }
            
            cache.set('complex_key', complex_data)
            retrieved_complex = cache.get('complex_key')
            if retrieved_complex != complex_data:
                consistency_results['data_integrity'] = False
            
            # 4. ë™ì‹œ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
            def concurrent_cache_operation(cache_obj, thread_id):
                for i in range(10):
                    key = f'thread_{thread_id}_key_{i}'
                    value = f'thread_{thread_id}_value_{i}'
                    cache_obj.set(key, value)
                    retrieved = cache_obj.get(key)
                    if retrieved != value:
                        return False
                return True
            
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = [executor.submit(concurrent_cache_operation, cache, i) for i in range(3)]
                concurrent_results = [future.result() for future in futures]
                
                if not all(concurrent_results):
                    consistency_results['concurrent_access'] = False
            
            # ìºì‹œ í†µê³„ í™•ì¸
            stats = cache.get_stats()
            consistency_results['cache_stats'] = stats
            
            self.logger.info("âœ… ìºì‹œ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            consistency_results['success'] = False
            consistency_results['error'] = str(e)
        
        return consistency_results
    
    def test_cache_performance_impact(self) -> Dict[str, Any]:
        """ìºì‹œ ì„±ëŠ¥ ì˜í–¥ í…ŒìŠ¤íŠ¸"""
        self.logger.info("ìºì‹œ ì„±ëŠ¥ ì˜í–¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        from cached_db_processor import CachedDBProcessor, CacheConfig
        
        # DB íŒŒì¼ í™•ì¸
        db_dir = Path('database')
        db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000][:1]  # 1ê°œ íŒŒì¼ë§Œ
        
        if not db_files:
            return {'success': False, 'error': 'No DB files available for testing'}
        
        db_file = db_files[0]
        
        performance_results = {}
        
        try:
            # 1. ìºì‹œ ì—†ì´ ì²˜ë¦¬
            config_no_cache = CacheConfig(
                enable_redis_cache=False,
                enable_memory_cache=False
            )
            
            processor_no_cache = CachedDBProcessor(config_no_cache)
            
            start_time = time.perf_counter()
            result_no_cache = processor_no_cache.process_batch_cached([db_file])
            no_cache_time = time.perf_counter() - start_time
            
            processor_no_cache.cleanup()
            
            # 2. ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš©
            config_with_cache = CacheConfig(
                enable_redis_cache=False,
                enable_memory_cache=True,
                memory_cache_size=1000
            )
            
            processor_with_cache = CachedDBProcessor(config_with_cache)
            
            # ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ êµ¬ì¶•)
            start_time = time.perf_counter()
            result_first_run = processor_with_cache.process_batch_cached([db_file])
            first_run_time = time.perf_counter() - start_time
            
            # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ í™œìš©)
            start_time = time.perf_counter()
            result_second_run = processor_with_cache.process_batch_cached([db_file])
            second_run_time = time.perf_counter() - start_time
            
            # ìºì‹œ í†µê³„
            cache_stats = processor_with_cache.get_cache_stats()
            
            processor_with_cache.cleanup()
            
            # ì„±ëŠ¥ ë¶„ì„
            performance_results = {
                'no_cache_time': no_cache_time,
                'first_run_time': first_run_time,
                'second_run_time': second_run_time,
                'cache_effect': first_run_time / second_run_time if second_run_time > 0 else 0,
                'cache_overhead': (first_run_time - no_cache_time) / no_cache_time * 100 if no_cache_time > 0 else 0,
                'cache_stats': cache_stats,
                'data_consistency': (
                    result_no_cache['total_processed_items'] == 
                    result_first_run['total_processed_items'] == 
                    result_second_run['total_processed_items']
                )
            }
            
            self.logger.info("âœ… ìºì‹œ ì„±ëŠ¥ ì˜í–¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            performance_results = {
                'success': False,
                'error': str(e)
            }
        
        return performance_results
    
    def generate_quality_report(self) -> Dict[str, Any]:
        """ìºì‹œ ì‹œìŠ¤í…œ í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        self.logger.info("Redis ìºì‹œ ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ì‹œì‘")
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        redis_test = self.test_redis_connection()
        fallback_test = self.test_cache_fallback_mechanism()
        key_test = self.test_cache_key_generation()
        consistency_test = self.test_cache_data_consistency()
        performance_test = self.test_cache_performance_impact()
        
        # ì¢…í•© í‰ê°€
        quality_score = 0
        max_score = 5
        
        # Redis ì—°ê²° (ì„ íƒì‚¬í•­)
        if redis_test.get('redis_available', False):
            quality_score += 1
        
        # Fallback ë©”ì»¤ë‹ˆì¦˜ (í•„ìˆ˜)
        if fallback_test.get('memory_only', {}).get('success', False):
            quality_score += 1
        
        # í‚¤ ìƒì„± ë¡œì§ (í•„ìˆ˜)
        if all(key_test.get(key, False) for key in ['consistency', 'parameter_sensitivity', 'modification_sensitivity']):
            quality_score += 1
        
        # ë°ì´í„° ì¼ê´€ì„± (í•„ìˆ˜)
        if all(consistency_test.get(key, False) for key in ['storage_retrieval', 'data_integrity']):
            quality_score += 1
        
        # ì„±ëŠ¥ íš¨ê³¼ (í•„ìˆ˜)
        if performance_test.get('cache_effect', 0) > 1.5:  # 1.5ë°° ì´ìƒ ì„±ëŠ¥ í–¥ìƒ
            quality_score += 1
        
        quality_report = {
            'overall_quality_score': quality_score,
            'max_possible_score': max_score,
            'quality_percentage': (quality_score / max_score) * 100,
            'redis_connection_test': redis_test,
            'fallback_mechanism_test': fallback_test,
            'cache_key_generation_test': key_test,
            'data_consistency_test': consistency_test,
            'performance_impact_test': performance_test,
            'recommendations': self.generate_recommendations(redis_test, fallback_test, key_test, consistency_test, performance_test)
        }
        
        return quality_report
    
    def generate_recommendations(self, redis_test, fallback_test, key_test, consistency_test, performance_test) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not redis_test.get('redis_available', False):
            recommendations.append("Redis ì„œë²„ ì„¤ì¹˜ ë° ì„¤ì •ì„ ê³ ë ¤í•˜ì—¬ ë” ë‚˜ì€ ìºì‹œ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        if not fallback_test.get('redis_fallback', {}).get('success', False):
            recommendations.append("Redis fallback ë©”ì»¤ë‹ˆì¦˜ì„ ê°œì„ í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        if performance_test.get('cache_effect', 0) < 2.0:
            recommendations.append("ìºì‹œ íš¨ê³¼ê°€ ì œí•œì ì…ë‹ˆë‹¤. ìºì‹œ í¬ê¸°ë‚˜ ì „ëµì„ ì¡°ì •í•´ë³´ì„¸ìš”.")
        
        if consistency_test.get('concurrent_access', True) == False:
            recommendations.append("ë™ì‹œ ì ‘ê·¼ ì‹œ ì•ˆì •ì„±ì„ ê°œì„ í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.")
        
        if not recommendations:
            recommendations.append("ìºì‹œ ì‹œìŠ¤í…œì´ ìš°ìˆ˜í•œ í’ˆì§ˆë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return recommendations

def main():
    """Redis ìºì‹œ í’ˆì§ˆ í‰ê°€ ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ”§ Redis ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„ í’ˆì§ˆ í‰ê°€")
    print("=" * 80)
    
    evaluator = RedisCacheQualityEvaluator()
    
    try:
        # í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
        quality_report = evaluator.generate_quality_report()
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š Redis ìºì‹œ ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ê²°ê³¼:")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {quality_report['quality_percentage']:.1f}% ({quality_report['overall_quality_score']}/{quality_report['max_possible_score']})")
        
        # ì£¼ìš” í…ŒìŠ¤íŠ¸ ê²°ê³¼
        print(f"\nğŸ” ì£¼ìš” í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   Redis ì—°ê²°: {'âœ…' if quality_report['redis_connection_test'].get('redis_available') else 'âš ï¸ ë¯¸ì‚¬ìš©'}")
        print(f"   Fallback ë©”ì»¤ë‹ˆì¦˜: {'âœ…' if quality_report['fallback_mechanism_test'].get('memory_only', {}).get('success') else 'âŒ'}")
        print(f"   í‚¤ ìƒì„± ë¡œì§: {'âœ…' if quality_report['cache_key_generation_test'].get('consistency') else 'âŒ'}")
        print(f"   ë°ì´í„° ì¼ê´€ì„±: {'âœ…' if quality_report['data_consistency_test'].get('storage_retrieval') else 'âŒ'}")
        
        # ì„±ëŠ¥ íš¨ê³¼
        perf_test = quality_report['performance_impact_test']
        if 'cache_effect' in perf_test:
            print(f"   ìºì‹œ ì„±ëŠ¥ íš¨ê³¼: {perf_test['cache_effect']:.2f}ë°°")
        
        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for rec in quality_report['recommendations']:
            print(f"   - {rec}")
        
        # ê²°ê³¼ ì €ì¥
        with open('redis_cache_quality_report.json', 'w', encoding='utf-8') as f:
            json.dump(quality_report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ 'redis_cache_quality_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
        logging.error(f"í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
    
    print("=" * 80)

if __name__ == "__main__":
    main()
