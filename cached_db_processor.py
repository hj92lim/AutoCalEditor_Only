"""
Phase 3: ìºì‹± ê¸°ë°˜ DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ
Redis ìºì‹±ì„ ì´ìš©í•œ ì¤‘ê°„ ê²°ê³¼ ìºì‹± êµ¬í˜„
"""

import time
import gc
import logging
import os
import sys
import hashlib
import pickle
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

@dataclass
class CacheConfig:
    """ìºì‹± ì„¤ì •"""
    batch_size: int = 500
    chunk_size: int = 500  # ë” ì‘ì€ ì²­í¬ë¡œ ìºì‹œ íš¨ìœ¨ì„± í–¥ìƒ
    gc_interval: int = 4
    enable_redis_cache: bool = True
    redis_host: str = 'localhost'
    redis_port: int = 6379
    redis_db: int = 0
    cache_ttl: int = 3600  # 1ì‹œê°„
    enable_memory_cache: bool = True
    memory_cache_size: int = 5000  # 5ë°° í™•ëŒ€
    cache_compression: bool = True
    enable_sheet_level_cache: bool = True  # ì‹œíŠ¸ ë‹¨ìœ„ ìºì‹± ì¶”ê°€

class MemoryCache:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ìºì‹œ (Redis ëŒ€ì•ˆ)"""
    
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.access_order = []
        self.max_size = max_size
        self.hits = 0
        self.misses = 0
        self.logger = logging.getLogger(__name__)
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        if key in self.cache:
            # LRU ì—…ë°ì´íŠ¸
            self.access_order.remove(key)
            self.access_order.append(key)
            self.hits += 1
            return self.cache[key]
        else:
            self.misses += 1
            return None
    
    def set(self, key: str, value: Any):
        """ìºì‹œì— ê°’ ì €ì¥"""
        if key in self.cache:
            # ê¸°ì¡´ í‚¤ ì—…ë°ì´íŠ¸
            self.access_order.remove(key)
        elif len(self.cache) >= self.max_size:
            # LRU ì œê±°
            oldest_key = self.access_order.pop(0)
            del self.cache[oldest_key]
        
        self.cache[key] = value
        self.access_order.append(key)
    
    def delete(self, key: str):
        """ìºì‹œì—ì„œ ê°’ ì‚­ì œ"""
        if key in self.cache:
            del self.cache[key]
            self.access_order.remove(key)
    
    def clear(self):
        """ìºì‹œ ì „ì²´ ì‚­ì œ"""
        self.cache.clear()
        self.access_order.clear()
        self.hits = 0
        self.misses = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        total_requests = self.hits + self.misses
        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache),
            'max_size': self.max_size
        }

class RedisCache:
    """Redis ê¸°ë°˜ ìºì‹œ"""
    
    def __init__(self, host: str = 'localhost', port: int = 6379, db: int = 0, ttl: int = 3600):
        self.host = host
        self.port = port
        self.db = db
        self.ttl = ttl
        self.redis_client = None
        self.logger = logging.getLogger(__name__)
        
        try:
            import redis
            self.redis_client = redis.Redis(host=host, port=port, db=db, decode_responses=False)
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            self.redis_client.ping()
            self.logger.info(f"Redis ì—°ê²° ì„±ê³µ: {host}:{port}/{db}")
        except ImportError:
            self.logger.warning("Redis ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            self.logger.warning(f"Redis ì—°ê²° ì‹¤íŒ¨: {e}. ë©”ëª¨ë¦¬ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.redis_client = None
    
    def get(self, key: str) -> Optional[Any]:
        """Redisì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        if not self.redis_client:
            return None
        
        try:
            data = self.redis_client.get(key)
            if data:
                return pickle.loads(data)
            return None
        except Exception as e:
            self.logger.error(f"Redis GET ì‹¤íŒ¨: {e}")
            return None
    
    def set(self, key: str, value: Any):
        """Redisì— ê°’ ì €ì¥"""
        if not self.redis_client:
            return
        
        try:
            data = pickle.dumps(value)
            self.redis_client.setex(key, self.ttl, data)
        except Exception as e:
            self.logger.error(f"Redis SET ì‹¤íŒ¨: {e}")
    
    def delete(self, key: str):
        """Redisì—ì„œ ê°’ ì‚­ì œ"""
        if not self.redis_client:
            return
        
        try:
            self.redis_client.delete(key)
        except Exception as e:
            self.logger.error(f"Redis DELETE ì‹¤íŒ¨: {e}")
    
    def clear(self):
        """Redis ìºì‹œ ì „ì²´ ì‚­ì œ"""
        if not self.redis_client:
            return
        
        try:
            self.redis_client.flushdb()
        except Exception as e:
            self.logger.error(f"Redis CLEAR ì‹¤íŒ¨: {e}")

class CachedDBProcessor:
    """ìºì‹± ê¸°ë°˜ DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, config: CacheConfig = None):
        self.config = config or CacheConfig()
        self.logger = logging.getLogger(__name__)
        
        # ìºì‹œ ì´ˆê¸°í™”
        self.redis_cache = None
        self.memory_cache = None
        
        if self.config.enable_redis_cache:
            self.redis_cache = RedisCache(
                host=self.config.redis_host,
                port=self.config.redis_port,
                db=self.config.redis_db,
                ttl=self.config.cache_ttl
            )
        
        if self.config.enable_memory_cache:
            self.memory_cache = MemoryCache(self.config.memory_cache_size)
        
        self.stats = {
            'total_files_processed': 0,
            'total_items_processed': 0,
            'total_execution_time': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
    
    def generate_cache_key(self, db_file: Path, sheet_id: int, chunk_start: int, chunk_end: int) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ê³¼ í¬ê¸°ë¥¼ í¬í•¨í•˜ì—¬ ìºì‹œ ë¬´íš¨í™”
        stat = db_file.stat()
        key_data = f"{db_file.name}:{sheet_id}:{chunk_start}:{chunk_end}:{stat.st_mtime}:{stat.st_size}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def generate_sheet_cache_key(self, db_file: Path, sheet_id: int) -> str:
        """ì‹œíŠ¸ ë‹¨ìœ„ ìºì‹œ í‚¤ ìƒì„±"""
        stat = db_file.stat()
        key_data = f"SHEET:{db_file.name}:{sheet_id}:{stat.st_mtime}:{stat.st_size}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def get_from_cache(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸° (Redis -> Memory ìˆœì„œ)"""
        # Redis ìºì‹œ í™•ì¸
        if self.redis_cache:
            value = self.redis_cache.get(key)
            if value is not None:
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                if self.memory_cache:
                    self.memory_cache.set(key, value)
                self.stats['cache_hits'] += 1
                return value
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if self.memory_cache:
            value = self.memory_cache.get(key)
            if value is not None:
                self.stats['cache_hits'] += 1
                return value
        
        self.stats['cache_misses'] += 1
        return None
    
    def set_to_cache(self, key: str, value: Any):
        """ìºì‹œì— ê°’ ì €ì¥ (Redis + Memory)"""
        if self.redis_cache:
            self.redis_cache.set(key, value)
        
        if self.memory_cache:
            self.memory_cache.set(key, value)
    
    def process_sheet_chunk_cached(self, db_handler, sheet: Dict[str, Any], 
                                 chunk_start: int, chunk_end: int, db_file: Path) -> int:
        """ìºì‹œë¥¼ í™œìš©í•œ ì‹œíŠ¸ ì²­í¬ ì²˜ë¦¬"""
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self.generate_cache_key(db_file, sheet['id'], chunk_start, chunk_end)
        
        # ìºì‹œì—ì„œ í™•ì¸
        cached_result = self.get_from_cache(cache_key)
        if cached_result is not None:
            self.logger.debug(f"ìºì‹œ íˆíŠ¸: {sheet['name']} chunk {chunk_start}-{chunk_end}")
            return cached_result
        
        # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ì²˜ë¦¬
        self.logger.debug(f"ìºì‹œ ë¯¸ìŠ¤: {sheet['name']} chunk {chunk_start}-{chunk_end}")
        
        # ì‹œíŠ¸ ë°ì´í„° ì¡°íšŒ
        sheet_data = db_handler.get_sheet_data(sheet['id'])
        if not sheet_data:
            result = 0
            self.set_to_cache(cache_key, result)
            return result
        
        # ì²­í¬ ë°ì´í„° ì¶”ì¶œ
        chunk_data = sheet_data[chunk_start:chunk_end]
        
        # Ultra ìµœì í™”ëœ Cython ëª¨ë“ˆ ì‚¬ìš©
        from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing
        
        total_processed_items = 0
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½”ë“œ ìƒì„±
        batch_count = 0
        for batch_start in range(0, len(chunk_data), self.config.batch_size):
            batch_end = min(batch_start + self.config.batch_size, len(chunk_data))
            batch_data = chunk_data[batch_start:batch_end]
            
            # ì½”ë“œ ì•„ì´í…œ ìƒì„±
            code_items = []
            for row_data in batch_data:
                if len(row_data) >= 3:
                    code_items.append([
                        "DEFINE", "CONST", "FLOAT32",
                        f"VAL_{row_data[0]}_{row_data[1]}", 
                        str(row_data[2]) if row_data[2] else "",
                        f"Generated from {sheet['name']}"
                    ])
            
            # Ultra ìµœì í™”ëœ Cython ì½”ë“œ ìƒì„±
            if code_items:
                processed_code = ultra_fast_write_cal_list_processing(code_items)
                total_processed_items += len(processed_code)
            
            # ë°°ì¹˜ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
            del code_items
            if 'processed_code' in locals():
                del processed_code
            
            batch_count += 1
            
            # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            if batch_count % self.config.gc_interval == 0:
                gc.collect()
        
        # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        self.set_to_cache(cache_key, total_processed_items)
        
        return total_processed_items
    
    def process_single_db_cached(self, db_file: Path) -> Dict[str, Any]:
        """ìºì‹œë¥¼ í™œìš©í•œ ë‹¨ì¼ DB ì²˜ë¦¬"""
        start_time = time.perf_counter()
        file_name = db_file.name
        
        try:
            self.logger.info(f"ìºì‹œ ê¸°ë°˜ DB ì²˜ë¦¬ ì‹œì‘: {file_name}")
            
            # DB ì—°ê²°
            from data_manager.db_handler_v2 import DBHandlerV2
            db_handler = DBHandlerV2(str(db_file))
            db_handler.connect()
            
            # $ ì‹œíŠ¸ ì°¾ê¸°
            sheets = db_handler.get_sheets()
            dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
            
            if not dollar_sheets:
                self.logger.warning(f"$ ì‹œíŠ¸ê°€ ì—†ìŒ: {file_name}")
                db_handler.disconnect()
                return {
                    'success': True,
                    'file_name': file_name,
                    'execution_time': time.perf_counter() - start_time,
                    'processed_items': 0,
                    'warning': 'No dollar sheets found'
                }
            
            total_processed_items = 0
            
            # ì‹œíŠ¸ë³„ ì²˜ë¦¬
            for sheet in dollar_sheets:
                sheet_data = db_handler.get_sheet_data(sheet['id'])
                if not sheet_data:
                    continue
                
                self.logger.debug(f"ìºì‹œ ê¸°ë°˜ ì‹œíŠ¸ ì²˜ë¦¬: {sheet['name']} ({len(sheet_data)}ê°œ ì…€)")
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ìºì‹œ í™œìš© ì²˜ë¦¬
                for chunk_start in range(0, len(sheet_data), self.config.chunk_size):
                    chunk_end = min(chunk_start + self.config.chunk_size, len(sheet_data))
                    
                    chunk_result = self.process_sheet_chunk_cached(
                        db_handler, sheet, chunk_start, chunk_end, db_file
                    )
                    total_processed_items += chunk_result
            
            # DB ì—°ê²° í•´ì œ
            db_handler.disconnect()
            
            execution_time = time.perf_counter() - start_time
            
            self.logger.info(f"ìºì‹œ ê¸°ë°˜ DB ì²˜ë¦¬ ì™„ë£Œ: {file_name} ({execution_time:.3f}ì´ˆ, {total_processed_items:,}ê°œ í•­ëª©)")
            
            return {
                'success': True,
                'file_name': file_name,
                'execution_time': execution_time,
                'processed_items': total_processed_items,
                'sheets_processed': len(dollar_sheets)
            }
            
        except Exception as e:
            execution_time = time.perf_counter() - start_time
            error_msg = f"ìºì‹œ ê¸°ë°˜ DB ì²˜ë¦¬ ì‹¤íŒ¨: {file_name} - {str(e)}"
            self.logger.error(error_msg)
            
            return {
                'success': False,
                'file_name': file_name,
                'execution_time': execution_time,
                'error': str(e)
            }
    
    def process_batch_cached(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìºì‹œë¥¼ í™œìš©í•œ ì¼ê´„ ì²˜ë¦¬"""
        start_time = time.perf_counter()
        
        self.logger.info(f"ìºì‹œ ê¸°ë°˜ ì¼ê´„ ì²˜ë¦¬ ì‹œì‘: {len(db_files)}ê°œ íŒŒì¼")
        
        results = []
        for db_file in db_files:
            result = self.process_single_db_cached(db_file)
            results.append(result)
        
        execution_time = time.perf_counter() - start_time
        
        # í†µê³„ ê³„ì‚°
        successful_results = [r for r in results if r['success']]
        total_processed_items = sum(r.get('processed_items', 0) for r in successful_results)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['total_files_processed'] += len(successful_results)
        self.stats['total_items_processed'] += total_processed_items
        self.stats['total_execution_time'] += execution_time
        
        self.logger.info(f"ìºì‹œ ê¸°ë°˜ ì¼ê´„ ì²˜ë¦¬ ì™„ë£Œ: {execution_time:.3f}ì´ˆ, {total_processed_items:,}ê°œ í•­ëª©")
        
        return {
            'success': True,
            'execution_time': execution_time,
            'total_processed_items': total_processed_items,
            'files_processed': len(successful_results),
            'files_failed': len(results) - len(successful_results),
            'results': results,
            'processing_mode': 'cached'
        }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        cache_stats = {}
        
        if self.memory_cache:
            cache_stats['memory_cache'] = self.memory_cache.get_stats()
        
        if self.redis_cache and self.redis_cache.redis_client:
            try:
                info = self.redis_cache.redis_client.info()
                cache_stats['redis_cache'] = {
                    'connected_clients': info.get('connected_clients', 0),
                    'used_memory': info.get('used_memory', 0),
                    'keyspace_hits': info.get('keyspace_hits', 0),
                    'keyspace_misses': info.get('keyspace_misses', 0)
                }
            except:
                cache_stats['redis_cache'] = {'status': 'unavailable'}
        
        cache_stats['total_hits'] = self.stats['cache_hits']
        cache_stats['total_misses'] = self.stats['cache_misses']
        
        total_requests = self.stats['cache_hits'] + self.stats['cache_misses']
        if total_requests > 0:
            cache_stats['hit_rate'] = (self.stats['cache_hits'] / total_requests) * 100
        else:
            cache_stats['hit_rate'] = 0
        
        return cache_stats
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.logger.info("ìºì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘")
        
        if self.memory_cache:
            self.memory_cache.clear()
        
        gc.collect()
        self.logger.info("ìºì‹œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

def main():
    """ìºì‹œ ê¸°ë°˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Phase 3: ìºì‹± ê¸°ë°˜ DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ")
    print("=" * 80)
    
    # ì„¤ì •
    config = CacheConfig(
        batch_size=500,
        chunk_size=1000,
        gc_interval=4,
        enable_redis_cache=True,
        enable_memory_cache=True,
        memory_cache_size=1000,
        cache_ttl=3600
    )
    
    # DB íŒŒì¼ ìˆ˜ì§‘
    db_dir = Path('database')
    if not db_dir.exists():
        print("âŒ Database ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000]
    
    if not db_files:
        print("âŒ ì²˜ë¦¬í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ì²˜ë¦¬ ëŒ€ìƒ: {len(db_files)}ê°œ íŒŒì¼")
    
    # ìºì‹œ ê¸°ë°˜ í”„ë¡œì„¸ì„œ ìƒì„± ë° ì‹¤í–‰
    processor = CachedDBProcessor(config)
    
    try:
        # ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ ë¯¸ìŠ¤)
        print("\nğŸ”„ ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ êµ¬ì¶•)")
        result1 = processor.process_batch_cached(db_files)
        
        # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ íˆíŠ¸)
        print("\nğŸ”„ ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ í™œìš©)")
        result2 = processor.process_batch_cached(db_files)
        
        # ê²°ê³¼ ë¹„êµ
        print(f"\nğŸ“Š ìºì‹œ íš¨ê³¼ ë¹„êµ:")
        print(f"   ì²« ë²ˆì§¸ ì‹¤í–‰: {result1['execution_time']:.3f}ì´ˆ")
        print(f"   ë‘ ë²ˆì§¸ ì‹¤í–‰: {result2['execution_time']:.3f}ì´ˆ")
        
        if result1['execution_time'] > 0:
            speedup = result1['execution_time'] / result2['execution_time']
            improvement = (1 - result2['execution_time'] / result1['execution_time']) * 100
            print(f"   ìºì‹œ íš¨ê³¼: {speedup:.2f}ë°° ë¹ ë¦„ ({improvement:.1f}% ê°œì„ )")
        
        # ìºì‹œ í†µê³„
        cache_stats = processor.get_cache_stats()
        print(f"\nğŸ“ˆ ìºì‹œ í†µê³„:")
        print(f"   ìºì‹œ íˆíŠ¸ìœ¨: {cache_stats['hit_rate']:.1f}%")
        print(f"   ì´ íˆíŠ¸: {cache_stats['total_hits']:,}íšŒ")
        print(f"   ì´ ë¯¸ìŠ¤: {cache_stats['total_misses']:,}íšŒ")
        
        if 'memory_cache' in cache_stats:
            mem_stats = cache_stats['memory_cache']
            print(f"   ë©”ëª¨ë¦¬ ìºì‹œ: {mem_stats['cache_size']}/{mem_stats['max_size']} í•­ëª©")
        
        # ê²°ê³¼ ì €ì¥
        with open('cached_processing_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'first_run': result1,
                'second_run': result2,
                'cache_stats': cache_stats,
                'config': config.__dict__
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ê²°ê³¼ê°€ 'cached_processing_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ìºì‹œ ê¸°ë°˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        logging.error(f"ìºì‹œ ê¸°ë°˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    finally:
        processor.cleanup()
    
    print("=" * 80)

if __name__ == "__main__":
    main()
