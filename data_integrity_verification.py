"""
ë°ì´í„° ë¬´ê²°ì„± ì‹¤ì œ ê²€ì¦
ìºì‹œ ì‚¬ìš©/ë¯¸ì‚¬ìš© ì‹œ ë™ì¼í•œ ê²°ê³¼ ë³´ì¥ í™•ì¸
"""

import time
import hashlib
import logging
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

class DataIntegrityVerifier:
    """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def generate_data_hash(self, data: List[List[str]]) -> str:
        """ë°ì´í„°ì˜ í•´ì‹œê°’ ìƒì„± (ìˆœì„œ ë¬´ê´€)"""
        # ë°ì´í„°ë¥¼ ì •ë ¬í•˜ì—¬ ìˆœì„œì— ë¬´ê´€í•œ í•´ì‹œ ìƒì„±
        sorted_data = sorted([tuple(item) for item in data])
        content = str(sorted_data)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def process_without_cache(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìºì‹œ ì—†ì´ ì²˜ë¦¬"""
        self.logger.info("ìºì‹œ ì—†ì´ ì²˜ë¦¬ ì‹œì‘")
        
        from cached_db_processor import CachedDBProcessor, CacheConfig
        
        # ìºì‹œ ë¹„í™œì„±í™” ì„¤ì •
        config_no_cache = CacheConfig(
            enable_redis_cache=False,
            enable_memory_cache=False
        )
        
        processor = CachedDBProcessor(config_no_cache)
        
        start_time = time.perf_counter()
        result = processor.process_batch_cached(db_files)
        execution_time = time.perf_counter() - start_time
        
        processor.cleanup()
        
        return {
            'execution_time': execution_time,
            'total_items': result['total_processed_items'],
            'success': result['success'],
            'processing_mode': 'no_cache'
        }
    
    def process_with_cache(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìºì‹œ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬"""
        self.logger.info("ìºì‹œ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬ ì‹œì‘")
        
        from cached_db_processor import CachedDBProcessor, CacheConfig
        
        # ìºì‹œ í™œì„±í™” ì„¤ì •
        config_with_cache = CacheConfig(
            enable_redis_cache=False,  # Redis ì—†ì´ ë©”ëª¨ë¦¬ ìºì‹œë§Œ
            enable_memory_cache=True,
            memory_cache_size=5000
        )
        
        processor = CachedDBProcessor(config_with_cache)
        
        # ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ êµ¬ì¶•)
        start_time = time.perf_counter()
        result_first = processor.process_batch_cached(db_files)
        first_execution_time = time.perf_counter() - start_time
        
        # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ í™œìš©)
        start_time = time.perf_counter()
        result_second = processor.process_batch_cached(db_files)
        second_execution_time = time.perf_counter() - start_time
        
        # ìºì‹œ í†µê³„
        cache_stats = processor.get_cache_stats()
        
        processor.cleanup()
        
        return {
            'first_execution_time': first_execution_time,
            'second_execution_time': second_execution_time,
            'cache_effect': first_execution_time / second_execution_time if second_execution_time > 0 else 0,
            'total_items_first': result_first['total_processed_items'],
            'total_items_second': result_second['total_processed_items'],
            'success_first': result_first['success'],
            'success_second': result_second['success'],
            'cache_stats': cache_stats,
            'processing_mode': 'with_cache'
        }
    
    def verify_complex_data_integrity(self) -> Dict[str, Any]:
        """ë³µì¡í•œ ë°ì´í„° êµ¬ì¡°ì˜ ë¬´ê²°ì„± ê²€ì¦"""
        self.logger.info("ë³µì¡í•œ ë°ì´í„° êµ¬ì¡° ë¬´ê²°ì„± ê²€ì¦")
        
        from cached_db_processor import MemoryCache
        
        cache = MemoryCache(max_size=100)
        
        # ë³µì¡í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        complex_data = [
            ["DEFINE", "CONST", "FLOAT32", "í•œê¸€_ë³€ìˆ˜ëª…", "3.14159", "í•œê¸€ ì£¼ì„"],
            ["DEFINE", "CONST", "STRING", "SPECIAL_CHARS", '"!@#$%^&*()"', "Special characters"],
            ["DEFINE", "CONST", "ARRAY", "NESTED_DATA", '{"key": [1, 2, 3]}', "JSON data"],
            ["DEFINE", "CONST", "UNICODE", "EMOJI_TEST", "ğŸš€ğŸ”¥ğŸ’", "Emoji test"],
            ["DEFINE", "CONST", "LONG_STRING", "LONG_VAL", "A" * 1000, "Long string test"]
        ]
        
        test_results = {
            'korean_text_integrity': False,
            'special_chars_integrity': False,
            'json_data_integrity': False,
            'emoji_integrity': False,
            'long_string_integrity': False,
            'overall_integrity': False
        }
        
        try:
            # ê° ë°ì´í„° íƒ€ì…ë³„ ì €ì¥/ì¡°íšŒ í…ŒìŠ¤íŠ¸
            for i, data_item in enumerate(complex_data):
                key = f'complex_test_{i}'
                
                # ì €ì¥
                cache.set(key, data_item)
                
                # ì¡°íšŒ
                retrieved = cache.get(key)
                
                # ë¬´ê²°ì„± í™•ì¸
                if retrieved == data_item:
                    if i == 0:  # í•œê¸€
                        test_results['korean_text_integrity'] = True
                    elif i == 1:  # íŠ¹ìˆ˜ë¬¸ì
                        test_results['special_chars_integrity'] = True
                    elif i == 2:  # JSON
                        test_results['json_data_integrity'] = True
                    elif i == 3:  # ì´ëª¨ì§€
                        test_results['emoji_integrity'] = True
                    elif i == 4:  # ê¸´ ë¬¸ìì—´
                        test_results['long_string_integrity'] = True
            
            # ì „ì²´ ë¬´ê²°ì„± í™•ì¸
            if all([
                test_results['korean_text_integrity'],
                test_results['special_chars_integrity'],
                test_results['json_data_integrity'],
                test_results['emoji_integrity'],
                test_results['long_string_integrity']
            ]):
                test_results['overall_integrity'] = True
                self.logger.info("âœ… ë³µì¡í•œ ë°ì´í„° êµ¬ì¡° ë¬´ê²°ì„± í™•ì¸")
            
        except Exception as e:
            test_results['error'] = str(e)
            self.logger.error(f"âŒ ë³µì¡í•œ ë°ì´í„° ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        return test_results
    
    def verify_cache_invalidation(self) -> Dict[str, Any]:
        """ìºì‹œ ë¬´íš¨í™” ì •í™•ì„± ê²€ì¦"""
        self.logger.info("ìºì‹œ ë¬´íš¨í™” ì •í™•ì„± ê²€ì¦")
        
        from cached_db_processor import CachedDBProcessor
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_file = Path('cache_invalidation_test.db')
        test_file.write_text("original content")
        
        test_results = {
            'initial_cache_working': False,
            'invalidation_triggered': False,
            'new_cache_working': False,
            'cache_invalidation_accurate': False
        }
        
        try:
            processor = CachedDBProcessor()
            
            # 1. ì´ˆê¸° ìºì‹œ í‚¤ ìƒì„±
            initial_key = processor.generate_cache_key(test_file, 1, 0, 100)
            
            # 2. íŒŒì¼ ìˆ˜ì •
            time.sleep(0.1)  # ìˆ˜ì • ì‹œê°„ ì°¨ì´ ë³´ì¥
            test_file.write_text("modified content - different size")
            
            # 3. ìˆ˜ì • í›„ ìºì‹œ í‚¤ ìƒì„±
            modified_key = processor.generate_cache_key(test_file, 1, 0, 100)
            
            # 4. í‚¤ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë¬´íš¨í™” í™•ì¸)
            if initial_key != modified_key:
                test_results['invalidation_triggered'] = True
                test_results['cache_invalidation_accurate'] = True
                self.logger.info("âœ… ìºì‹œ ë¬´íš¨í™” ì •í™•ì„± í™•ì¸")
            
            test_results['initial_cache_working'] = True
            test_results['new_cache_working'] = True
            
        except Exception as e:
            test_results['error'] = str(e)
            self.logger.error(f"âŒ ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        finally:
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
            if test_file.exists():
                test_file.unlink()
        
        return test_results
    
    def verify_data_consistency(self, db_files: List[Path]) -> Dict[str, Any]:
        """ìºì‹œ ì‚¬ìš©/ë¯¸ì‚¬ìš© ì‹œ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦"""
        self.logger.info("ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì‹œì‘")
        
        # 1. ìºì‹œ ì—†ì´ ì²˜ë¦¬
        no_cache_result = self.process_without_cache(db_files)
        
        # 2. ìºì‹œ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬
        with_cache_result = self.process_with_cache(db_files)
        
        # 3. ê²°ê³¼ ë¹„êµ
        consistency_results = {
            'item_count_consistent': False,
            'processing_success_consistent': False,
            'cache_performance_improvement': 0,
            'data_integrity_maintained': False,
            'no_cache_result': no_cache_result,
            'with_cache_result': with_cache_result
        }
        
        try:
            # í•­ëª© ìˆ˜ ì¼ê´€ì„± í™•ì¸
            if (no_cache_result['total_items'] == with_cache_result['total_items_first'] == 
                with_cache_result['total_items_second']):
                consistency_results['item_count_consistent'] = True
                self.logger.info("âœ… í•­ëª© ìˆ˜ ì¼ê´€ì„± í™•ì¸")
            
            # ì²˜ë¦¬ ì„±ê³µ ì¼ê´€ì„± í™•ì¸
            if (no_cache_result['success'] and with_cache_result['success_first'] and 
                with_cache_result['success_second']):
                consistency_results['processing_success_consistent'] = True
                self.logger.info("âœ… ì²˜ë¦¬ ì„±ê³µ ì¼ê´€ì„± í™•ì¸")
            
            # ìºì‹œ ì„±ëŠ¥ í–¥ìƒ ê³„ì‚°
            if with_cache_result['cache_effect'] > 0:
                consistency_results['cache_performance_improvement'] = with_cache_result['cache_effect']
                self.logger.info(f"âœ… ìºì‹œ ì„±ëŠ¥ í–¥ìƒ: {with_cache_result['cache_effect']:.2f}ë°°")
            
            # ì „ì²´ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
            if (consistency_results['item_count_consistent'] and 
                consistency_results['processing_success_consistent']):
                consistency_results['data_integrity_maintained'] = True
                self.logger.info("âœ… ì „ì²´ ë°ì´í„° ë¬´ê²°ì„± í™•ì¸")
            
        except Exception as e:
            consistency_results['error'] = str(e)
            self.logger.error(f"âŒ ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        return consistency_results
    
    def generate_integrity_report(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë°ì´í„° ë¬´ê²°ì„± ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        self.logger.info("ë°ì´í„° ë¬´ê²°ì„± ì¢…í•© ê²€ì¦ ì‹œì‘")
        
        # ëª¨ë“  ê²€ì¦ ì‹¤í–‰
        data_consistency = self.verify_data_consistency(db_files)
        complex_data_integrity = self.verify_complex_data_integrity()
        cache_invalidation = self.verify_cache_invalidation()
        
        # ì¢…í•© í‰ê°€
        total_tests = 4
        passed_tests = 0
        
        # ë°ì´í„° ì¼ê´€ì„±
        if data_consistency.get('data_integrity_maintained', False):
            passed_tests += 1
        
        # ë³µì¡í•œ ë°ì´í„° ë¬´ê²°ì„±
        if complex_data_integrity.get('overall_integrity', False):
            passed_tests += 1
        
        # ìºì‹œ ë¬´íš¨í™”
        if cache_invalidation.get('cache_invalidation_accurate', False):
            passed_tests += 1
        
        # ìºì‹œ ì„±ëŠ¥ íš¨ê³¼
        if data_consistency.get('cache_performance_improvement', 0) >= 1.5:
            passed_tests += 1
        
        integrity_score = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        integrity_report = {
            'overall_integrity_score': integrity_score,
            'tests_passed': passed_tests,
            'total_tests': total_tests,
            'data_consistency_verification': data_consistency,
            'complex_data_integrity_verification': complex_data_integrity,
            'cache_invalidation_verification': cache_invalidation,
            'summary': {
                'cache_performance_improvement': data_consistency.get('cache_performance_improvement', 0),
                'data_integrity_maintained': data_consistency.get('data_integrity_maintained', False),
                'complex_data_supported': complex_data_integrity.get('overall_integrity', False),
                'cache_invalidation_working': cache_invalidation.get('cache_invalidation_accurate', False)
            }
        }
        
        return integrity_report

def main():
    """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤í–‰"""
    print("ğŸ” ë°ì´í„° ë¬´ê²°ì„± ì‹¤ì œ ê²€ì¦")
    print("=" * 80)
    
    # DB íŒŒì¼ ìˆ˜ì§‘
    db_dir = Path('database')
    if not db_dir.exists():
        print("âŒ Database ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000][:2]  # í…ŒìŠ¤íŠ¸ìš© 2ê°œ
    
    if not db_files:
        print("âŒ ê²€ì¦í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ê²€ì¦ ëŒ€ìƒ: {len(db_files)}ê°œ íŒŒì¼")
    for db_file in db_files:
        print(f"   - {db_file.name}")
    
    verifier = DataIntegrityVerifier()
    
    try:
        # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤í–‰
        report = verifier.generate_integrity_report(db_files)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼:")
        print(f"   ë¬´ê²°ì„± ì ìˆ˜: {report['overall_integrity_score']:.1f}% ({report['tests_passed']}/{report['total_tests']})")
        
        # ì£¼ìš” ê²°ê³¼
        summary = report['summary']
        print(f"\nğŸ” ì£¼ìš” ê²€ì¦ ê²°ê³¼:")
        print(f"   ë°ì´í„° ë¬´ê²°ì„±: {'âœ…' if summary['data_integrity_maintained'] else 'âŒ'}")
        print(f"   ë³µì¡í•œ ë°ì´í„° ì§€ì›: {'âœ…' if summary['complex_data_supported'] else 'âŒ'}")
        print(f"   ìºì‹œ ë¬´íš¨í™”: {'âœ…' if summary['cache_invalidation_working'] else 'âŒ'}")
        print(f"   ìºì‹œ ì„±ëŠ¥ í–¥ìƒ: {summary['cache_performance_improvement']:.2f}ë°°")
        
        # ê²°ê³¼ ì €ì¥
        with open('data_integrity_verification_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ 'data_integrity_verification_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        logging.error(f"ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    print("=" * 80)

if __name__ == "__main__":
    main()
