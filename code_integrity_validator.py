"""
Phase 3 ì½”ë“œ ìƒì„± ë¬´ê²°ì„± ê²€ì¦ê¸°
ë³‘ë ¬/ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œ ìƒì„±ë˜ëŠ” C ì½”ë“œì˜ ì¼ê´€ì„±ê³¼ ì •í™•ì„± ê²€ì¦
"""

import asyncio
import multiprocessing as mp
import time
import hashlib
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
import json
from collections import defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

class CodeIntegrityValidator:
    """ì½”ë“œ ìƒì„± ë¬´ê²°ì„± ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.validation_results = {}
    
    def generate_code_hash(self, code_items: List[List[str]]) -> str:
        """ìƒì„±ëœ ì½”ë“œì˜ í•´ì‹œê°’ ê³„ì‚°"""
        # ì½”ë“œ í•­ëª©ë“¤ì„ ì •ë ¬í•˜ì—¬ ìˆœì„œì— ë¬´ê´€í•œ í•´ì‹œ ìƒì„±
        sorted_items = sorted([tuple(item) for item in code_items])
        content = str(sorted_items)
        return hashlib.md5(content.encode()).hexdigest()
    
    def process_sequential_reference(self, db_file: Path) -> Dict[str, Any]:
        """ìˆœì°¨ ì²˜ë¦¬ ì°¸ì¡° êµ¬í˜„ (ê¸°ì¤€ì )"""
        start_time = time.perf_counter()
        
        try:
            from data_manager.db_handler_v2 import DBHandlerV2
            from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing
            
            # DB ì—°ê²°
            db_handler = DBHandlerV2(str(db_file))
            db_handler.connect()
            
            # $ ì‹œíŠ¸ ì°¾ê¸°
            sheets = db_handler.get_sheets()
            dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
            
            all_code_items = []
            sheet_results = {}
            
            # ì‹œíŠ¸ë³„ ìˆœì°¨ ì²˜ë¦¬
            for sheet in dollar_sheets:
                sheet_data = db_handler.get_sheet_data(sheet['id'])
                if not sheet_data:
                    continue
                
                sheet_code_items = []
                
                # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                for row_data in sheet_data:
                    if len(row_data) >= 3:
                        code_item = [
                            "DEFINE", "CONST", "FLOAT32",
                            f"VAL_{row_data[0]}_{row_data[1]}", 
                            str(row_data[2]) if row_data[2] else "",
                            f"Generated from {sheet['name']}"
                        ]
                        sheet_code_items.append(code_item)
                
                # Cython ì½”ë“œ ìƒì„±
                if sheet_code_items:
                    processed_code = ultra_fast_write_cal_list_processing(sheet_code_items)
                    all_code_items.extend(processed_code)
                    
                    # ì‹œíŠ¸ë³„ ê²°ê³¼ ì €ì¥
                    sheet_results[sheet['name']] = {
                        'items_count': len(processed_code),
                        'hash': self.generate_code_hash(processed_code)
                    }
            
            db_handler.disconnect()
            
            execution_time = time.perf_counter() - start_time
            total_hash = self.generate_code_hash(all_code_items)
            
            return {
                'success': True,
                'execution_time': execution_time,
                'total_items': len(all_code_items),
                'total_hash': total_hash,
                'sheet_results': sheet_results,
                'processing_mode': 'sequential'
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'sequential'
            }
    
    def process_distributed_test(self, db_file: Path) -> Dict[str, Any]:
        """ë¶„ì‚° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        try:
            from distributed_db_processor import DistributedDBProcessor, DistributedConfig
            
            config = DistributedConfig(
                batch_size=500,
                chunk_size=1000,
                max_processes=2,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 2ê°œ í”„ë¡œì„¸ìŠ¤
                worker_timeout=60.0
            )
            
            processor = DistributedDBProcessor(config)
            result = processor.process_batch_distributed([db_file])
            
            if result['success'] and result['results']:
                file_result = result['results'][0]
                if file_result['success']:
                    return {
                        'success': True,
                        'execution_time': file_result['execution_time'],
                        'total_items': file_result['processed_items'],
                        'processing_mode': 'distributed',
                        'worker_pid': file_result.get('worker_pid', 'unknown')
                    }
            
            return {
                'success': False,
                'error': 'Distributed processing failed',
                'processing_mode': 'distributed'
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'distributed'
            }
    
    async def process_async_test(self, db_file: Path) -> Dict[str, Any]:
        """ë¹„ë™ê¸° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        try:
            from async_db_processor import AsyncDBProcessor, AsyncConfig
            
            config = AsyncConfig(
                batch_size=500,
                chunk_size=1000,
                max_concurrent_dbs=2,
                max_concurrent_sheets=4
            )
            
            processor = AsyncDBProcessor(config)
            result = await processor.process_batch_async([db_file])
            
            await processor.cleanup()
            
            if result['success'] and result['results']:
                file_result = result['results'][0]
                if file_result['success']:
                    return {
                        'success': True,
                        'execution_time': file_result['execution_time'],
                        'total_items': file_result['processed_items'],
                        'processing_mode': 'async'
                    }
            
            return {
                'success': False,
                'error': 'Async processing failed',
                'processing_mode': 'async'
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'async'
            }
    
    def process_cached_test(self, db_file: Path) -> Dict[str, Any]:
        """ìºì‹± ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        try:
            from cached_db_processor import CachedDBProcessor, CacheConfig
            
            config = CacheConfig(
                batch_size=500,
                chunk_size=1000,
                enable_memory_cache=True,
                memory_cache_size=100
            )
            
            processor = CachedDBProcessor(config)
            
            # ì²« ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ ë¯¸ìŠ¤)
            result1 = processor.process_batch_cached([db_file])
            
            # ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œ íˆíŠ¸)
            result2 = processor.process_batch_cached([db_file])
            
            processor.cleanup()
            
            if result1['success'] and result2['success']:
                return {
                    'success': True,
                    'first_run': {
                        'execution_time': result1['execution_time'],
                        'total_items': result1['total_processed_items']
                    },
                    'second_run': {
                        'execution_time': result2['execution_time'],
                        'total_items': result2['total_processed_items']
                    },
                    'cache_effect': result1['execution_time'] / result2['execution_time'] if result2['execution_time'] > 0 else 0,
                    'processing_mode': 'cached'
                }
            
            return {
                'success': False,
                'error': 'Cached processing failed',
                'processing_mode': 'cached'
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'processing_mode': 'cached'
            }
    
    def validate_processing_consistency(self, db_files: List[Path]) -> Dict[str, Any]:
        """ì²˜ë¦¬ ë°©ì‹ë³„ ì¼ê´€ì„± ê²€ì¦"""
        self.logger.info("ì²˜ë¦¬ ë°©ì‹ë³„ ì¼ê´€ì„± ê²€ì¦ ì‹œì‘")
        
        validation_results = {}
        
        for db_file in db_files:
            file_name = db_file.name
            self.logger.info(f"íŒŒì¼ ê²€ì¦ ì‹œì‘: {file_name}")
            
            file_results = {}
            
            # 1. ìˆœì°¨ ì²˜ë¦¬ (ì°¸ì¡°)
            self.logger.info(f"  ìˆœì°¨ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸: {file_name}")
            sequential_result = self.process_sequential_reference(db_file)
            file_results['sequential'] = sequential_result
            
            # 2. ë¶„ì‚° ì²˜ë¦¬
            self.logger.info(f"  ë¶„ì‚° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸: {file_name}")
            distributed_result = self.process_distributed_test(db_file)
            file_results['distributed'] = distributed_result
            
            # 3. ë¹„ë™ê¸° ì²˜ë¦¬
            self.logger.info(f"  ë¹„ë™ê¸° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸: {file_name}")
            try:
                async_result = asyncio.run(self.process_async_test(db_file))
                file_results['async'] = async_result
            except Exception as e:
                file_results['async'] = {
                    'success': False,
                    'error': str(e),
                    'processing_mode': 'async'
                }
            
            # 4. ìºì‹± ì²˜ë¦¬
            self.logger.info(f"  ìºì‹± ì²˜ë¦¬ í…ŒìŠ¤íŠ¸: {file_name}")
            cached_result = self.process_cached_test(db_file)
            file_results['cached'] = cached_result
            
            # ê²°ê³¼ ë¶„ì„
            analysis = self.analyze_consistency(file_results)
            file_results['analysis'] = analysis
            
            validation_results[file_name] = file_results
            
            self.logger.info(f"íŒŒì¼ ê²€ì¦ ì™„ë£Œ: {file_name}")
        
        return validation_results
    
    def analyze_consistency(self, file_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì¼ê´€ì„± ë¶„ì„"""
        analysis = {
            'item_count_consistency': True,
            'processing_success_rate': 0,
            'performance_comparison': {},
            'issues': []
        }
        
        # ì„±ê³µí•œ ì²˜ë¦¬ ë°©ì‹ë“¤
        successful_modes = [mode for mode, result in file_results.items() 
                          if mode != 'analysis' and result.get('success', False)]
        
        analysis['processing_success_rate'] = len(successful_modes) / (len(file_results) - 1) * 100
        
        if not successful_modes:
            analysis['issues'].append("ëª¨ë“  ì²˜ë¦¬ ë°©ì‹ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return analysis
        
        # í•­ëª© ìˆ˜ ì¼ê´€ì„± í™•ì¸
        item_counts = {}
        execution_times = {}
        
        for mode in successful_modes:
            result = file_results[mode]
            
            if mode == 'cached':
                # ìºì‹±ì˜ ê²½ìš° ì²« ë²ˆì§¸ ì‹¤í–‰ ê²°ê³¼ ì‚¬ìš©
                item_counts[mode] = result['first_run']['total_items']
                execution_times[mode] = result['first_run']['execution_time']
            else:
                item_counts[mode] = result.get('total_items', 0)
                execution_times[mode] = result.get('execution_time', 0)
        
        # í•­ëª© ìˆ˜ ì¼ê´€ì„± ê²€ì¦
        if len(set(item_counts.values())) > 1:
            analysis['item_count_consistency'] = False
            analysis['issues'].append(f"í•­ëª© ìˆ˜ ë¶ˆì¼ì¹˜: {item_counts}")
        
        # ì„±ëŠ¥ ë¹„êµ
        if 'sequential' in execution_times:
            baseline = execution_times['sequential']
            for mode, time_taken in execution_times.items():
                if mode != 'sequential' and baseline > 0:
                    speedup = baseline / time_taken if time_taken > 0 else 0
                    analysis['performance_comparison'][mode] = {
                        'execution_time': time_taken,
                        'speedup': speedup
                    }
        
        analysis['item_counts'] = item_counts
        analysis['execution_times'] = execution_times
        
        return analysis
    
    def generate_integrity_report(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬´ê²°ì„± ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        report = {
            'summary': {
                'total_files_tested': len(validation_results),
                'overall_consistency': True,
                'critical_issues': [],
                'performance_insights': {}
            },
            'detailed_results': validation_results
        }
        
        # ì „ì²´ ì¼ê´€ì„± ë¶„ì„
        all_consistent = True
        all_performance = defaultdict(list)
        
        for file_name, file_result in validation_results.items():
            analysis = file_result.get('analysis', {})
            
            # ì¼ê´€ì„± ì²´í¬
            if not analysis.get('item_count_consistency', True):
                all_consistent = False
                report['summary']['critical_issues'].append(
                    f"{file_name}: í•­ëª© ìˆ˜ ë¶ˆì¼ì¹˜"
                )
            
            # ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
            for mode, perf in analysis.get('performance_comparison', {}).items():
                all_performance[mode].append(perf['speedup'])
        
        report['summary']['overall_consistency'] = all_consistent
        
        # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
        for mode, speedups in all_performance.items():
            if speedups:
                report['summary']['performance_insights'][mode] = {
                    'avg_speedup': sum(speedups) / len(speedups),
                    'min_speedup': min(speedups),
                    'max_speedup': max(speedups)
                }
        
        return report

def main():
    """ë¬´ê²°ì„± ê²€ì¦ ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ” Phase 3 ì½”ë“œ ìƒì„± ë¬´ê²°ì„± ê²€ì¦")
    print("=" * 80)
    
    # DB íŒŒì¼ ìˆ˜ì§‘
    db_dir = Path('database')
    if not db_dir.exists():
        print("âŒ Database ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000][:2]  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 2ê°œë§Œ
    
    if not db_files:
        print("âŒ ê²€ì¦í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ê²€ì¦ ëŒ€ìƒ: {len(db_files)}ê°œ íŒŒì¼")
    for db_file in db_files:
        print(f"   - {db_file.name}")
    
    # ë¬´ê²°ì„± ê²€ì¦ ì‹¤í–‰
    validator = CodeIntegrityValidator()
    
    try:
        # Windowsì—ì„œ multiprocessing ì‚¬ìš© ì‹œ í•„ìš”
        mp.set_start_method('spawn', force=True)
        
        validation_results = validator.validate_processing_consistency(db_files)
        
        # ë³´ê³ ì„œ ìƒì„±
        report = validator.generate_integrity_report(validation_results)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ë¬´ê²°ì„± ê²€ì¦ ê²°ê³¼:")
        print(f"   ê²€ì¦ íŒŒì¼ ìˆ˜: {report['summary']['total_files_tested']}ê°œ")
        print(f"   ì „ì²´ ì¼ê´€ì„±: {'âœ… í†µê³¼' if report['summary']['overall_consistency'] else 'âŒ ì‹¤íŒ¨'}")
        
        if report['summary']['critical_issues']:
            print(f"   ì‹¬ê°í•œ ë¬¸ì œ:")
            for issue in report['summary']['critical_issues']:
                print(f"     - {issue}")
        
        print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ (ìˆœì°¨ ì²˜ë¦¬ ê¸°ì¤€):")
        for mode, perf in report['summary']['performance_insights'].items():
            print(f"   {mode:12s}: {perf['avg_speedup']:.2f}ë°° (ë²”ìœ„: {perf['min_speedup']:.2f}-{perf['max_speedup']:.2f})")
        
        # ê²°ê³¼ ì €ì¥
        with open('code_integrity_validation_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ 'code_integrity_validation_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        logging.error(f"ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    print("=" * 80)

if __name__ == "__main__":
    main()
