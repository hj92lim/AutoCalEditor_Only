"""
ê³ ê¸‰ DB â†’ C ì½”ë“œ ë³€í™˜ ì„±ëŠ¥ ë¶„ì„ê¸°
ì‹¤ì œ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ì™€ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ ë¶„ì„
"""

import time
import psutil
import gc
import logging
import os
import sys
import threading
from pathlib import Path
from typing import Dict, List, Any, Tuple
import json
import traceback
from concurrent.futures import ThreadPoolExecutor
import multiprocessing

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

class AdvancedDBPerformanceAnalyzer:
    """ê³ ê¸‰ DB ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.analysis_results = {}
        
    def create_large_test_databases(self, count: int = 5, size_multiplier: int = 1000) -> List[Path]:
        """ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ DB íŒŒì¼ë“¤ ìƒì„±"""
        print(f"ğŸ”„ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ DB {count}ê°œ ìƒì„± (í¬ê¸° ë°°ìˆ˜: {size_multiplier})")
        print("=" * 60)
        
        try:
            from data_manager.db_handler_v2 import DBHandlerV2
            
            large_dbs = []
            
            for i in range(count):
                db_name = f'large_performance_test_{i+1}.db'
                db_path = Path('database') / db_name
                
                if db_path.exists():
                    os.remove(db_path)
                
                db_handler = DBHandlerV2(str(db_path))
                db_handler.connect()
                db_handler.init_tables()
                
                # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì‹œíŠ¸ ìƒì„±
                total_cells = 0
                
                # ì‘ì€ ì‹œíŠ¸ë“¤
                for j in range(3):
                    sheet_id = db_handler.create_sheet_v2(f"$(Small)Sheet_{i}_{j}", is_dollar_sheet=True, source_file=f"large_test_{i}.xlsx")
                    small_data = [(k % 20, k % 10, f"SMALL_{i}_{j}_{k}") for k in range(100 * size_multiplier)]
                    
                    # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ê³ ìœ í•œ row, col ìƒì„±
                    unique_data = []
                    for idx, (row, col, value) in enumerate(small_data):
                        unique_row = row + (j * 100)
                        unique_data.append((unique_row, col, value))
                    
                    db_handler.batch_insert_cells(sheet_id, unique_data)
                    total_cells += len(unique_data)
                
                # ì¤‘ê°„ ì‹œíŠ¸ë“¤
                for j in range(2):
                    sheet_id = db_handler.create_sheet_v2(f"$(Medium)Sheet_{i}_{j}", is_dollar_sheet=True, source_file=f"large_test_{i}.xlsx")
                    medium_data = [(k % 50, k % 20, f"MEDIUM_{i}_{j}_{k}") for k in range(500 * size_multiplier)]
                    
                    # ì¤‘ë³µ ë°©ì§€
                    unique_data = []
                    for idx, (row, col, value) in enumerate(medium_data):
                        unique_row = row + (j * 200) + 1000  # ë‹¤ë¥¸ ì‹œíŠ¸ì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡
                        unique_data.append((unique_row, col, value))
                    
                    db_handler.batch_insert_cells(sheet_id, unique_data)
                    total_cells += len(unique_data)
                
                # í° ì‹œíŠ¸ (ì¼ë¶€ì—ë§Œ)
                if i % 2 == 0:
                    sheet_id = db_handler.create_sheet_v2(f"$(Large)Sheet_{i}", is_dollar_sheet=True, source_file=f"large_test_{i}.xlsx")
                    large_data = [(k % 100, k % 30, f"LARGE_{i}_{k}") for k in range(1000 * size_multiplier)]
                    
                    # ì¤‘ë³µ ë°©ì§€
                    unique_data = []
                    for idx, (row, col, value) in enumerate(large_data):
                        unique_row = row + 2000  # ë‹¤ë¥¸ ì‹œíŠ¸ì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡
                        unique_data.append((unique_row, col, value))
                    
                    db_handler.batch_insert_cells(sheet_id, unique_data)
                    total_cells += len(unique_data)
                
                db_handler.disconnect()
                
                file_size = db_path.stat().st_size
                
                print(f"   âœ… {db_name} ìƒì„± ì™„ë£Œ")
                print(f"      íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
                print(f"      ë°ì´í„°: {total_cells:,}ê°œ ì…€")
                
                large_dbs.append(db_path)
            
            return large_dbs
            
        except Exception as e:
            print(f"âŒ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ DB ìƒì„± ì‹¤íŒ¨: {e}")
            print(traceback.format_exc())
            return []
    
    def measure_system_resources(self) -> Dict[str, float]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¸¡ì •"""
        memory_info = self.process.memory_info()
        cpu_percent = self.process.cpu_percent(interval=0.1)
        
        # ì‹œìŠ¤í…œ ì „ì²´ ë©”ëª¨ë¦¬ ì •ë³´
        system_memory = psutil.virtual_memory()
        
        return {
            'process_rss_mb': memory_info.rss / 1024 / 1024,
            'process_vms_mb': memory_info.vms / 1024 / 1024,
            'process_cpu_percent': cpu_percent,
            'system_memory_percent': system_memory.percent,
            'system_available_mb': system_memory.available / 1024 / 1024
        }
    
    def stress_test_batch_processing(self, db_files: List[Path], iterations: int = 3) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: ë°˜ë³µì ì¸ ì¼ê´„ ì²˜ë¦¬"""
        print(f"\nğŸ”¥ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸: {len(db_files)}ê°œ DB íŒŒì¼ x {iterations}íšŒ ë°˜ë³µ")
        print("=" * 60)
        
        try:
            from data_manager.db_handler_v2 import DBHandlerV2
            from cython_extensions.code_generator_v2 import fast_write_cal_list_processing
            
            iteration_results = []
            
            for iteration in range(iterations):
                print(f"\nğŸ”„ ë°˜ë³µ {iteration + 1}/{iterations}")
                
                start_resources = self.measure_system_resources()
                start_time = time.perf_counter()
                
                total_processed_items = 0
                
                for i, db_file in enumerate(db_files):
                    file_start_time = time.perf_counter()
                    
                    # DB ì—°ê²°
                    db_handler = DBHandlerV2(str(db_file))
                    db_handler.connect()
                    
                    # $ ì‹œíŠ¸ ì°¾ê¸°
                    sheets = db_handler.get_sheets()
                    dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
                    
                    file_processed_items = 0
                    
                    for sheet in dollar_sheets:
                        sheet_data = db_handler.get_sheet_data(sheet['id'])
                        if sheet_data:
                            # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                            code_items = []
                            for row_data in sheet_data:
                                if len(row_data) >= 3:
                                    code_items.append([
                                        "DEFINE", "CONST", "FLOAT32",
                                        f"VAL_{row_data[0]}_{row_data[1]}", 
                                        str(row_data[2]) if row_data[2] else "",
                                        f"Generated from {sheet['name']}"
                                    ])
                            
                            # Cython ì½”ë“œ ìƒì„±
                            processed_code = fast_write_cal_list_processing(code_items)
                            file_processed_items += len(processed_code)
                    
                    # DB ì—°ê²° í•´ì œ
                    db_handler.disconnect()
                    
                    file_end_time = time.perf_counter()
                    file_time = file_end_time - file_start_time
                    
                    total_processed_items += file_processed_items
                    
                    print(f"      ğŸ“ {db_file.name}: {file_time:.3f}ì´ˆ, {file_processed_items:,}ê°œ í•­ëª©")
                    
                    # ì¤‘ê°„ ë¦¬ì†ŒìŠ¤ ì²´í¬
                    if i % 2 == 0:  # 2ê°œ íŒŒì¼ë§ˆë‹¤ ì²´í¬
                        mid_resources = self.measure_system_resources()
                        memory_growth = mid_resources['process_rss_mb'] - start_resources['process_rss_mb']
                        if memory_growth > 100:  # 100MB ì´ìƒ ì¦ê°€ì‹œ ê²½ê³ 
                            print(f"      âš ï¸ ë©”ëª¨ë¦¬ ì¦ê°€: +{memory_growth:.1f}MB")
                
                end_time = time.perf_counter()
                end_resources = self.measure_system_resources()
                
                iteration_time = end_time - start_time
                memory_delta = end_resources['process_rss_mb'] - start_resources['process_rss_mb']
                
                iteration_results.append({
                    'iteration': iteration + 1,
                    'execution_time': iteration_time,
                    'processed_items': total_processed_items,
                    'memory_delta_mb': memory_delta,
                    'start_memory_mb': start_resources['process_rss_mb'],
                    'end_memory_mb': end_resources['process_rss_mb'],
                    'cpu_usage': end_resources['process_cpu_percent']
                })
                
                print(f"   âœ… ë°˜ë³µ {iteration + 1} ì™„ë£Œ: {iteration_time:.3f}ì´ˆ")
                print(f"      ì²˜ë¦¬ í•­ëª©: {total_processed_items:,}ê°œ")
                print(f"      ë©”ëª¨ë¦¬ ë³€í™”: {start_resources['process_rss_mb']:.1f}MB â†’ {end_resources['process_rss_mb']:.1f}MB (+{memory_delta:.1f}MB)")
                
                # ë°˜ë³µ ê°„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                gc.collect()
                time.sleep(0.5)  # ì‹œìŠ¤í…œ ì•ˆì •í™”
            
            # ì„±ëŠ¥ ì €í•˜ ë¶„ì„
            if len(iteration_results) > 1:
                first_time = iteration_results[0]['execution_time']
                last_time = iteration_results[-1]['execution_time']
                performance_degradation = (last_time - first_time) / first_time * 100
                
                print(f"\nğŸ“Š ì„±ëŠ¥ ì €í•˜ ë¶„ì„:")
                print(f"   ì²« ë²ˆì§¸ ë°˜ë³µ: {first_time:.3f}ì´ˆ")
                print(f"   ë§ˆì§€ë§‰ ë°˜ë³µ: {last_time:.3f}ì´ˆ")
                print(f"   ì„±ëŠ¥ ë³€í™”: {performance_degradation:+.1f}%")
                
                if performance_degradation > 20:
                    print(f"   âš ï¸ ì‹¬ê°í•œ ì„±ëŠ¥ ì €í•˜ ê°ì§€!")
                elif performance_degradation > 10:
                    print(f"   âš ï¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€")
                else:
                    print(f"   âœ… ì•ˆì •ì ì¸ ì„±ëŠ¥ ìœ ì§€")
            
            return {
                'success': True,
                'iteration_results': iteration_results,
                'total_iterations': iterations,
                'files_count': len(db_files)
            }
            
        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            print(traceback.format_exc())
            return {
                'success': False,
                'error': str(e)
            }
    
    def parallel_processing_test(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ”€ ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: {len(db_files)}ê°œ íŒŒì¼")
        print("=" * 60)
        
        def process_single_db(db_file: Path) -> Dict[str, Any]:
            """ë‹¨ì¼ DB íŒŒì¼ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
            try:
                from data_manager.db_handler_v2 import DBHandlerV2
                from cython_extensions.code_generator_v2 import fast_write_cal_list_processing
                
                start_time = time.perf_counter()
                
                # DB ì—°ê²°
                db_handler = DBHandlerV2(str(db_file))
                db_handler.connect()
                
                # $ ì‹œíŠ¸ ì°¾ê¸°
                sheets = db_handler.get_sheets()
                dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
                
                total_processed_items = 0
                
                for sheet in dollar_sheets:
                    sheet_data = db_handler.get_sheet_data(sheet['id'])
                    if sheet_data:
                        # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                        code_items = []
                        for row_data in sheet_data:
                            if len(row_data) >= 3:
                                code_items.append([
                                    "DEFINE", "CONST", "FLOAT32",
                                    f"VAL_{row_data[0]}_{row_data[1]}", 
                                    str(row_data[2]) if row_data[2] else "",
                                    f"Generated from {sheet['name']}"
                                ])
                        
                        # Cython ì½”ë“œ ìƒì„±
                        processed_code = fast_write_cal_list_processing(code_items)
                        total_processed_items += len(processed_code)
                
                # DB ì—°ê²° í•´ì œ
                db_handler.disconnect()
                
                end_time = time.perf_counter()
                
                return {
                    'success': True,
                    'file_name': db_file.name,
                    'execution_time': end_time - start_time,
                    'processed_items': total_processed_items
                }
                
            except Exception as e:
                return {
                    'success': False,
                    'file_name': db_file.name,
                    'error': str(e)
                }
        
        # ìˆœì°¨ ì²˜ë¦¬ ì¸¡ì •
        print("ğŸ”„ ìˆœì°¨ ì²˜ë¦¬ ì¸¡ì •")
        sequential_start = time.perf_counter()
        sequential_results = []
        
        for db_file in db_files:
            result = process_single_db(db_file)
            sequential_results.append(result)
            if result['success']:
                print(f"   âœ… {result['file_name']}: {result['execution_time']:.3f}ì´ˆ")
        
        sequential_time = time.perf_counter() - sequential_start
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì¸¡ì •
        print("\nğŸ”€ ë³‘ë ¬ ì²˜ë¦¬ ì¸¡ì •")
        parallel_start = time.perf_counter()
        
        max_workers = min(len(db_files), multiprocessing.cpu_count())
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            parallel_results = list(executor.map(process_single_db, db_files))
        
        parallel_time = time.perf_counter() - parallel_start
        
        for result in parallel_results:
            if result['success']:
                print(f"   âœ… {result['file_name']}: {result['execution_time']:.3f}ì´ˆ")
        
        # ì„±ëŠ¥ ë¹„êµ
        speedup = sequential_time / parallel_time if parallel_time > 0 else 0
        
        print(f"\nğŸ“Š ë³‘ë ¬ ì²˜ë¦¬ ì„±ëŠ¥ ë¹„êµ:")
        print(f"   ìˆœì°¨ ì²˜ë¦¬: {sequential_time:.3f}ì´ˆ")
        print(f"   ë³‘ë ¬ ì²˜ë¦¬: {parallel_time:.3f}ì´ˆ")
        print(f"   ì„±ëŠ¥ í–¥ìƒ: {speedup:.2f}ë°°")
        print(f"   ì‚¬ìš© ì›Œì»¤: {max_workers}ê°œ")
        
        return {
            'sequential_time': sequential_time,
            'parallel_time': parallel_time,
            'speedup': speedup,
            'max_workers': max_workers,
            'sequential_results': sequential_results,
            'parallel_results': parallel_results
        }

if __name__ == "__main__":
    print("ğŸ” ê³ ê¸‰ DB â†’ C ì½”ë“œ ë³€í™˜ ì„±ëŠ¥ ë¶„ì„")
    print("=" * 80)
    
    analyzer = AdvancedDBPerformanceAnalyzer()
    
    # 1. ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ DB ìƒì„±
    large_dbs = analyzer.create_large_test_databases(count=5, size_multiplier=2)
    
    if not large_dbs:
        print("âŒ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ DB ìƒì„± ì‹¤íŒ¨")
        sys.exit(1)
    
    # 2. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    stress_results = analyzer.stress_test_batch_processing(large_dbs, iterations=3)
    
    # 3. ë³‘ë ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    parallel_results = analyzer.parallel_processing_test(large_dbs)
    
    # ê²°ê³¼ ì €ì¥
    final_results = {
        'stress_test': stress_results,
        'parallel_test': parallel_results,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open('advanced_db_performance_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ê°€ 'advanced_db_performance_analysis.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 80)
