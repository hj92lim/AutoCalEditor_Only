"""
Phase 3: ë¶„ì‚° DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ
multiprocessingì„ ì´ìš©í•œ ë¶„ì‚° ì²˜ë¦¬ êµ¬í˜„
"""

import multiprocessing as mp
import time
import gc
import logging
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import json
import queue
import psutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s'
)

@dataclass
class DistributedConfig:
    """ë¶„ì‚° ì²˜ë¦¬ ì„¤ì •"""
    batch_size: int = 500
    chunk_size: int = 1000
    gc_interval: int = 4
    max_processes: int = None  # Noneì´ë©´ CPU ì½”ì–´ ìˆ˜
    max_queue_size: int = 100
    worker_timeout: float = 300.0  # 5ë¶„
    enable_shared_memory: bool = True
    memory_limit_mb: int = 512  # í”„ë¡œì„¸ìŠ¤ë‹¹ ë©”ëª¨ë¦¬ ì œí•œ

def process_single_db_worker(args: Tuple[str, DistributedConfig]) -> Dict[str, Any]:
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë‹¨ì¼ DB ì²˜ë¦¬ í•¨ìˆ˜"""
    db_path, config = args
    
    # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™”
    worker_logger = logging.getLogger(f"worker_{os.getpid()}")
    start_time = time.perf_counter()
    file_name = Path(db_path).name
    
    try:
        worker_logger.info(f"ì›Œì»¤ {os.getpid()}: DB ì²˜ë¦¬ ì‹œì‘ - {file_name}")
        
        # í”„ë¡œì„¸ìŠ¤ë³„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024
        
        # í”„ë¡œì íŠ¸ ê²½ë¡œ ì¬ì„¤ì • (ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì—ì„œ)
        sys.path.insert(0, str(Path(__file__).parent))
        
        # DB ì—°ê²°
        from data_manager.db_handler_v2 import DBHandlerV2
        db_handler = DBHandlerV2(db_path)
        db_handler.connect()
        
        # $ ì‹œíŠ¸ ì°¾ê¸°
        sheets = db_handler.get_sheets()
        dollar_sheets = [s for s in sheets if s.get('is_dollar_sheet', False)]
        
        if not dollar_sheets:
            worker_logger.warning(f"ì›Œì»¤ {os.getpid()}: $ ì‹œíŠ¸ê°€ ì—†ìŒ - {file_name}")
            db_handler.disconnect()
            return {
                'success': True,
                'file_name': file_name,
                'execution_time': time.perf_counter() - start_time,
                'processed_items': 0,
                'worker_pid': os.getpid(),
                'warning': 'No dollar sheets found'
            }
        
        # Ultra ìµœì í™”ëœ Cython ëª¨ë“ˆ ì‚¬ìš©
        from cython_extensions.code_generator_v2 import ultra_fast_write_cal_list_processing
        
        total_processed_items = 0
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
        for sheet_idx, sheet in enumerate(dollar_sheets):
            sheet_data = db_handler.get_sheet_data(sheet['id'])
            if not sheet_data:
                continue
            
            worker_logger.debug(f"ì›Œì»¤ {os.getpid()}: ì‹œíŠ¸ ì²˜ë¦¬ - {sheet['name']} ({len(sheet_data)}ê°œ ì…€)")
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for chunk_start in range(0, len(sheet_data), config.chunk_size):
                chunk_end = min(chunk_start + config.chunk_size, len(sheet_data))
                chunk_data = sheet_data[chunk_start:chunk_end]
                
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì½”ë“œ ìƒì„±
                batch_count = 0
                for batch_start in range(0, len(chunk_data), config.batch_size):
                    batch_end = min(batch_start + config.batch_size, len(chunk_data))
                    batch_data = chunk_data[batch_start:batch_end]
                    
                    # ì½”ë“œ ì•„ì´í…œ ìƒì„±
                    code_items = []
                    for row_data in batch_data:
                        if len(row_data) >= 3:
                            code_items.append([
                                "DEFINE", "CONST", "FLOAT32",
                                f"VAL_{row_data[0]}_{row_data[1]}", 
                                str(row_data[2]) if row_data[2] else "",
                                f"Generated from {sheet['name']}"
                            ])
                    
                    # Ultra ìµœì í™”ëœ Cython ì½”ë“œ ìƒì„±
                    if code_items:
                        processed_code = ultra_fast_write_cal_list_processing(code_items)
                        total_processed_items += len(processed_code)
                    
                    # ë°°ì¹˜ ê°„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    del code_items
                    if 'processed_code' in locals():
                        del processed_code
                    
                    batch_count += 1
                    
                    # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
                    if batch_count % config.gc_interval == 0:
                        gc.collect()
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                    if batch_count % 20 == 0:
                        current_memory = process.memory_info().rss / 1024 / 1024
                        if current_memory > config.memory_limit_mb:
                            worker_logger.warning(f"ì›Œì»¤ {os.getpid()}: ë©”ëª¨ë¦¬ ì œí•œ ì´ˆê³¼ {current_memory:.1f}MB")
                            gc.collect()
        
        # DB ì—°ê²° í•´ì œ
        db_handler.disconnect()
        
        execution_time = time.perf_counter() - start_time
        end_memory = process.memory_info().rss / 1024 / 1024
        memory_delta = end_memory - start_memory
        
        worker_logger.info(f"ì›Œì»¤ {os.getpid()}: DB ì²˜ë¦¬ ì™„ë£Œ - {file_name} ({execution_time:.3f}ì´ˆ, {total_processed_items:,}ê°œ í•­ëª©, ë©”ëª¨ë¦¬ +{memory_delta:.1f}MB)")
        
        return {
            'success': True,
            'file_name': file_name,
            'execution_time': execution_time,
            'processed_items': total_processed_items,
            'sheets_processed': len(dollar_sheets),
            'worker_pid': os.getpid(),
            'memory_delta_mb': memory_delta
        }
        
    except Exception as e:
        execution_time = time.perf_counter() - start_time
        error_msg = f"ì›Œì»¤ {os.getpid()}: DB ì²˜ë¦¬ ì‹¤íŒ¨ - {file_name} - {str(e)}"
        worker_logger.error(error_msg)
        
        return {
            'success': False,
            'file_name': file_name,
            'execution_time': execution_time,
            'error': str(e),
            'worker_pid': os.getpid()
        }

class DistributedDBProcessor:
    """ë¶„ì‚° DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self, config: DistributedConfig = None):
        self.config = config or DistributedConfig()
        
        # CPU ì½”ì–´ ìˆ˜ ìë™ ì„¤ì •
        if self.config.max_processes is None:
            self.config.max_processes = min(mp.cpu_count(), 8)  # ìµœëŒ€ 8ê°œ í”„ë¡œì„¸ìŠ¤
        
        self.logger = logging.getLogger(__name__)
        self.stats = {
            'total_files_processed': 0,
            'total_items_processed': 0,
            'total_execution_time': 0,
            'worker_stats': {}
        }
    
    def process_batch_distributed(self, db_files: List[Path]) -> Dict[str, Any]:
        """ë¶„ì‚° ì¼ê´„ ì²˜ë¦¬"""
        start_time = time.perf_counter()
        
        self.logger.info(f"ë¶„ì‚° ì²˜ë¦¬ ì‹œì‘: {len(db_files)}ê°œ íŒŒì¼, {self.config.max_processes}ê°œ í”„ë¡œì„¸ìŠ¤")
        
        # ì‘ì—… ì¸ìˆ˜ ì¤€ë¹„
        work_args = [(str(db_file), self.config) for db_file in db_files]
        
        try:
            # í”„ë¡œì„¸ìŠ¤ í’€ ìƒì„± ë° ì‹¤í–‰
            with mp.Pool(processes=self.config.max_processes) as pool:
                # ë¹„ë™ê¸° ì‹¤í–‰ìœ¼ë¡œ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥
                async_results = []
                
                # ì‘ì—… ì œì¶œ
                for args in work_args:
                    async_result = pool.apply_async(process_single_db_worker, (args,))
                    async_results.append(async_result)
                
                # ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ ì ìš©)
                results = []
                for i, async_result in enumerate(async_results):
                    try:
                        result = async_result.get(timeout=self.config.worker_timeout)
                        results.append(result)
                        
                        if result['success']:
                            self.logger.info(f"ì‘ì—… ì™„ë£Œ ({i+1}/{len(async_results)}): {result['file_name']}")
                        else:
                            self.logger.error(f"ì‘ì—… ì‹¤íŒ¨ ({i+1}/{len(async_results)}): {result['file_name']}")
                            
                    except mp.TimeoutError:
                        self.logger.error(f"ì‘ì—… íƒ€ì„ì•„ì›ƒ ({i+1}/{len(async_results)}): {work_args[i][0]}")
                        results.append({
                            'success': False,
                            'file_name': Path(work_args[i][0]).name,
                            'error': 'Worker timeout',
                            'execution_time': self.config.worker_timeout
                        })
                    except Exception as e:
                        self.logger.error(f"ì‘ì—… ì˜ˆì™¸ ({i+1}/{len(async_results)}): {e}")
                        results.append({
                            'success': False,
                            'file_name': Path(work_args[i][0]).name,
                            'error': str(e),
                            'execution_time': 0
                        })
            
            execution_time = time.perf_counter() - start_time
            
            # í†µê³„ ê³„ì‚°
            successful_results = [r for r in results if r['success']]
            total_processed_items = sum(r.get('processed_items', 0) for r in successful_results)
            
            # ì›Œì»¤ë³„ í†µê³„
            worker_stats = {}
            for result in successful_results:
                worker_pid = result.get('worker_pid', 'unknown')
                if worker_pid not in worker_stats:
                    worker_stats[worker_pid] = {
                        'files_processed': 0,
                        'items_processed': 0,
                        'total_time': 0,
                        'memory_usage': 0
                    }
                
                worker_stats[worker_pid]['files_processed'] += 1
                worker_stats[worker_pid]['items_processed'] += result.get('processed_items', 0)
                worker_stats[worker_pid]['total_time'] += result.get('execution_time', 0)
                worker_stats[worker_pid]['memory_usage'] += result.get('memory_delta_mb', 0)
            
            self.stats['worker_stats'] = worker_stats
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.stats['total_files_processed'] += len(successful_results)
            self.stats['total_items_processed'] += total_processed_items
            self.stats['total_execution_time'] += execution_time
            
            self.logger.info(f"ë¶„ì‚° ì²˜ë¦¬ ì™„ë£Œ: {execution_time:.3f}ì´ˆ, {total_processed_items:,}ê°œ í•­ëª©")
            
            return {
                'success': True,
                'execution_time': execution_time,
                'total_processed_items': total_processed_items,
                'files_processed': len(successful_results),
                'files_failed': len(results) - len(successful_results),
                'results': results,
                'processing_mode': 'distributed',
                'processes_used': self.config.max_processes,
                'worker_stats': worker_stats
            }
            
        except Exception as e:
            execution_time = time.perf_counter() - start_time
            self.logger.error(f"ë¶„ì‚° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            return {
                'success': False,
                'execution_time': execution_time,
                'error': str(e),
                'processing_mode': 'distributed'
            }
    
    def get_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        stats = dict(self.stats)
        
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
        stats['system_info'] = {
            'cpu_count': mp.cpu_count(),
            'processes_used': self.config.max_processes,
            'memory_limit_per_process': self.config.memory_limit_mb
        }
        
        return stats

def main():
    """ë¶„ì‚° ì²˜ë¦¬ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Phase 3: ë¶„ì‚° DB â†’ C ì½”ë“œ ë³€í™˜ í”„ë¡œì„¸ì„œ")
    print("=" * 80)
    
    # ì„¤ì •
    config = DistributedConfig(
        batch_size=500,
        chunk_size=1000,
        gc_interval=4,
        max_processes=None,  # CPU ì½”ì–´ ìˆ˜ ìë™ ì„¤ì •
        worker_timeout=300.0,
        memory_limit_mb=512
    )
    
    # DB íŒŒì¼ ìˆ˜ì§‘
    db_dir = Path('database')
    if not db_dir.exists():
        print("âŒ Database ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    
    db_files = [f for f in db_dir.glob('*.db') if f.stat().st_size > 50000]
    
    if not db_files:
        print("âŒ ì²˜ë¦¬í•  DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“ ì²˜ë¦¬ ëŒ€ìƒ: {len(db_files)}ê°œ íŒŒì¼")
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ê°€ëŠ¥ CPU: {mp.cpu_count()}ê°œ ì½”ì–´")
    print(f"âš™ï¸ ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤: {config.max_processes or mp.cpu_count()}ê°œ")
    
    # ë¶„ì‚° í”„ë¡œì„¸ì„œ ìƒì„± ë° ì‹¤í–‰
    processor = DistributedDBProcessor(config)
    
    try:
        # ë¶„ì‚° ì¼ê´„ ì²˜ë¦¬ ì‹¤í–‰
        result = processor.process_batch_distributed(db_files)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“Š ë¶„ì‚° ì²˜ë¦¬ ê²°ê³¼:")
        print(f"   ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.3f}ì´ˆ")
        print(f"   ì²˜ë¦¬ í•­ëª©: {result['total_processed_items']:,}ê°œ")
        print(f"   ì„±ê³µ íŒŒì¼: {result['files_processed']}ê°œ")
        print(f"   ì‹¤íŒ¨ íŒŒì¼: {result['files_failed']}ê°œ")
        print(f"   ì²˜ë¦¬ ëª¨ë“œ: {result['processing_mode']}")
        print(f"   ì‚¬ìš© í”„ë¡œì„¸ìŠ¤: {result['processes_used']}ê°œ")
        
        # ì²˜ë¦¬ ì†ë„ ê³„ì‚°
        if result['execution_time'] > 0:
            items_per_second = result['total_processed_items'] / result['execution_time']
            print(f"   ì²˜ë¦¬ ì†ë„: {items_per_second:,.0f} í•­ëª©/ì´ˆ")
        
        # ì›Œì»¤ë³„ í†µê³„
        if 'worker_stats' in result:
            print(f"\nğŸ“ˆ ì›Œì»¤ë³„ í†µê³„:")
            for worker_pid, stats in result['worker_stats'].items():
                print(f"   ì›Œì»¤ {worker_pid}: {stats['files_processed']}ê°œ íŒŒì¼, {stats['items_processed']:,}ê°œ í•­ëª©")
        
        # í†µê³„ ì •ë³´
        stats = processor.get_stats()
        print(f"\nğŸ–¥ï¸ ì‹œìŠ¤í…œ ì •ë³´:")
        print(f"   CPU ì½”ì–´: {stats['system_info']['cpu_count']}ê°œ")
        print(f"   ì‚¬ìš© í”„ë¡œì„¸ìŠ¤: {stats['system_info']['processes_used']}ê°œ")
        print(f"   í”„ë¡œì„¸ìŠ¤ë‹¹ ë©”ëª¨ë¦¬ ì œí•œ: {stats['system_info']['memory_limit_per_process']}MB")
        
        # ê²°ê³¼ ì €ì¥
        with open('distributed_processing_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                'result': result,
                'stats': stats,
                'config': config.__dict__
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ê²°ê³¼ê°€ 'distributed_processing_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ë¶„ì‚° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        logging.error(f"ë¶„ì‚° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    print("=" * 80)

if __name__ == "__main__":
    # Windowsì—ì„œ multiprocessing ì‚¬ìš© ì‹œ í•„ìš”
    mp.set_start_method('spawn', force=True)
    main()
