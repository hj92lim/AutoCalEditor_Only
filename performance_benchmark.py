"""
Cython vs Python ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸
Excel â†’ DB â†’ C ì½”ë“œ ë³€í™˜ ê³¼ì •ì˜ ì„±ëŠ¥ ì¸¡ì • ë° ë¹„êµ
"""

import time
import logging
import traceback
import os
import sys
import gc
import psutil
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import sqlite3

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    handlers=[
        logging.FileHandler('performance_benchmark.log', mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.results = {}
        self.process = psutil.Process()
        
    def measure_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        memory_info = self.process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,  # ë¬¼ë¦¬ ë©”ëª¨ë¦¬ (MB)
            'vms_mb': memory_info.vms / 1024 / 1024,  # ê°€ìƒ ë©”ëª¨ë¦¬ (MB)
        }
    
    def benchmark_function(self, func_name: str, func, *args, **kwargs) -> Dict:
        """í•¨ìˆ˜ ì„±ëŠ¥ ì¸¡ì •"""
        logging.info(f"ğŸ” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {func_name}")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        gc.collect()
        
        # ì‹œì‘ ì‹œì  ë©”ëª¨ë¦¬ ì¸¡ì •
        start_memory = self.measure_memory_usage()
        start_time = time.perf_counter()
        
        try:
            # í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)
            
            # ì¢…ë£Œ ì‹œì  ì¸¡ì •
            end_time = time.perf_counter()
            end_memory = self.measure_memory_usage()
            
            execution_time = end_time - start_time
            memory_delta = {
                'rss_mb': end_memory['rss_mb'] - start_memory['rss_mb'],
                'vms_mb': end_memory['vms_mb'] - start_memory['vms_mb']
            }
            
            benchmark_result = {
                'success': True,
                'execution_time': execution_time,
                'start_memory': start_memory,
                'end_memory': end_memory,
                'memory_delta': memory_delta,
                'result': result
            }
            
            logging.info(f"âœ… {func_name} ì™„ë£Œ: {execution_time:.3f}ì´ˆ, ë©”ëª¨ë¦¬ ì¦ê°€: {memory_delta['rss_mb']:.1f}MB")
            
        except Exception as e:
            end_time = time.perf_counter()
            execution_time = end_time - start_time
            
            benchmark_result = {
                'success': False,
                'execution_time': execution_time,
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            
            logging.error(f"âŒ {func_name} ì‹¤íŒ¨: {e} ({execution_time:.3f}ì´ˆ)")
        
        self.results[func_name] = benchmark_result
        return benchmark_result

    def compare_implementations(self, test_name: str, python_func, cython_func, *args, **kwargs) -> Dict:
        """Python vs Cython êµ¬í˜„ ë¹„êµ"""
        logging.info(f"ğŸ†š êµ¬í˜„ ë¹„êµ ì‹œì‘: {test_name}")
        
        # Python ë²„ì „ ì¸¡ì •
        python_result = self.benchmark_function(f"{test_name}_python", python_func, *args, **kwargs)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        time.sleep(0.1)
        
        # Cython ë²„ì „ ì¸¡ì •
        cython_result = self.benchmark_function(f"{test_name}_cython", cython_func, *args, **kwargs)
        
        # ì„±ëŠ¥ ë¹„êµ ê³„ì‚°
        if python_result['success'] and cython_result['success']:
            speedup = python_result['execution_time'] / cython_result['execution_time']
            memory_improvement = python_result['memory_delta']['rss_mb'] - cython_result['memory_delta']['rss_mb']
            
            comparison = {
                'test_name': test_name,
                'python_time': python_result['execution_time'],
                'cython_time': cython_result['execution_time'],
                'speedup': speedup,
                'memory_improvement_mb': memory_improvement,
                'python_success': True,
                'cython_success': True
            }
            
            if speedup > 1:
                logging.info(f"ğŸš€ {test_name}: Cythonì´ {speedup:.2f}ë°° ë¹ ë¦„")
            else:
                logging.warning(f"âš ï¸ {test_name}: Pythonì´ {1/speedup:.2f}ë°° ë¹ ë¦„ (Cython ìµœì í™” í•„ìš”)")
                
        else:
            comparison = {
                'test_name': test_name,
                'python_success': python_result['success'],
                'cython_success': cython_result['success'],
                'python_error': python_result.get('error'),
                'cython_error': cython_result.get('error')
            }
            
            logging.error(f"âŒ {test_name}: ë¹„êµ ì‹¤íŒ¨ (Python: {python_result['success']}, Cython: {cython_result['success']})")
        
        return comparison

def create_test_data() -> Tuple[List, List, Dict]:
    """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ìƒì„±"""
    logging.info("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # Excel ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (1000x100 í¬ê¸°)
    excel_data = []
    for i in range(1000):
        row = []
        for j in range(100):
            if j == 0:
                row.append(f"Item_{i}")
            elif j == 1:
                row.append("FLOAT32")
            elif j == 2:
                row.append(f"value_{i}")
            elif j == 3:
                row.append(str(i * 1.5))
            else:
                row.append(f"data_{i}_{j}")
        excel_data.append(row)
    
    # ì…€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    cells_data = []
    for i, row in enumerate(excel_data):
        for j, value in enumerate(row):
            if value:
                cells_data.append((i, j, str(value)))
    
    # ì‹œíŠ¸ ë°ì´í„°
    sheet_data = {
        'id': 1,
        'name': 'TestSheet',
        'data': excel_data
    }
    
    logging.info(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(excel_data)}í–‰ x {len(excel_data[0])}ì—´, {len(cells_data)}ê°œ ì…€")
    
    return excel_data, cells_data, sheet_data

def test_excel_processing(benchmark: PerformanceBenchmark, excel_data: List, cells_data: List):
    """Excel ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logging.info("ğŸ“ˆ Excel ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # Python ë²„ì „ í•¨ìˆ˜
    def python_excel_processing(values):
        cells_data = []
        if values is None:
            return cells_data
        
        rows = len(values)
        for i in range(rows):
            row = values[i]
            if isinstance(row, list):
                cols = len(row)
                for j in range(cols):
                    val = row[j]
                    if val is not None:
                        value_str = str(val)
                        cells_data.append((i, j, value_str))
        return cells_data
    
    # Cython ë²„ì „ import ì‹œë„
    try:
        from cython_extensions.excel_processor_v2 import fast_process_excel_data
        cython_func = fast_process_excel_data
    except ImportError:
        logging.warning("Cython Excel ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, Python ë²„ì „ë§Œ í…ŒìŠ¤íŠ¸")
        cython_func = python_excel_processing
    
    # ì„±ëŠ¥ ë¹„êµ
    comparison = benchmark.compare_implementations(
        "excel_processing", 
        python_excel_processing, 
        cython_func, 
        excel_data
    )
    
    return comparison

def test_data_processing(benchmark: PerformanceBenchmark, cells_data: List):
    """ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logging.info("ğŸ”§ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # Python ë²„ì „ í•¨ìˆ˜
    def python_batch_processing(cells_data):
        processed_data = []
        for cell_tuple in cells_data:
            row, col, value = cell_tuple
            value_str = str(value) if value is not None else ""
            if value_str and value_str.strip():
                processed_data.append((row, col, value_str))
        return processed_data
    
    # Cython ë²„ì „ import ì‹œë„
    try:
        from cython_extensions.data_processor import fast_db_batch_processing
        cython_func = fast_db_batch_processing
    except ImportError:
        logging.warning("Cython ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, Python ë²„ì „ë§Œ í…ŒìŠ¤íŠ¸")
        cython_func = python_batch_processing
    
    # ì„±ëŠ¥ ë¹„êµ
    comparison = benchmark.compare_implementations(
        "data_processing", 
        python_batch_processing, 
        cython_func, 
        cells_data
    )
    
    return comparison

if __name__ == "__main__":
    logging.info("ğŸš€ Cython vs Python ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    
    # ë²¤ì¹˜ë§ˆí¬ ê°ì²´ ìƒì„±
    benchmark = PerformanceBenchmark()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    excel_data, cells_data, sheet_data = create_test_data()
    
    # ê°ì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = []
    
    # 1. Excel ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    excel_result = test_excel_processing(benchmark, excel_data, cells_data)
    results.append(excel_result)
    
    # 2. ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    data_result = test_data_processing(benchmark, cells_data)
    results.append(data_result)
    
    logging.info("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ, ê²°ê³¼ ë¶„ì„ ì¤‘...")

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "="*80)
    print("ğŸ¯ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("="*80)

    total_speedup = 0
    successful_tests = 0

    for result in results:
        if result.get('python_success') and result.get('cython_success'):
            test_name = result['test_name']
            python_time = result['python_time']
            cython_time = result['cython_time']
            speedup = result['speedup']
            memory_improvement = result.get('memory_improvement_mb', 0)

            print(f"\nğŸ“ˆ {test_name.upper()}")
            print(f"   Python ì‹œê°„:    {python_time:.4f}ì´ˆ")
            print(f"   Cython ì‹œê°„:    {cython_time:.4f}ì´ˆ")
            print(f"   ì„±ëŠ¥ í–¥ìƒ:      {speedup:.2f}ë°°")
            print(f"   ë©”ëª¨ë¦¬ ê°œì„ :    {memory_improvement:.1f}MB")

            if speedup > 1:
                print(f"   âœ… Cythonì´ {speedup:.1f}ë°° ë¹ ë¦„")
            else:
                print(f"   âš ï¸ Pythonì´ {1/speedup:.1f}ë°° ë¹ ë¦„")

            total_speedup += speedup
            successful_tests += 1
        else:
            test_name = result['test_name']
            print(f"\nâŒ {test_name.upper()}: í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            if not result.get('python_success'):
                print(f"   Python ì˜¤ë¥˜: {result.get('python_error', 'Unknown')}")
            if not result.get('cython_success'):
                print(f"   Cython ì˜¤ë¥˜: {result.get('cython_error', 'Unknown')}")

    # ì „ì²´ ìš”ì•½
    print(f"\n{'='*80}")
    if successful_tests > 0:
        avg_speedup = total_speedup / successful_tests
        print(f"ğŸ† ì „ì²´ í‰ê·  ì„±ëŠ¥ í–¥ìƒ: {avg_speedup:.2f}ë°°")

        if avg_speedup > 1.5:
            print("ğŸš€ Cython ìµœì í™”ê°€ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
        elif avg_speedup > 1.0:
            print("âœ… Cython ìµœì í™”ê°€ ì¼ë¶€ íš¨ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ Cython ìµœì í™” íš¨ê³¼ê°€ ë¯¸ë¯¸í•˜ê±°ë‚˜ ì—­íš¨ê³¼ê°€ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.")
            print("   - ì»´íŒŒì¼ ìµœì í™” ì˜µì…˜ ê²€í†  í•„ìš”")
            print("   - Python-Cython ê°„ ë°ì´í„° ì „ë‹¬ ì˜¤ë²„í—¤ë“œ í™•ì¸ í•„ìš”")
            print("   - íƒ€ì… ì„ ì–¸ ë° ìµœì í™” ì§€ì‹œë¬¸ ê°œì„  í•„ìš”")
    else:
        print("âŒ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. Cython ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    print("="*80)

    # ìƒì„¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    import json
    with open('benchmark_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'summary': {
                'total_tests': len(results),
                'successful_tests': successful_tests,
                'average_speedup': total_speedup / successful_tests if successful_tests > 0 else 0
            },
            'detailed_results': results,
            'all_benchmark_data': benchmark.results
        }, f, indent=2, ensure_ascii=False)

    logging.info("ğŸ“„ ìƒì„¸ ê²°ê³¼ê°€ benchmark_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def test_code_generation(benchmark: PerformanceBenchmark, test_data: List):
    """ì½”ë“œ ìƒì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    logging.info("âš™ï¸ ì½”ë“œ ìƒì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    # í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ ì•„ì´í…œ ìƒì„±
    temp_code_items = []
    for i in range(1000):
        temp_code_items.append([
            "DEFINE",
            "CONST",
            "FLOAT32",
            f"TEST_VALUE_{i}",
            str(i * 1.5),
            f"Test value {i}"
        ])

    # Python ë²„ì „ í•¨ìˆ˜
    def python_code_processing(temp_code_items):
        processed_items = []
        for line_str in temp_code_items:
            if line_str and len(line_str) >= 6:
                op_code = line_str[0] if line_str[0] else ""
                key_str = line_str[1] if line_str[1] else ""
                type_str = line_str[2] if line_str[2] else ""
                name_str = line_str[3] if line_str[3] else ""
                val_str = line_str[4] if line_str[4] else ""
                desc_str = line_str[5] if line_str[5] else ""

                # FLOAT32 ë³€ìˆ˜ì˜ ìˆ«ìì— f ì ‘ë¯¸ì‚¬ ì¶”ê°€
                if val_str and type_str and "FLOAT32" in type_str:
                    if val_str.replace('.', '').replace('-', '').isdigit():
                        if '.' in val_str:
                            val_str = val_str + 'f'
                        else:
                            val_str = val_str + '.f'

                processed_items.append([op_code, key_str, type_str, name_str, val_str, desc_str])
        return processed_items

    # Cython ë²„ì „ import ì‹œë„
    try:
        from cython_extensions.code_generator_v2 import fast_write_cal_list_processing
        cython_func = fast_write_cal_list_processing
    except ImportError:
        logging.warning("Cython ì½”ë“œ ìƒì„± ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, Python ë²„ì „ë§Œ í…ŒìŠ¤íŠ¸")
        cython_func = python_code_processing

    # ì„±ëŠ¥ ë¹„êµ
    comparison = benchmark.compare_implementations(
        "code_generation",
        python_code_processing,
        cython_func,
        temp_code_items
    )

    return comparison

def analyze_cython_status():
    """Cython ëª¨ë“ˆ ìƒíƒœ ë¶„ì„"""
    logging.info("ğŸ” Cython ëª¨ë“ˆ ìƒíƒœ ë¶„ì„ ì‹œì‘")

    modules_to_check = [
        'cython_extensions.excel_processor_v2',
        'cython_extensions.data_processor',
        'cython_extensions.code_generator_v2',
        'cython_extensions.regex_optimizer'
    ]

    status = {}
    for module in modules_to_check:
        try:
            __import__(module)
            status[module] = {'available': True, 'error': None}
            logging.info(f"âœ… {module}: ì‚¬ìš© ê°€ëŠ¥")
        except ImportError as e:
            status[module] = {'available': False, 'error': str(e)}
            logging.warning(f"âŒ {module}: ì‚¬ìš© ë¶ˆê°€ - {e}")

    # ì»´íŒŒì¼ëœ íŒŒì¼ í™•ì¸
    cython_dir = Path('cython_extensions')
    if cython_dir.exists():
        compiled_files = list(cython_dir.glob('*.pyd')) + list(cython_dir.glob('*.so'))
        logging.info(f"ğŸ“ ì»´íŒŒì¼ëœ íŒŒì¼: {[f.name for f in compiled_files]}")
    else:
        logging.warning("ğŸ“ cython_extensions ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

    return status

def run_comprehensive_benchmark():
    """ì¢…í•©ì ì¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    logging.info("ğŸš€ ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")

    # Cython ìƒíƒœ ë¶„ì„
    cython_status = analyze_cython_status()

    # ë²¤ì¹˜ë§ˆí¬ ê°ì²´ ìƒì„±
    benchmark = PerformanceBenchmark()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    excel_data, cells_data, sheet_data = create_test_data()

    # ê°ì¢… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = []

    try:
        # 1. Excel ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        excel_result = test_excel_processing(benchmark, excel_data, cells_data)
        results.append(excel_result)

        # 2. ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        data_result = test_data_processing(benchmark, cells_data)
        results.append(data_result)

        # 3. ì½”ë“œ ìƒì„± í…ŒìŠ¤íŠ¸
        code_result = test_code_generation(benchmark, excel_data)
        results.append(code_result)

    except Exception as e:
        logging.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        logging.error(traceback.format_exc())

    return results, cython_status, benchmark

if __name__ == "__main__":
    # ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results, cython_status, benchmark = run_comprehensive_benchmark()
